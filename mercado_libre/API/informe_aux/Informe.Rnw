\documentclass[12pt,twoside,spanish,a4paper]{book}
\usepackage{geometry}\geometry{top=3cm,bottom=3cm,left=3cm,right=3cm}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
%\usepackage{mathrsfs}
\usepackage{longtable}
\usepackage{tocbibind}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{boxedminipage}
\usepackage[utf8]{inputenc}
%\usepackage[all,2cell,dvips]{xy}
\usepackage{graphicx}
\usepackage{float}
\usepackage[spanish,es-tabla]{babel}
%\usepackage{parskip}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{verbatim}
\usepackage{hyperref}
%\usepackage{fancyvrb}
\usepackage[authoryear]{natbib}
\usepackage{booktabs}
\usepackage{caption}
\fancyhf{} 
\fancyhead[LE]{\leftmark} 
\fancyhead[RO]{\nouppercase{\rightmark}} 
%\fancyfoot[LE,RO]{\thepage} 
\rfoot{\thepage} 
\pagestyle{fancy} 

\topmargin 2mm
\oddsidemargin 2mm
\evensidemargin 2mm

\makeindex
\setcounter{secnumdepth}{3}

\linespread{1.6}

\begin{document}
\SweaveOpts{concordance=TRUE,cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align = "c", caption = TRUE, comment = FALSE, fig.pos = "h"}

<<echo=FALSE>>=
options(scipen = 999, cache = TRUE)  
@

<<librerias>>=
library(knitr)
library(tidyverse)
library(sf)
library(scales)
library(here)
library(gridExtra)
library(data.table)
library(magrittr)
library(ggcorrplot)
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(caret)
library(doParallel)
library(rpart)
library(rpart.plot)
library(rattle)
library(ranger)
library(missRanger)
library(kableExtra)
library(xtable)
library(RColorBrewer)
library(lmtest)
library(nortest)
library(glmnet)
library(GGally)
@


<<funciones, echo=FALSE, message=FALSE, warning=FALSE>>=
source(here("mercado_libre/API/funciones","funcion_imput_media.R"))
@

<<Datos_imputmedia>>=
options(scipen = 999)

#### DATOS

aptos_yearmonth <- list.files(path = here("mercado_libre/API/datos/limpios/apt"), 
                              pattern = "*.csv", full.names = T)

yearmonth <- c('aptos_202106','aptos_202107',"aptos_202108", "aptos_202109")

aptos <- sapply(aptos_yearmonth, FUN=function(yearmonth){
      read_csv(file=yearmonth)}, simplify=FALSE) %>% bind_rows()


aptos <- aptos %>% group_by(id) %>% 
      arrange(desc(fecha_bajada)) %>%
      slice(1) %>% ungroup()

aptos <- aptos %>% mutate_if(is.character, as.factor)

# Filtramos por el criterio en price - eliminamos obs. con price superior al percentil 95%

aptos_todos <- aptos

aptos <- aptos %>% filter(price <= quantile(aptos$price,.95))

# Perdemos esta cantidad de registros

# nrow(aptos_todos) - nrow(aptos)

# vemos prop de NA
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),4)) %>% data.frame() %>% 
      rename(prop_na=".") %>% arrange(desc(prop_na))

#### Definimos variables Sin na imputamos por la media

aptos_sin_na <- imput_media(aptos,p=.1)

aptos_mr <- read_csv(here("mercado_libre/API/datos/limpios/apt","aptos_mr.csv")) 

############ train - test 

set.seed(1234)
ids <- sample(nrow(aptos_sin_na), 0.8*nrow(aptos_sin_na))

train <- aptos_sin_na[ids,]
test <- aptos_sin_na[-ids,]

train_mr <- aptos_mr[ids,]
test_mr <- aptos_mr[-ids,]

############## Cargamos los modelos

#### Arbol

load(here("mercado_libre/modelos/ARBOL","arbol_train.RDS"))

load(here("mercado_libre/modelos/ARBOL","arbol_prune_train.RDS"))

#### RF

load(here("mercado_libre/modelos/RF","RF_train.RDS"))

rf_train_mr <- load(here("mercado_libre/modelos/RF","RF_train_mr.RDS"))


#### Boosting

boosting_train <- load(here("mercado_libre/modelos/BOOSTING","boosting_train.RDS"))

boosting_train_mr <- load(here("mercado_libre/modelos/BOOSTING","boosting_train_mr.RDS"))

#### SVR

#SVR_train <- load(here("mercado_libre/modelos","SVR_train.RDS"))

#SVR_train_mr <- load(here("mercado_libre/modelos","SVR_train_mr.RDS"))


@

\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%% CARATULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\begin{center}

%\includegraphics[width=0.20\textwidth]{img/udelar_logo.jpg}

UNIVERSIDAD DE LA REPÚBLICA

Facultad de Ciencias Económicas y de Administración

Licenciatura en Estadística

Trabajo final de grado

\vspace{2.5cm}

\textbf{\large TÍTULO}

\vspace{1.5 cm}

\textbf{Lucia Coudet}
\textbf{Alvaro Valiño}


\end{center}


\vspace{2cm}

\noindent Tutores:\\
\noindent Natalia Da Silva\\


\vspace{1cm}

\begin{center}

\noindent Montevideo, Fecha.

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESUMEN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 'resumen.Rnw'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables


\setcounter{page}{1} 
 
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCCION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introducción \label{cap:Intro}}

El presente trabajo tiene como principal objetivo la implementación y estudio de diferentes técnicas de aprendizaje estadístico multivariadas, mediante las cuáles poder realizar predicciones de una variable de interés.

Con el fin de llevar a cabo este objetivo, fueron realizadas todas las etapas de un análisis estadístico, desde la obtención de los datos, limpieza y pre-procesamiento, hasta la generación de nuevas variable (ETL y data engeenering).

Uno de los puntos muy importantes de éste trabajo es la obtención y procesamiento de los datos, los cuales fueron obtenidos través de la Interfaz para acceder a la página web (API) de Mercado Libre. Para ello fue necesario la creación de un código que permita una descarga automatizada de la información disponible.

Se destaca que los resultados obtenidos fueron a través del lenguaje y entorno de programación para análisis estadístico y gráfico, \textit{R}, enfocándose en la optimización de todos los procesos principalmente mediante la programación en paralelo. 

En lo que respecta a la técnicas de aprendizaje estadístico, se hizo especial énfasis en árboles de decisión.
Los mismos fueron utilizados no solamente para la predicción de la variable de respuesta sino también para la implementación de técnicas de imputación de valores faltantes.

Asimismo, con el fin de mejorar el desempeño predictivo de los modelos fue realizado un proceso de entraniento y validación de los resultados. 

En la medida que existe un trade off entre interpretación del modelo y poder predictivo, fueron tomados dos enfoques. En primer lugar, se trabajó con técnicas de estimación clásicas tales como el modelo lineal de precios hedónicos. Por otro lado, se implentaron técnicas de aprendizaje automático. El modelo líneal clásico de precios hedónicos permite una interpretación directa de la relación entra la variable de respuesta y las covariables, pero supone la existencia de una relación lineal entre l variable de respuesta y las variables explicativas. No obstante, éste supuesto puede no ajustar a la realidad. Una de las ventajas de las técnicas de aprendizaje automático supervisado es que permiten captar relaciones no lineales. Por su parte, las metodologías como random forest suelen tener un desempeño predictivo superior pero a expensas de una pérdida en el grado de interpretabilidad. Debido a ésto, se suele denominar a éstos modelos como \textit{modelos de caja negra}.

Sin embargo, con el fin de atenuar ésta limitante, se realizó un análisis de interpretabilidad de los modelos. Principalmente, mediante métodos globales modelo-agnósticos. Estos métodos permiten describir el comportamiento medio de los modelos de aprendizaje automático y son particularmente útiles cuando se quiere entender los mecanismos generales en los datos.

Todas las etapas fueron realizando preservando la reproducibilidad de todos los resultados. 

\chapter{Datos \label{cap:Antec}}

\section{Descripción de los datos utilizados \label{sec:desc}}

\section{Obtención \label{sec:obtencion}}

Los datos de precio de oferta de los apartamentos a la venta en Montevideo y todas las covariables utilizadas para modelizar fueron obtenidas a través de la Interfaz para acceder a la página web (API) de Mercado Libre \url{https://www.mercadolibre.com.uy/}. Para ello, es necesario registrarse en la web \url{https://developers.mercadolibre.com.uy/} y desde allí crear una aplicación. Una vez realizado este paso es posible obtener una clave (token) válida por 6 hs que mercado libre proporciona para la conexión a su API.

De ésta manera y como fue mencionado, una vez que se obtiene el token es posible consultar la API. Para obtener la información de todos los inmuebles tanto a la venta como para el alquier en Uruguay es necesario realizar la consulta filtrando la categoría MLU1459. Sobre esta categoría existen distintas subcategorías. A los efectos del interés del presente trabajo fue consultada la API filtrando según la categoría MLU1474 que corresponde a todos los apartamentos a la venta en Uruguay.

Ahora bien, para obtener la información sobre los apartamentos a la venta en el departamento de Montevideo, se consultó la API filtrando según categoría MLU1474 y además filtrando según el id (identificador) de cada uno de los barrios en Montevideo.

Esto permitió obtener numerosos datos. No obstante, fue necesario acceder a los atributos específicos de cada publicación (ítem) filtrando específicamente en cada id de cada publicación. Es decir una vez btenida la información disponible consultando la categoría MLU1474 y filtrando los barrios en Montevideo, se utilizaron los id entonces obtenidos de cada una de las publicaciones y se realizaron consultas por publicación.

De ésta manera fue posible obtener la información de todos los apartamentos a la venta en Montevideo disponible en la API de mercado libre, de manera automatizada. El programa tarda aproximadamente 3 hs en obtener la totalidad de los datos, pudiendo variar según la máquina utilizada.

Para mayor información es posible consultar el código del script funcion api barrios. Disponible en el repositotio público en Github en el siguiebte link: \url{https://github.com/alvarovalinio/TFG/tree/main/mercado_libre/API/funciones}.


\section{Procesamiento y criterios de limpieza\label{sec:procesamiento}}

La obtención y limpieza de los datos fue una parte muy importante de éste trabajo.
En lo que respecta a la limpieza, el Anexo X contiene detalles específicos de los criterios de limpieza seleccionados para cada una de las variable en la base de datos.

En lo que respecta a las variables cuantitativas como precio de oferta del inmueble (price), gastos comunes (maintainance fee), áera cubierta (covered area), etre otras, un punto importante en la limpieza fue tratar de reconocer valores sin sentido, por ejemplo valores que repitan una secuencia de números como 11111, 5555555. Para ello fue construida una función auxiliar que es capaz de detectar cuándo un valor tiene 3 o más números iguales repetidos. En ese caso se considera que el dato es erróneo y en el caso de la variable precio de oferta del inmueble la observación completa es eliminada, mientras que en el caso de gastos comunes se le imputa NA. También se eliminan las observaciones cuyo precio de oferta es inferior al valor del percentil 75\% entre las observaciones con precio inferior a USD 40.000 ya que se consideran datos erróneos.

De manera similar y debido a la elevada presencia de valores atípicos, se eliminan todas las observaciones cuyos valores de la variable price superan el percentil 95\% de la misma.

En lo que respecta a las variables área total (totalarea) y área cubierta (coveredarea) se decidió asignar NA a todos los valores superiores a 1000 metros cuadrados, ya que se consideran datos erróneos. 

Existe un conjunto de variables que toman valor Si, No, NA. Para todas ellas se asume que los NA son No y se realiza la recodificación correspondiente en función de ello.

Valores de latitud y longitud en las georreferencias de los inmuebles que no corresponden a coordenadas geográficas dentro de Montevdeo (latitudes inferiores a -35 y superiores a -34.7 , y longitudes inferiores a -56.5 y superiores a -56 ) son considerados datos erróneos. De esta forma se optó por recodificarlos imputando la coordenada del centroide del barrio donde está ubicado el apartamento. 

Más aun, se pudo detectar la existencia de georreferencias incorrectas ya que en algunos casos la misma no se encontraba dentro del polígono del barrio dónde se ubica el apartamento. Para estos casos, asumimos que el dato correcto es el nombre del barrio y no la georreferencia específica.

Dada la complejidad que implica la detección de éstas georreferencias erróneas, mediante un procedimiento de trade-off entre costo  y complejidad, se optó por utilizar el corte avd italia en continuación con 18 de Julio para detectar éstos datos erróneos. En particular, se comparó la ubicación de la georreferencia respecto a avenida italia y 18 de julio (norte o sur) y la del centroide del barrrio, en caso que no coincidan, se imputó la georreferencia del baricentro de dicho barrio.

Asimismo, se decidió dejar fuera del análisis algunas de las variables con gran porcentaje de NA. Entre ellas cantidad de habitaciones (rooms), garages (parkinglots), unidad (unirfloors), piso (floor), condición nuevo o usado (itemcondition), property age, entre otras.

En lo que respecta al tipo de cambio, los valores del precio de oferta expresados en moneda nacional fueron convertidos a dólares estadounidenses utilizando el tipo de cambio de fecha de bajada de los datos. La obtención del valor del tipo de cambio se realiza de manera automatizada haciendo web scrapping sobre la págine del \textit{Instituto Nacional de Estadistica} (INE).

De manera similar, los gastos comunes expresados en dólares estadounidenses fueron convertidos a pesos uruguayos.

Por mayor información respecto a los criterios de limpieza seleccionados para cada una de las variables disponibles en la base de daros se recomienda dirigirse al Anexo X.

\section{Variables geoespaciales\label{sec:geo}}

La base de datos obtenida contiene información sobre la latitud y longitud dónde está ubicado cada apartamento lo cuál implica tener la georreferencia específica. Esto motivó la elaboración de variables geoespaciales como distancia al shopping más cercano, ubicación respecto a la calle avenida italia en continuación con la calle 18 de Julio, distancia a la Rambla.

\subsection{Zona respecto a avenida italia \label{subsec:zona}}

Toma valor Norte o Sur según el centroide del barrio en el cuál se encuentra disponible el apartamento se encuentra al Norte o al Sur de la calle avenida italia o 18 de Julio. 

\subsection{Distancia al shopping más cercano \label{subsec:shop}}

Fue contruida calculando la distancia en metros cuadrados entre el apartamento y el shopping más cercano. Para ello, fue necesaria la obtención de las coordenadas de todos los shoppings ubicados en montevideo y elaboración propia del archivo shapefile, la cual se detalle en la sección Fuentes externas de información.

Luego se decidió realizar la siguiente recodificación: 
  
\begin{itemize}    
\item Menos de 1 km: distancia menor o igual de 1 km,
\item Entre 1 km y 5 km: distancia superior a 1 km e inferior 5 km, 
\item Más de 5 km: distancia superior a 5 km.
\end{itemize}

\subsection{Distancia a la rambla \label{subsec:rambla}}

Fue construida utilizando la distancia (en m2) entre el apartamento y la rambla este de Montevideo. Para selccionar qué zona de la rambla es la adecuada para diferenciar precio fue llevado a cabo un análisis de árbol de regresión.


\section{Encuesta contínua de hogares\label{sec:ech}}

La variable ingresomedioECH fue construida utilizando la información de la encuesta contínua de hogares (ECH) del año 2020. En particular se utilizó la variable HT11: Ingreso total del hogar con valor locativo sin servicio doméstico (en pesos uruguayos). Se calculó el ingreso medio por barrio de los hogares de Montevideo y luego se asignó a cada observación, el nivel de ingreso medio que le corresponda según el barrio dónde se encuentre el apartamento.

\section{Fuentes externas de información\label{sec:fuentes}}


Para la construcción de las variables geoespaciales fue necesario recurrir a fuentes externas de información. Asímismo y como fue mencionado, en la construcción de la variable ingresomedio ECH fue utilizada también la encuesta contínua de hogares 2020. 

La herramienta utilizada para construir los archivos .kml que permite georreferenciar los shoppings en Montevideo y la calle avenida italia en continuación con 18 de julio fue Google My Maps, el cual es un servicio puesto en marcha por Google en abril del 2007, que permite a los usuarios crear mapas personalizados para uso propio o para compartir. Los usuarios pueden añadir puntos, líneas y formas sobre Google Maps. fuente wikipedia \url{https://es.wikipedia.org/wiki/Google_My_Maps}.

De ésta forma, fueron construidos los archivos .kml que contienen las georreferencias de los shoppings de montevideo y de la calle avenida italia en contrinuación con 18 de julio.

Los shoppings georreferenciados son:

\begin{itemize}      
\item Montevideo shopping center
\item Punta Carretas shopping
\item Tres cruces shopping
\item Nuevocentro shopping
\item Portones shopping
\end{itemize}      

Una vez obtenidos los archivos .kml los mismos son transformados a archivos ESRI Shapefile utilizando QGis la cual es un Sistema de Información Geográfica SIG).

De manera similar fueron construidos los archivos que guardan la información necesaria para la construcción de la variable avenida italia en constinuación con 18 de Julio.

\subsection{Formato ESRI Shapefila\label{subsec:shapefile}}

El formato ESRI (Environmental Systems Research Institute, Inc.) Shapefile (SHP) es un formato de archivo informático propietario de datos espaciales desarrollado por la compañía ESRI, quien crea y comercializa software para Sistemas de Información Geográfica como Arc Info o ArcGIS. Originalmente se creó para la utilización con su producto ArcView GIS, pero actualmente se ha convertido en formato estándar de facto para el intercambio de información geográfica entre Sistemas de Información Geográfica por la importancia que los productos ESRI tienen en el mercado SIG y por estar muy bien documentado.

Un shapefile es un formato vectorial de almacenamiento digital donde se guarda la localización de los elementos geográficos y los atributos asociados a ellos. No obstante carece de capacidad para almacenar información topológica. Es un formato multiarchivo, es decir está generado por varios ficheros informáticos. El número mínimo requerido es de tres y tienen las extensiones siguientes:
      
\begin{itemize}       
\item  shp es el archivo que almacena las entidades geométricas de los objetos.
\item  shx es el archivo que almacena el índice de las entidades geométricas.
\item  dbf es la base de datos, en formato dBASE, donde se almacena la información de los atributos de los objetos.
\end{itemize}       

(Fuete wikipedia https://es.wikipedia.org/wiki/Shapefile)

A continuación s presentan el mapa de Montevideo con la georreferencia de los shoppings y de la calle avenida italia en continuación con 18 de julio. Es importente mencionar que la geometría del departamento de Montevideo fue construida con los archivos shapefile disponibles en la página web del Instituto Nacional de Estadística (INE) en la siguiente dirección: \url{https://www.ine.gub.uy/}.


<<echo=FALSE>>=
# Vectoria INE
mapa_barrio <- st_read(here("mercado_libre/API/scripts_aux/Mapas", "vectorial_INE_barrios/ine_barrios"), quiet = TRUE)
mapa_barrio <- st_transform(mapa_barrio, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

#Puntos shoppings
mall <- st_read(here("mercado_libre/API/scripts_aux/Mapas","puntos_googlemaps/shoppings"), quiet = TRUE)
mall <- st_transform(mall, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
mall <- mall %>% select(Name, geometry)

# Líneas avd_italia
avd_italia <- st_read(here("mercado_libre/API/scripts_aux/Mapas","lineas_googlemaps/avditalia_18"), quiet = TRUE)
avd_italia <- st_transform(avd_italia, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
avd_italia <- avd_italia %>% select(Name, geometry)
@

Mapa del departamento de Montevideo, Uruguay, y georreferencia de los shoppings y calle avenida italia en continuación con 18 de julio

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
      geom_sf() +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
      stat = "sf_coordinates", min.segment.length = 0,
      colour = "black", segment.colour = "black",
      size = 3, alpha = 0.8) +
      xlab('Longitud') +
      ylab('Latitud') 
@


A continuación se presenta el mapa según zona avditalia asignada a cada barrio.



<<echo=FALSE>>=
# Centroide barrios

#devuleve geometría con el centroide de cada barrios
centroide_barrios <- st_centroid(mapa_barrio)

# Extrae coordenadas (longitud y latitud) degeometría del centroide
centroide_barrios <- centroide_barrios %>%
      mutate(lon_barrio = st_coordinates(centroide_barrios$geometry)[,1],
             lat_barrio = st_coordinates(centroide_barrios$geometry)[,2])

# Pasa latitud y longitud del centroide a objeto sf
centroide_barrios_sf <- centroide_barrios %>% 
      st_as_sf(coords = c("lat_barrio","lon_barrio"), crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Tranforma coordenadas a formato long lat
centroide_barrios_sf_t <- st_transform(centroide_barrios_sf,crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

centroide_barrios <- centroide_barrios %>% 
      mutate(aux_lon = NA,
             zona_avditalia = NA)

# Extrae latitud y longitud de puntos en avd_italia (geometria)

puntos_avditalia <- st_coordinates(avd_italia)

puntos_avditalia <- as_tibble(puntos_avditalia) %>% select(-Z, -L1) %>%
      rename('lon_avditalia' = 'X',
             'lat_avditalia' = 'Y')

# Avd italia se conforma en total de 60 puntos
# Para cada barrios tomamos el punto en avd.italia con menor diferencia de longitud
# min {longitud centroide - longitud avd_italia }
# luego comparamos las latitudes del centroide y el punto de avd italia con mínima diferencia en cuanto a longitud
# si latitud avd italia < lat centroide barrio -> NORTE 
# si latitud avd italia >= lat centroide barrio -> NORTE 

for (i in 1:nrow(centroide_barrios)) {
      centroide_barrios$aux_lon[i] <- which.min(abs(centroide_barrios$lon_barrio[i] - 
                                                          puntos_avditalia$lon_avditalia))
      centroide_barrios$zona_avditalia[i] <- ifelse(
            puntos_avditalia$lat_avditalia[centroide_barrios$aux_lon[i]] < 
                  centroide_barrios$lat_barrio[i], 'Norte', 'Sur')
}


centroide_barrios <- centroide_barrios %>% 
      data.frame() %>% 
      select(NOMBBARR, zona_avditalia)

mapa_barrio <- mapa_barrio %>% left_join(centroide_barrios, by = 'NOMBBARR')
@

Mapa del departamento de Montevideo según zona respecto la callle avenida italia en constinuación con 18 de julio

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
                                stat = "sf_coordinates", min.segment.length = 0,
                                colour = "black", segment.colour = "black",
                                size = 3, alpha = 0.8) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@


<<echo=FALSE>>=

load(here("mercado_libre/API/ECH/RDATA_junio2021/HyP_2020_Terceros.RData"))


f <- f %>% select(numero, nper, hogar, nombarrio, HT11, ht13, YHOG, YSVL, lp_06, pobre_06,
             i228, i174, i259, i175, h155, h155_1, h156, h156_1, pesomen) %>%
      filter(hogar == 1)

# Para considerar pesos, multiplicar ingreso hogar i* peso hogar i
# Sum ingreso hogar i * peso hogar i / sum

f <- f %>% 
      group_by(nombarrio) %>%
      summarise(media_ingbarr = sum(pesomen*HT11, na.rm = TRUE) / 
                      sum(pesomen, na.rm = TRUE))

f <- f %>% rename('NOMBBARR' = 'nombarrio')

# quitamos espacios en blanco al final de nombbarr
f$NOMBBARR <- trimws(f$NOMBBARR, which = "right", whitespace = "[ \t\r\n]")

f$NOMBBARR <- recode(as.factor(f$NOMBBARR), 
                 'Malvín' = 'Malvin',
                  'Malvín Norte' = 'Malvin Norte')

mapa_barrio <- mapa_barrio %>% left_join(f, by = 'NOMBBARR')
@

Mapa del ingreso promedio de los hogares por barrio de Montevideo

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = media_ingbarr/1000 )) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_gradient(low = 'firebrick', high = 'darkgreen', name = "Ingreso promedio \n por mil ECH",labels = comma)
@

El anexo X contiene la tabla con el detalle de las variables en la base de datos.


\chapter{Antecedentes \label{cap:Antec}}

\chapter{Marco teórico y metodología\label{cap:MT}}

\section{Supervisado – aprendizaje automatico\label{sec:machinelearning}}

Con el fin de obtener predicciones del precio de oferta de los inmuebles, fueron implementadas diferentes técnicas de aprendizaje automático. Estas consisten en modelar y analizar conjuntos de datos, mediante el aprendizaje de ejemplos, con el fin de predecir y estimar resultados en forma automática.

En este contexto, se realizó un análisis supervisado, en la medida de que se cuenta con una variable de salida ($Y$) y varias variables de entrada ($X$). Por lo tanto, se tiene que los posibles modelos son de la forma:

\begin{center}

$Y=f(x)+\epsilon$

\end{center}

Siendo $f$ una función desconocida y $\epsilon$ un error aleatorio independiente de $X$ e $Y$ con media 0. Se denota a la matriz $X$ de dimensión $n \times p$ a la matriz de datos, donde se tiene $n$ observaciones de entrenamiento y $p$ variables. 

La i-ésima fila se corresponde a la i-ésima observación (perteneciente al conjunto de entrenamiento) siendo de la forma $x_{i}=(x_{i1},\dots,x_{ip})^{T}$, con $x_{i}\in\mathbb R^{p}$. Por otro lado, se denota una nueva observación (o pertenciente al conjunto de testeo) como $x^{*}=(x_{i1}^{*},\dots,x^{*}_{ip})^{T}$, donde está es un vector p-dimensional (al igual que $x_{i}$). 

Se destaca que al ajustar los diferentes modelos se tomó como conjunto de entrenamiento a aproximadamente el $80\%$ de las observaciones. A la hora de estimar $f$, se realizó mediante métodos paramétricos y no paramétricos. 

En el primer caso, se asumió la forma funcional de $f$ y se procedió a estimar sus respectivos parámetros. Por otro lado, en los métodos no paramétricos, no se asumió la forma funcional de $f$.

\subsection{Modelo lineal de precios hedónicos\label{subsec:ml}}

El modelo lineal de precios hedónicos parte del supuesto de que los precios observados de los productos se pueden desglosar en una suma de cantidades específicas de determinadas características asociadas al bien lo cual define un set implícito de precios, también conocidos como precios hedónicos.

De ésta forma, el precio del bien es regresado sobre las características del mismo, y utilizando técnicas clásicas de estimación se obtienen los anteriormente mencionados precios hedónicos.

Formalmente, el modelo se especifíca como:

$$y = f(x) + \epsilon$$

dónde 

$$f(x) = \mathbf{X'\beta} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p$$

$\epsilon$ es el vector de errores del modelo, con $\epsilon_i$ independientes e identicamente distribuidos $N(0, \sigma^2)$.

Siendo $(X_1, \cdots, X_p)$ el vector de las $p$ características asociadas al bien y $(\beta_1, \cdots, \beta_p)$ el vector de los precios hedónicos. Es importante observar que el vector de precios hedónicos asociados a las características coincide con el vector de parámetros de un modelo líneal clásico.

En particular, éste modelo puede aplicarse a los precios de los bienes inmuebles.
Entre las características asociadas al bien pueden considerarse características que son propias del mismo así como también características asociadas a la geolocaliación, entre otras. (De Bruyne, K., Van Hove, J., 2013).

Una de las ventajas más importantes de éste tipo de modelos es la fácil interpretación. No obstante suelen tener una mala performance predictiva en comparación a otros enfoques ya que puden presentar problemas de heteroscedasticidad, multicolinealidad, y variables omitidas.

El modelo de precios hedónicos puede ser generalizado para el caso no lineal, lo cual no ha sido implementado en el presente trabajo.


\subsection{Árboles de regresión\label{subsec:arbol}}

Luego de relizar la implemetación del modelo lineal de precios hedónicos, se procedió a modelar mediante un árbol de decisión. En la medida de que se cuenta con una variable de salida continua, se construyó un árbol de regresión.

A pesar de que en la literatura existen diversos enfoques para la construcción de estos modelos, se trabajó con el método \textit{CART} el cual fue propuesto por \textit{Breiman}, \textit{Friedman}, \textit{Olshen} y \textit{Stone} en 1984.

Éste método se caracteriza por la realización de particiones binarias recursivas del espacio de las variables de entrada. Mediante las mismas, se conforma una organización jerárquica en forma de árbol, donde en cada nodo interior se tiene una pregunta (dicotómica) sobre una variable de entrada y en cada nodo terminal (denominado "hoja") una decisión.

De está forma, se procede a dividir el conjunto de los valores posibles de $X_{1}\dots,X_{p}$ (variables de entrada) en $J$ regiones disjuntas $R_{1},\dots,R_{J}$.(James, 2013)

Luego para cada observación que se encuentra en la región $R_{j}$ se realiza la misma predicción. Siendo está, en el contexto de árboles de regresión, el promedio de la variable respuesta en dicha región.

En el momento de la construcción de las regiones ($R_{1},\dots,R_{J}$) se realiza de tal forma que en cada subconjunto resultante (denominados como "nodos hijos") en cada iteración implique una disminución en la impureza de estos.
Para ello, se construyen las regiones $R_{1]$, \dots, $R_{j]$ de forma tal que minimicen la suma de cuadrados de los residuos (o por sus silabas en ingles \textit{RSS}).  
\begin{center}

$\displaystyle \sum^{J}_{j=1} \sum_{i\  \in R_{j}} \left(y_{i} - \hat{y}_{R_{j}} \right) ^{2}$ (Hastie, 2001)

\end{center}

Siendo $\hat{y}_{R_{j}}$ el promedio de la variable respuesta en la j-ésima región.
Para lograr este cometido se utiliza una separación recursiva binaria de la siguiente forma. Se selecciona la variable $X_{j}$ y el número $s$ dividiendo el espacio en dos regiones $R_{1}(j,s) = \lbrace{ X : X_{j} < s \rbrace}$ y $R_{2}(j,s) = \lbrace{ X: X_{j} \geq s \rbrace}$  de forma tal que se haga mínimo 

\begin{center}

$\displaystyle \sum_{i:x_{i} \in R_{1}(j,s)} \left(y - \hat{y}_{R_{1}}\right)^{2} + \sum_{i:x_{i} \in R_{2}(j,s)} \left(y - \hat{y}_{R_{2}}\right)^{2}$

\end{center}

Una vez encontrada la mejor partición se separan los datos en las regiones resultantes y se repite el proceso en cada región. Es decir, se busca nuevamente la mejor variable y el mejor punto de corte de forma se incremente la disminución de la impureza en los nodos hijos.

El proceso continúa hasta que se satisfaga algún criterio de parada. Un criterio de para puede ser por ejemplo que los nodos terminales tengan cierto número de observaciones.

Luego de definido el criterio de construcción de las regiones y el criterio de parada, se procede a realizar un proceso de poda en el árbol obtenido basado en un criterio de \textit{costo-complejidad}. Esto en la medida de que si se deja crecer el árbol de forma indefinida se obtiene un modelo con un alto grado de sobre ajuste (\textit{overfitting}). Por su contraparte, un árbol muy "pequeño", posiblemente no logre capturar la estructura del conjunto de datos. (Hastie, 2001).

El proceso de poda realizado, consiste en dejar crecer el árbol hasta que los nodos terminales tengan cierto número de observaciones (dicho árbol se denota como $T_{0}$). Luego se elige aquel subárbol el cual posee un menor error de predicción en el conjunto de testeo. En la medida de que un procedimiento de \textit{cross-validation} aplicado en cada posible subárbol es muy costoso en términos de "tiempo computacional", surge como alternativa el método basado en un criterio de \textit{costo-complejidad}.

En dicho método se define a $T_{\alpha}$ como un subárbol obtenido al podar a $T_{0}$. De esta forma, para cada $\alpha$ se busca $T_{\alpha}$ que minimice la siguiente expresión:

\begin{center}

$C_{\alpha}(T)=\displaystyle \sum^{|T|}_{m=1} N_{m} Q_{m}(T) + \alpha|T|$ (Hastie, 2001)

\end{center}

Donde se tiene que $|T|$ es igual número de nodos terminales del árbol $T$, mientras que $N_{m}$ es el número de observaciones en la región $R_{m}$. Por otro lado, la expresión $Q_{m}(T)$ consiste la medida de impureza.

En cuanto al parámetro $\alpha$, el mismo consiste en un parámetro de penalización aplicado a la complejidad (tamaño) del árbol. Donde valores altos de este, penalizan a árboles de gran tamaño. De esta forma, controla el compromiso entre la complejidad y la bondad de ajuste del modelo. Dicho parámetro se estima mediante \textit{cross-validation}.

\subsection{Bagging - Ranfom Forest\label{subsec:rf}}

A pesar de que los árboles de regresión poseen un alto grado de interpretabilidad, estos poseen la gran limitante de ser inestables. Esto en el sentido de que pequeñas variaciones en el conjunto de entrenamiento y testeo generan grandes cambios en las estimaciones.
Por lo tanto, se emplearon diferentes métodos alternativos buscando estabilidad en las predicciones.

En primer lugar, se aplicó el método \textit{Random Forest} desarrollado por \textit{Breiman} en 1994. Este método consiste en construir un estimador combinando distintas versiones de estimadores.
En este contexto, estas nuevas versiones se construyen generando nuevos conjuntos de entrenamiento, mediante la técnica de remuestreo \textit{bootstrap}. Ésta técnica consiste en la generación de varias muestras con reemplazo, del conjunto de datos de entrenamiento, donde a cada observación se le asigna el mismo peso ($\frac{1}{n}$, siendo $n$ el número de observaciones). Al número de muestras \textit{bootstrap} se le suele denotar con la letra $B$.
A la hora de utilizar este método en problemas de regresión, se procede a tomar varias muestras \textit{bootstrap}, donde a partir de cada una de ellas se construye un estimador. Luego, se le asigna a la observación el promedio de las respuestas de los estimadores construidos en cada muestra. 

Este método, en el contexto de árboles de regresión, consiste en la creación de $B$ árboles, cada uno mediante un nuevo conjunto de entrenamiento obtenido mediante una muestra \textit{bootstrap}. Se destaca que a estos árboles no se le realiza un proceso de poda. Por lo que estos mismos presentan una gran varianza, pero bajo sesgo. Sin embargo, al predecir mediante un promedio de los $B$ árboles, se logra una reducción considerable en la varianza del estimador y de esta forma se mejora la precisión de la predicción.

A su vez, el algoritmo a la hora de construir los diferentes estimadores (árboles), no considera en cada división el total de variables, sino un subconjunto de estas elegido de forma aleatoria. Como primera aproximación se procedió a utilizar la parte entera de $\sqrt{p}$, siendo $p$ el número de variables. Esto en la medida de que es el número de variables que utiliza por defecto la función \textit{ranger()} del paquete {ranger}. En etapas posteriores del análisis, se modifico el valor del mismo con el fin de obtener un mejor poder predictivo.

Este último punto es lo que diferencia al algoritmo con su versión más simple denominada \textit{Bagging} (también desarrollada por \textit{Breiman}). En este último se considera en cada división el total de las variables, por lo que resulta ser un caso particular del método \textit{Random Forest}.

Se optó por trabajar con el método \texit{Random Forest} ya que se destaca sobre el método \textit{Bagging} principalmente cuando se tiene que una variable es muy influyente. Esto se debe a que si se consideran todas las variables a la hora de construir los diferentes $B$ árboles, en la medida de que se tiene una variable muy influyente, posiblemente dichos árboles no difieran mucho entre sí. Esta limitante no se presenta en \textit{Random Forest} en el sentido de que selecciona de forma aleatoria un subconjunto de los predictores en cada iteración.

Una caracteristica relevante del método \textit{Random Forest} (al igual que en \textit{Bagging}), es que cada observación posee una probabilidad de aproximadamente $\frac{2}{3}$ de ser seleccionada en cada remuestra realizada. De esta forma, se cuenta con un conjunto de observaciones las cuales no son utilizadas para construir el estimador.

Este conjunto de observaciones se denomina como \textit{out of bag observations} (\textit{OOB}). Por lo tanto, en cada iteración se procede a predecir dichas observaciones, mediante el estimador obtenido. Repitiendo este procedimiento para las $n$ observaciones, se calcula el \textit{error OOB}. Dicha medida se procedió a utilizar como una primera aproximación en cuanto a la performance predictiva del modelo.

A pesar de que el método anteriormente mencionado logra solucionar el problema de la inestabilidad por parte de los árboles de decisión, este método se caracteriza por presentar una baja interpretabilidad. 

Sin embargo, en la medida de que se construyen varios árboles, es posible obtener cierta medida de la importancia de cada predictor. En los algoritmos \textit{Bagging} y \textit{Random Forest} se calcula la reducción de la medida de impureza en las divisiones de una variable dada promediando en todos los árboles obtenidos. De esta forma, si la reducción es "grande" la variable se considera "importante".

\subsection{Boosting\label{subsec:boosting}}

Por otro lado, el tercer método empleado consiste en el procedimiento \textit{Boosting} aplicado, nuevamente a árboles de decisión. Este método, al igual que los anteriores, consiste en la combinación de la salida de varios estimadores con el fin de producir un estimador más preciso.

Sin embargo, el mismo difiere con los métodos anteriores en la forma de realizar este proceso. En el método \textit{Boosting}, se construye una sucesión de estimadores, los cuales surgen de forma iterativa usando una modificación del conjunto de datos realizada a partir de la performance del estimador en el paso anterior.

Boosting tiene 3 tuning parameters:


1. El número de árboles $B$. A diferencia de bagging y random forest, boosting puede generar sobreajuste a los datos en caso que $B$ sea grande, a pesar de que éste sobreajuste ocurra de manera lenta. Se utiliza validación cruzada para estimar a $B$.

2. El parámetro $\lambda$ que controla la tasa a la que aprende el algoritmo. $\lambda$ suele ser un número positivo pequeño, usualmente 0.01 o 0.001. La selección correcta de éste parámetro depende del problema específico. Por lo general, cuánto menor el valor de $\lambda$, mayor el $B$ necesario.

3. El número de particiones en cada árbol $d$ que controla la complejidad de cada estimador. Usualmente un valor de $d=1$ funciona bien. Esto implica que cada árbol tenga solamente una partición. En éste caso lo que se tiene entonces es un modelo aditivo ya que cada término involucra una sola variable. $d$ puede ser intepretado también como el parámetro que controla el orden de interacción (interaction depth) entre los modelos, ya que las $d$ particiones pueden involucrar a los sumo $d$ variables.

Algoritmo 

1. Se establece $\hat{f}(x) = 0$ y $r_i = y_i$ en el set de entrenamiento.

2. Para cada $b = 1, 2, \cdots, B$ repite:

a. Se ajusta un árbol $\hat{f}^b(x)$ con $d$ particiones (es decir, $d+1$ nodos terminales) en los datos de entrenamiento (X,r).

b. Se actualiza $\hat{f}$ agregando el nuevo árbol en una versión reducida:

$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$

c. Se actualizan los residuos 

$$r_i \leftarrow r_i - \lambda \hat{f}^b(x_i) $$

3. Se general el modelo

$$\hat{f}(x) =  \displaystyle \sum_{b = 1}^B \lambda \hat{f}^b(x)$$


\subsection{Support Vector Regression (SVR)\label{subsec:svr}}


Los modelos denominados \textit{Support Vector Regression} (SVR), surgen como una generalización aplicada a problemas de regresión de los modelos \textit{Support Vector Machine} (SVM). 

Por lo tanto, al ser una generalización de los SVM (en problemas de clasificación), poseen características muy similares, principalmente la robustez en cuanto a observaciones atípicas. De esta forma, se tiene que los SVR pertenecen al "grupo"  denominado \textit{robust regression}, donde en estos métodos se busca minimizar el efecto de observaciones atípicas en la ecuación de regresión. (Kunh-Johnson, 2013)

Estos métodos surgen como altenartiva a los modelos de regresión lineal, ya que estos ultimos a la hora de estimar los parámetros buscan minimizar la suma de cuadrados residuales (SSE). Lo cual conlleva que una observación que no sigue la tendencia del resto, puede ser muy influyente. (Kunh-Johnson, 2013)

A pesar de que existen varios enfoques para llevar acabo SVR en este trabajo se centró en el denominado $\epsilon$-\textit{insensitive regression} (Kunh-Johnson, 2013). En este contexto, a la hora de obtener las estimaciones de los parámetros del modelo, se define una nueva función de perdida denominada $\epsilon$-\textif{insensitive loss function}, siendo de la forma:

\begin{center}

$L(y,f(x,\alpha))=L(|y-f(x,\alpha)|_{\epsilon})$ (Vapnik, 2000)

\end{center}


\begin{center}

$|y-f(x,\alpha)|_{\epsilon} = \begin{cases} 0, & \mbox{si } |y-f(x,\alpha)| \leq \epsilon \mbox{} \\ |y-f(x,\alpha)|-\epsilon, & \mbox{en otro caso } \mbox{} \end{cases}$     (Vapnik, 2000)

\end{center}

En función a la ecuación anterior, se tiene que la perdida es igual a 0 si la discrepancia entre los predicho y lo observado es menor a $\epsilon$, siendo $\epsilon$ un limite fijado de antemano. Por lo tanto se tiene que tanto los outliers, como las observaciones que poseen un buen ajuste (residuos pequeños), no tienen efecto en la ecuación de regresión. 

En este contexto, para estimar los parámetros del modelo, SVR utiliza la función de perdida anteriormente definida, pero a su vez considerando un parámetro de penalización. En dicho método se busca obtener los coeficientes que minimizan la siguiente expresión:

\begin{center}

$C \displaystyle \sum_{i=1}^{n}L(|y_i-f(x_i,\beta)|_{\epsilon}) + \displaystyle \sum_{j=1}^{P} \beta^{2}_{j}$ (Kunh-Johnson, 2013)


\end{center}

Donde el parámetro $C$ es un parámetro de penalización, el cual generalmente se estima mediante cross-validation. En este contexto el parámetro $C$ cumple un rol de indicar la complejidad del modelo. Conforme aumenta el valor de éste el modelo obtiene mayor flexibilidad, en la medida que el efecto de los errores es aumentado. Por otro lado, al disminuir este parámetro el modelo se vuelve más rígido y con menor posibilidad de sobre ajustar a las observaciones.

Luego, se tiene que la solución al problema de minimización anteriormente mencionado, involucra el producto escalar entre las observaciones y no a las observaciones en si (Hastie, 2017). De esta forma, se puede re expresar a la función de regresión mediante la siguiente expresión:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} \langle x^{*},x_{i}\rangle$ (Kunh-Johnson, 2013) 

\end{center}

De esta forma, se tiene que para evaluar $f(x)$ es necesario el cálculo del producto escalar entre la nueva observación ($x^{*}$) y cada una de las observaciones pertenecientes al conjunto de entrenamiento. A su vez, se cuenta con $n$ parámetros $\alpha_{i}$ con $i=1,\dots,n$, donde cada uno corresponde a una observación de entrenamiento.

Sin embargo, en SVR se tiene la propiedad de que solo un subconjunto de los datos tiene un rol activo en la predicción de una nueva observación. Esto en la medida de que los parámetros $\alpha_{i}$ asociados a las observaciones de entrenamiento las cuales se encuentran a $\pm\ \epsilon$ de la recta de regresión (es decir se encuentran dentro del intervalo de longitud $2 \epsilon$ alrededor de la recta de regresión) son iguales a $0$. (Kunh-Johnson, 2013)

A las observaciones las cuales determinan a la recta de regresión se les denomina support vectors. Además, en la medida de que el predictor se encuentra sujeto al producto escalar entre la nueva observacion y las observaciones de entrenamiento (en particular solo aquellas que sean support vectors), se puede generalizar con el fin de captar relaciones no lineales entre las variables. 

Para ello se utiliza una función denominada \textit{kernel} la cual permite agrandar el espacio original de las variables, con el fin de obtener relaciones lineales en un nuevo espacio de mayor dimensión (James, 2013). Esta función es una generalización del producto escalar y se denota de la siguiente forma:


\begin{center}

$K(x_{i},x_{j})$ (James, 2013)

\end{center}

De esta forma el predictor queda expresado como:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} K(x^{*},x_{i})$ (Kunh-Johnson, 2013)

\end{center}

A la hora de aplicar SVR existen diferentes kernels lo cuales se podrían utilizar. Uno de lo más utilizados en la bibliografía se denomina \textit{radial kernel}. Este es de la forma:

\begin{center}

$K(x^{*},x_{i})=exp \left(-\gamma \displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}  \right)$, $\gamma>0$ (James, 2013)

\end{center}

En donde si la observación $x^{*}$ se encuentra lejos de la observación $x_{i}$ en terminos de distancia euclidia, entonces se tiene que $\displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}$ es una cantidad grande y por consecuente $K(x^{*},x_{i})$ es pequeño. Por lo tanto, $x_{i}$ no va a tener un rol activo a la hora de predecir el valor de $x^{*}$.

Esto significa que el radial kernel posee un comportamiento local, en el sentido de que  las observaciones de entrenamiento cercanas tienen un mayor efecto en la predicción del valor de una nueva observación.

Por otro lado, se tiene que $\gamma$ es un parámetro de escala, el cual afecta la varianza en la estimación. Al igual que $C$, dicho parámetro generalmente se estima mediante cross-validation.

Luego, se destacan dos aspectos de los modelos SVR. En primer lugar, en el caso de que la relación entre las variables sea realmente lineal (problemas de regresión), se recomienda usar un linear kernel (producto escalar) sobre un radial kernel. (Kunh-Johnson, 2013)
 
A su vez, en la medida de la ecuación de regresión ($f(x)$) se expresa a través del producto escalar entre las observaciones, se recomienda estandarizar las mismas con el fin de tener una misma unidad de medida. (Kunh-Johnson, 2013)


\section{Cross validation e Hyperparameter tuning \label{sec:cv}}

A la hora de evaluar la performance de los diferentes modelos planteados, se realizó un procedimiento de \textit{cross-validation}, particularmente \textit{k-folds}.
El algoritmo consiste en dividir la muestra en $k$ submuestras de igual tamaño. Luego $k-1$ submuestras se usan como datos de entrenamiento y la muestra restante $k$ se usa para testear los datos.
A continuación, se procede a ajustar los datos de esa muestra con el modelo construido con las $k-1$ muestras. Donde el proceso se repite $k$ veces, con cada una de las $k$ muestras. De tal forma que cada $k$ muestras es utilizada una sola vez como datos de testeo. 
De esta forma, todas las observaciones se usan tanto para train como para test. A su vez, cada observación se usa para test una sola vez y para train $k-1$ veces. Los errores obtenidos en cada etapa se promedian para producir una sola estimación (error medio obtenido de los $k$ análisis realizados).
Con el fin de medir el error de predicción del modelo en los modelos planteados anteriormente se consideró la siguiente medida:

\begin{center}

$RMSE= \displaystyle \frac{1}{k} \sum_{k=1}^{K} RMSE_{k}$

\end{center}

Donde $RMSE_{k}$ es la raiz del error cuadratico medio en la k-ésima muestra.

\begin{center}

$RMSE_k= \displaystyle \sqrt{ \frac{1}{n_k} \sum_{j = 1}^{n_k} (y_j - \hat{y}_j)^{2}}$

\end{center}

dónde $n_k$ es la cantidad de observaciones en la k-ésima muestra. 

Con el fin de obtener el modelo con la mejor performance predictiva, se realizó un proceso de \textit{hyperparameter tuning}. El mismo consiste en obtener la mejor combinación de hiperparámetros posibles mediante una metodología de cross-validation.

Un hyperparámetro es una valor necesario para ajustar un modelo el cual no se determina a partir 
de los datos sino que, por el contrario, es necesario que sea especificado previamente a la realización del ajuste. Dependiendo del algoritmo con el que se trabaje, el rol de los mismos puede variar. Por ejemplo, como fue mencionado para el caso de Random Forest se tiene 2 hyperparámetros de gran importancia: la cantidad de variables que son seleccionadas para ajustar cada arbol y la cantidad de árboles. 

\section{Interpretabilidad \label{sec:cv}}

Una vez implementados los diferentes algoritmos de aprendizaje automático y realizado el proceso de hiperparameter tunning, se procedió a obtener una medida de interpretabilidad de los mismos. Para ello, se trabajó con métodos globales modelo - agnosticos de interpretación, aplicados a aquel modelo con mejor performance predictiva y haciendo hincapié en el análisis gráfico.

Estos métodos de interpretación para modelos de caja negra, consiste en describir el compartamiento promedio del modelo. Donde, los mismos generalmente se expresan mediante un valor esperado, basado en la distribución de los datos. (Molnar, 2021)

A pesar de que en la literatura existen diferentes métodos para llevar a cabo el análsis, se trabajó mediante dos aproximaciones. Como primera aproximación, se realizó el análisis mediante los gráficos denominados \texit{Partial Dependece Plot} (PDP). Luego, con el fin de realizar un análisis con mayor profundidad y en la medida de que la metodología anterior presenta ciertas limitantes, se trabajó mediante el estudio de \texit{Accumulated local effect plots} (ALE).

En las siguientes subsecciones se detallan los principales aspectos teóricos de ambas metodologías, al igual que sus ventajas y limitantes.

\subsection{Partial Dependence Plot (PDP) \label{subsec:pdp}}

Los gráficos denominados partial dependence plot (PDP) permiten observar el efecto marginal que una o dos variables tienen sobre la predicción de la variable de respuesta obtenida a través de un algoritmo de machine learning (Molnar, 2021). Estos gráficos pueden detectar cuando la relación entre la variable de respuesta y la variable predictora de interés es líneal, monótona, o bien cuando se trata de una forma funcional más compleja. 

La función \textit{partial dependence function} para el caso de regresión se define como:

$$\hat{f}_{x_S}(x_S)=E_{x_C}\left[\hat{f}(x_S,x_C)\right]=\int\hat{f}(x_S,x_C)d\mathbb{P}(x_C)$$

En dónde $x_s$ son las variables para las cuales se quiere conocer el efecto sobre la predicción, mientras que $x_c$ corresponde al resto de las variables utilizadas en el algoritmo de machine learning $\hat{f}$.

Ahora bien, la estimación para la función anterior se obtiene mediante la métodología Monte Carlo promediando sobre la muestra de entrenamiento:

$$\hat{f}_{x_S}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$

en dónde $x_c^{(i)}$ son los valores en la base de datos para las variables en las cuáles no estamos interesados, y $n$ el número de observaciones.

Es importante mencionar que PDP es un método global que permite determinar de manera global la relación entre una o dos variables predictoras sobre la variable de respuesta. Asimismo, interesa destacar que uno de los supuestos de PDP es que las variables en $x_c$ y $x_s$ no están correlacionadas (Molnar, 2021).

De manera similar se obtiene una estimación PDP para el caso de variables categóricas en dónde, para cada categoría, se realiza la estimación PDP forzando a que todas las observaciones tomen el valor correspondiente a dicha categoría.

\subsection{Accumulated Local Effects (ALE) Plot \label{subsec:aleplot}}


\subsection{Tratamiento de valores faltantes \label{subsec:nas}}

La base de datos construida contiene variables con diferentes grado de datos faltantes. 

Éste problema fue abordado siguiendo dos estrategias de imputación. En primera instancia se entrenan diferentes modelos imputando solamente a las variables numéricas que tienen proporción de valores faltantes inferior a 0.15, y se procede en ésta primera instancia a realizar imputación por la media, lo cual es equivalente a asumir que el proceso generador de datos de éstos valores es un proceso aleatorio. Es decir, que los datos faltantes son generados al azar.

Posteriormente, se realizó un proceso de imputación de valores faltantes mediante un análisis supervisado. Para ello se trabajó de la siguiente manera:

1. Se ajustó un modelo tomando como variable de salida cada variable con datos faltantes.
2. En cada uno de ellos, se consideró como variables de entrada todas aquellas que originalmente no presentan datos faltantes, pero excluyendo la variable precio.
3. En cada variable, se sustituyó cada dato faltante por su predicción utilizando el modelo ajustado.

En particular, el algoritmo utilizado para ajustar los modelos fue Random Forest, mediante el paquete missRanger. En lo que respecta a hyperparameter tuning, se destaca que debido al tiempo computacional que conlleva fueron utilizados en todos los casos los valores por defecto.

Como fue mencionado anteriormente el criterio genérico para seleccionar las variables a imputar fue según la proporción de valores faltantes (proporción inferior a 0.15). Sin embargo, ésta metodología de imputación, a diferencia de la anterior, permite realizar el proceso para variables de tipo cualitativas. De esta forma, se incluyó en el análisis la variables item condition. No obstante, si bien existen otras variables que podrían ser incluídas, se decidió dejarlas fuera del análisis en la medida que no se consideran relevantes para el mismo (principalmente por no estar balancedas).

\chapter{Análisis exploratorio de  datos \label{cap:EDA}}

En esta sección se presentan las principales características de los datos con los cuales se trabajó. La base de datos se conforma por un total de \Sexpr{paste(dim(aptos)[1])} observaciones y \Sexpr{paste(dim(aptos)[2])} variables de las cuales, una vez realizado el proceso de tratamiento de datos faltantes se cuenta con un total de 24 variables en el caso de imputación por la media, y 25 en el caso de imputación por Random Forest. En la tabla bla del anexo bla se detalla la proporción de observaciones con datos faltantes por variables.

En primer lugar se procedió a analizar el comportamiento de la variable de salida (precio). En forma de resumen se presenta a continuación la tabla con las principales medidas de resumen: 

<<>>=
sum.table <- aptos %>% summarise(Min = round(min(price),0),
                    Q1=round(quantile(price,.25),0),
                    Mediana = round(median(price),0),
                    Media = round(mean(price),0),
                    Q3 = round(quantile(price,.75),0),
                    Max = round(max(price),0),
                    Desvio = round(sd(price),0)
                    )

library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
sum.table <- xtable(sum.table)
align(sum.table) <- rep("l", ncol(sum.table)+1)
print(sum.table, include.rownames=FALSE, format.args = list(big.mark = ","))
@

Luego, se presenta gráficamente la distribución de la variable mediante un histograma.

\begin{figure}
\centering
<<fig = TRUE, out.width="0.3\\textwidth">>=
aptos %>% ggplot(aes(x=price)) + 
       geom_histogram(fill = 'navyblue', alpha = 0.9, bins = 40) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_text(face = 'bold')) +
      labs(x = 'Precio de oferta') +
      scale_fill_manual(values = c('navyblue')) +
      geom_vline(xintercept = mean(aptos$price, na.rm = TRUE), 
                 colour = 'firebrick') +
      annotate("text", x = mean(aptos$price, na.rm = TRUE) + 72000, y = 5500, label = paste0("Media = USD ",round(mean(aptos$price, na.rm = TRUE))), color = 'firebrick') +
      scale_x_continuous(label = comma)
@
\captionof{figure}{Histograma del precio de oferta ($price$) de los apartamentos a la venta en Montevideo. El precio promedio es de USD \Sexpr{paste(round(mean(aptos$price, na.rm = TRUE)))}}
\end{figure}

En lo que respecta a las variables de entrada, en primer lugar se presenta análisis de las variables cualítativas que se consideraron más relevantes. A continuación se presentan los gráficos de barras con el fin de observar la distribución de los niveles en los datos.

<<>>=
niveles <- aptos %>% group_by(bedrooms) %>% summarise(n = n())
colores <- data.frame(colores= c("gold1","darkviolet","green4", "dodgerblue2", "firebrick"))
niveles <- niveles %>% arrange(n)
niveles$id <- 1:nrow(niveles)
colores$id <- 1:nrow(niveles)
niveles <- left_join(niveles,colores,by="id")

niveles <- niveles %>% arrange(bedrooms)

p1 <- aptos %>% ggplot() +
  geom_bar(aes(x = bedrooms, y = (..count..)/sum(..count..), fill = bedrooms)) +
  theme(axis.ticks.x = element_blank(),
        legend.position = 'none',
        axis.title.y = element_blank(),
        axis.title.x = element_text(face = 'bold', size = 10),
        axis.text.x = element_text(face = 'bold')) + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(x = as.factor(bedrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
  labs(x = 'Cantidad de dormitorios') +
  scale_fill_manual(values = niveles$colores)

p2 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(full_bathrooms)), y = (..count..)/sum(..count..), fill = full_bathrooms)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(full_bathrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Cantidad de baños completos') +
      scale_fill_manual(values = c('aquamarine3','pink3'))

p3 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(zona_avditalia)), y = (..count..)/sum(..count..), fill = zona_avditalia)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(zona_avditalia),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Zona respecto a avenida italia') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))

p4 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(has_swimming_pool)), y = (..count..)/sum(..count..), fill = has_swimming_pool)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(has_swimming_pool),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'El edificio tiene piscina') +
      scale_fill_manual(values = c('mediumpurple2', 'salmon3'))

@

\begin{figure}
\centering
<<fig = TRUE, fig.pos = 'h'>>=
grid.arrange(p1, p2, p3, p4, ncol = 2)
@
\captionof{figure}{Gráficos de barras para diferentes variables cualitativas. En panel superior se encuentran a la izquierda la variable \text{bedrooms} y a la derecha la variable \text{full\_bathrooms}. Por otro lado, en el panel inferior a la izquierda se encuentra la variable \text{zona\_avditalia} y a la derecha \text{has\_swimming\_pool}}
\end{figure}

Con el fin de observar la distribución de algunas de las variables graficadas anteriormente a la vez que evaluar si las variables podrían ser candidatas a variables explicativas del modelo, se presentan a continuación algunos gráficos de violín y gráficos de caja.

<<>>=
p5 <- aptos %>% 
      ggplot(aes(x=as.character(zona_avditalia), y=price,
                 fill=as.character(zona_avditalia))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 14),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 12),
            axis.text.y = element_text(face = 'bold', size = 12),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('Precio de publicación') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('orangered2', 'springgreen4'))

p6 <- aptos %>%
      filter(!is.na(full_bathrooms)) %>%
      ggplot(aes(x=as.character(full_bathrooms), y=price, 
                 fill=as.character(full_bathrooms))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 14),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 12),
            axis.text.y = element_text(face = 'bold', size = 12),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('Precio de publicación') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('yellow4', 'palevioletred'))
@


\begin{figure}
\centering
<<fig = TRUE>>=
grid.arrange(p5, p6, ncol =3)
@
\captionof{figure}{Gráficos de violín y gráficos de caja. A la izquierda se encuentra el gráfico de price según \text{zona\_avditalia} mientras que a la derecha se encuentra el gráfico de price según \text{full\_bathrooms}. Es posible observar que los gráficos sugieren la existencia de una diferencia en media de price según \text{zona\_avditalia}, teniendo una media superior los apartamentos que se encuentran al Sur de avenida italia. A su vez, los apartamentos con dos o más baños completos tienen una media superior respecto a los apartamentos con 1 solo baño completo.}
\end{figure}

Ahora bien, con el objetivo de evaluar posibles interacciones sobre las variables anteriormente graficadas, se presenta el gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}.


\begin{figure}
\centering
<<fig = TRUE, out.width = '70%', fig.pos = 'h'>>=
aptos %>% ggplot(aes(y=price, group = full_bathrooms, fill = full_bathrooms)) + 
       geom_boxplot() +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_blank()) +
      labs(x = 'Precio de oferta') +
      scale_fill_manual(values = c('orangered2', 'springgreen4')) +
      facet_grid(full_bathrooms~zona_avditalia) +
      scale_y_continuous(label = comma)
@
\captionof{figure}{Gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}. El gráfico sugiere que las diferencias en media observardas se mantienen cuando condicionamos en \text{full\_bathrooms}. En particular, entre los apartamentos con 2 o más baños completos. se observa una media superior para los apartamentos ubicados al sur respecto a los apartamentos ubicados al Norte. En lo que respecta a los apartamentos con un sólo baño completo, el gráfico sugiere que los apartamentos al Sur poseen una media superior respecto a los ubicados al Norte, si bien la diferencia es menos en éste último caso.}
\end{figure}

\begin{figure}
\centering
<<fig = TRUE>>=
aptos %>% 
      ggplot(aes(y=price,x=dist_shop,fill=zona_avditalia)) + 
      geom_boxplot() +      
      theme(axis.ticks.x = element_blank(),
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text()) +
      labs(y = 'Precio de oferta', x = 'Distancia al shopping más cercano') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))  +
      scale_y_continuous(label = comma)      
@
\captionof{figure}{Gráfico de caja de \text{dist\_shop} segun \text{zona\_avditali}a. En todos los casos los apartementos ubicados al Sur tienen una media superior. Asimismo, el gráfico sugiere que entre los apartamentos ubicados al Norte, los apartamentos ubicados a menos de 1 km de un shopping tienen una media superior.}
\end{figure}


\begin{figure}
<<fig = TRUE>>=
aptos_cor <- aptos %>% select(price, maintenance_fee, dist_rambla, covered_area,
                              total_area, no_covered_area) 

corr<-round(cor(aptos_cor, use='pairwise.complete.obs') , 2)

ggcorrplot(corr, method = c("square"),type=c("upper"), 
           ggtheme = ggplot2::theme_gray,lab=TRUE)
@
\captionof{figure}{Mapa de correlación. Como es de esperar, precio y distancia a la ramble éste de montevideo estan correlacionadas negativamente.}
\end{figure}

Por último, se presentan algunos gráficos que permiten observar la relación entre el precio de los apartamentos, el área cubierta, y la variable zona_avditalia.

\begin{figure}
\centering
<<fig = TRUE>>=
ggpairs(aptos, c(3,14), mapping = ggplot2::aes(color = zona_avditalia, alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.5)))
@
\captionof{figure}{Gráficos de price según \text{covered\_area}. En el panel superior izquierdo se encuentra la densidad de price segú \text{convered\_area}. En el panel superior derecho se encuentra la correlación total entre price y \text{covered\_area} y la correlación por grupos según \text{zona\_avditalia}. En el panel inferior izquierdo se encuentra el gráfico de dispersión de price según \text{covered\_area} por \text{zona\_avditalia}, mientras que en el panel inferior derecho se encuentra la densidad de \text{covered\_area} según \text{zona\_avditalia}.}
\end{figure}

\chapter{Modelos \label{cap:Modelos}}

A continuación se presentan los resultados de los modelos implementados.

\section{Modelo lineal \label{sec:ml}}

<<lm>>=
lm <- lm(price ~ ., data = train)

#Homoscedasticidad

ggplot(as_tibble(lm$residuals) %>% 
             mutate(id = seq(1, length(lm$residuals), 1))) + 
      geom_point(aes(x = id, y = value), color = 'red', alpha = 1/5)

bp <- lmtest::bptest(lm) # se rechaza h0 de varianza constante 
# hay patrones que el modelo no longró captar -> no se cumple homoscedasicidad

#Normalidad

ggplot(as_tibble(lm$residuals)) + 
      geom_density(aes(value), color = 'red')

lillie.test(residuals(lm)) #se rechaza h0 de normalidad

# Residuos

RMSE_lm <- sqrt(mean((test$price - predict(lm,test))^2))
@

<<lasso>>=
# Regresión penalizada - Lasso

aptos_x_lasso <- model.matrix(price~.,train)
aptos_y <- train$price

aptos_x_lasso_test <- model.matrix(price~.,test)
aptos_y_test <- test$price

glmnet <- glmnet(aptos_x_lasso, aptos_y, family = 'gaussian')

RMSE_glmnet <- sqrt(mean((test$price - predict.glmnet(glmnet,aptos_x_lasso_test))^2))
@

<<errorlm>>=
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
print(xtable(cbind(RMSE_lm, RMSE_glmnet)))
@

Eliminar lasso - no interesa resultado no aporta.

\section{Árbol de regresión \label{sec:arbol}}

<<Arbol_imputmedia>>=
##### Arbol de regresion

# summary(arbol)

# Proceso de poda

#broom::tidy(arbol$cptable)

# Grafico de la evolucion del error

cp_error <- data.frame(arbol$cptable)

cp_error %>% ggplot(aes(x=CP,y=xerror))+geom_point(color="red")+geom_line()

# Otra forma


# Grafico

rpart.plot(arbol.prune,roundint = T,digits = 4)
@


El error cuadrático medio es de:

<<>>=
# RMSE
RMSE_arbol <- sqrt(mean((test$price-predict(arbol.prune,test))^2))
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
print(xtable(as.data.frame(RMSE_arbol)))
@

\section{Random Forest \label{sec:randomforest}}

<<RF_imputmedia, echo=FALSE, message=FALSE, warning=FALSE, fig = TRUE>>=
############ RF 

 # Importancia de las variables

importancia_rf <- data.frame(rf$variable.importance)

colnames(importancia_rf) <- c("importance")

importancia_rf$variables <- row.names(importancia_rf)

#### Veamos imputación por missranger

# Importancia de las variables

importancia_rf_mr <- data.frame(rf_mr$variable.importance)

colnames(importancia_rf_mr) <- c("importance")

importancia_rf_mr$variables <- row.names(importancia_rf_mr)

p1 <- importancia_rf %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="Variables",x="Importancia") +
      theme(axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 8),
            axis.title.y = element_blank())

p2 <- importancia_rf_mr %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="Variables",x="Importancia")+
      theme(axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 8),
            axis.title.y = element_blank())

grid.arrange(p1, p2, ncol = 2)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAFíA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apa}
\bibliography{TFGbiblo}

%%%%%%%%%%%%%%%%%%%%%%%%%%% ANEXO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{appendix}

\chapter{Anexo} 

<<>>=
vars <- readxl::read_excel(here('mercado_libre/Api', 'criterios_de_limpieza.xlsx'),
                                sheet = 'anexo_vars')
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
vars.table <- xtable(as.data.frame(vars)[1:26,], include.rownames=FALSE)
align(vars.table) <- rep("l", ncol(vars)+1)
print(vars.table, size = "\\fontsize{3pt}{3pt}\\setlength{\\tabcolsep}{5pt}", include.rownames=FALSE)
@

<<>>=
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
vars.table <- xtable(as.data.frame(vars)[27:52,], include.rownames=FALSE)
align(vars.table) <- rep("l", ncol(vars)+1)
print(vars.table, size = "\\fontsize{3pt}{3pt}\\setlength{\\tabcolsep}{5pt}", include.rownames=FALSE)
@


<<>>=
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
vars.table <- xtable(as.data.frame(vars)[53:nrow(vars),], include.rownames=FALSE)
align(vars.table) <- rep("l", ncol(vars)+1)
print(vars.table, size = "\\fontsize{3pt}{3pt}\\setlength{\\tabcolsep}{5pt}", include.rownames=FALSE)
@


El total de observaciones sin NA es de \Sexpr{paste(dim(drop_na(aptos))[1])}.
Las proporciones de NA por variable son:

<<nas>>=
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),3)) %>% data.frame() %>% 
   rename(prop_na=".") %>% arrange(desc(prop_na))
library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
print(xtable(p_na %>% filter(p_na > 0)))
@



\section{siete \label{ane:ind}}


\section{ocho \label{ane:var}}

\end{appendix}


\end{document}
