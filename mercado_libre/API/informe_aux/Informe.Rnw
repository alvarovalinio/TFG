\documentclass[12pt,twoside,spanish,a4paper]{book}
\usepackage{geometry}\geometry{top=3cm,bottom=3cm,left=3cm,right=3cm}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
%\usepackage{mathrsfs}
\usepackage{longtable}
\usepackage{tocbibind}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{boxedminipage}
\usepackage[utf8]{inputenc}
%\usepackage[all,2cell,dvips]{xy}
\usepackage{graphicx}
\usepackage{float}
\usepackage[spanish,es-tabla]{babel}
%\usepackage{parskip}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{verbatim}
\usepackage{hyperref}
%\usepackage{fancyvrb}
\usepackage[authoryear]{natbib}

\fancyhf{} 
\fancyhead[LE]{\leftmark} 
\fancyhead[RO]{\nouppercase{\rightmark}} 
%\fancyfoot[LE,RO]{\thepage} 
\rfoot{\thepage} 
\pagestyle{fancy} 

\topmargin 2mm
\oddsidemargin 2mm
\evensidemargin 2mm

\makeindex
\setcounter{secnumdepth}{3}

\linespread{1.6}

\begin{document}
\SweaveOpts{concordance=TRUE}

<<echo=FALSE>>=
options(scipen = 999)  
@

<<echo=FALSE, message=FALSE, warning=FALSE>>=
library(knitr)
library(tidyverse)
library(sf)
library(scales)
library(here)
@

\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%% CARATULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\begin{center}

%\includegraphics[width=0.20\textwidth]{img/udelar_logo.jpg}

UNIVERSIDAD DE LA REPÚBLICA

Facultad de Ciencias Económicas y de Administración

Licenciatura en Estadística

Trabajo final de grado

\vspace{2.5cm}

\textbf{\large TÍTULO}

\vspace{1.5 cm}

\textbf{Lucia Coudet}
\textbf{Alvaro Valiño}


\end{center}


\vspace{2cm}

\noindent Tutores:\\
\noindent Natalia Da Silva\\


\vspace{1cm}

\begin{center}

\noindent Montevideo, Fecha.

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESUMEN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 'resumen.Rnw'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables


\setcounter{page}{1} 
 
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCCION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introducción \label{cap:Intro}}

El presente trabajo tiene como principal objetivo la implementación y estudio de diferentes técnicas de aprendizaje estadístico multivariadas, mediante las cuáles poder realizar predicciones de una variable de interés.

Con el fin de llevar a cabo este objetivo, fueron realizadas todas las etapas de un análisis estadístico, desde la obtención de los datos, limpieza y pre-procesamiento, hasta la generación de nuevas variable (ETL y data engeenering).

Uno de los puntos muy importantes de éste trabajo es la obtención y procesamiento de los datos, los cuales fueron obtenidos través de la Interfaz para acceder a la página web (API) de Mercado Libre. Para ello fue necesario la creación de un código que permita una descarga automatizada de la información disponible.

Se destaca que los resultados obtenidos fueron a través del lenguaje y entorno de programación para análisis estadístico y gráfico, *R*, enfocándose en la optimización de todos los procesos principalmente mediante la programación en paralelo. 

En lo que respecta a la técnicas de aprendizaje estadístico, se hizo especial énfasis en árboles de decisión.
Los mismos fueron utilizados no solamente para la predicción de la variable de respuesta sino también para la implementación de técnicas de imputación de valores faltantes.


Asimismo, con el fin de mejorar el desempeño predictivo de los modelos fue realizado un proceso de entraniento y validación de los resultados. 

En la medida que existe un trade off entre interpretación del modelo y poder predictivo, fueron tomados dos enfoques: un enfoque desd las técnicas clases y un enfoque desde técnicas alternativas. El modelo líneal clásico de precios hedónicos permite una interpretación directa de la relación entra la variable de respuesta y las covariables. Por su parte, las metodologías como ser de árbol de regresión suelen tener un desempeño predictivo superior a expensas de la pérdida del grado de interpretabilidad.

Todas las etapas fueron realizando preservando la reproducibilidad de todos los resultados. 

\chapter{Antecedentes \label{cap:Antec}}

\chapter{Marco teórico y metodología\label{cap:MT}}

\textbf{Supervisado – aprendizaje automatico}

Con el fin de obtener predicciones del precio de oferta de los inmuebles, se procedió a realizar técnicas de aprendizaje automático. Estas consisten en modelar y analizar conjuntos de datos, mediante el aprendizaje de ejemplos, con el fin de predecir y estimar resultados en forma automática.

En este contexto, se realizó un análisis supervisado, en la medida de que se cuenta con una variable de salida ($Y$) y varias variables de entrada ($X$). Por lo tanto, se tiene que los posibles modelos son de la forma:

\begin{center}

$Y=f(x)+\epsilon$

\end{center}

Siendo $f$ una función desconocida y $\epsilon$ un error aleatorio independiente de $X$ e $Y$ con media 0. Se denota a la matriz $X$ de dimiensión $n \times p$ a la matriz de datos, donde se tiene $n$ observaciones de entrenamiento y $p$ variables. 

La i-ésima fila se corresponde a la i-ésima observación (perteneciente al conjunto de entrenamiento) siendo de la forma $x_{i}=(x_{i1},\dots,x_{ip})^{T}$, con $x_{i}\in\mathbb R^{p}$. Por otro lado, se denota una nueva observación (o pertenciente al conjunto de testeo) como $x^{*}=(x_{i1}^{*},\dots,x^{*}_{ip})^{T}$, donde está es un vector p-dimensional (al igual que $x_{i}$). 

Se destaca que al ajustar los diferentes modelos se tomó como conjunto de entrenamiento a aproximadamente el $80\%$ de las observaciones. A la hora de estimar $f$, se realizó mediante métodos paramétricos y no paramétricos. 

En el primer caso, se asumió la forma funcional de $f$ y se procedió a estimar sus respectivos parámetros. Por otro lado, en los métodos no paramétricos, no se asumió la forma funcional de $f$.

\textbf{Arboles de regresión}

En primer lugar, se procedió a modelar mediante la implementación de un árbol de decisión. En la medida de que se cuenta con una variable de salida continua, se construyó un árbol de regresión.

A pesar de que en la literatura existen diversos enfoques para la construcción de estos modelos, se trabajó con el método \textit{CART} el cual fue propuesto por \textit{Breiman}, \textit{Friedman}, \textit{Olshen} y \textit{Stone} en 1984.

Este método, se caracteriza por la realización de particiones binarias recursivas del espacio de las variables de entrada. Mediante las mismas, se conforma una organización jerárquica en forma de árbol, donde en cada nodo interior se tiene una pregunta (dicotómica) sobre una variable de entrada y en cada nodo terminal (denominado "hoja") una decisión.

De está forma, se procede a dividir el conjunto de los valores posibles de $X_{1}\dots,X_{p}$ (variables de entrada) en $J$ regiones disjuntas $R_{1},\dots,R_{J}$. *(James, 2013)*

Luego para cada observación que se encuentra en la región $R_{j}$ se realiza la misma predicción. Siendo está, en el contexto de árboles de regresión, el promedio de la variable respuesta en dicha región.


En el momento de la construcción de las regiones ($R_{1},\dots,R_{J}$) se realiza de tal forma que en cada subconjunto resultante (denominados como "nodos hijos") en cada iteración implique una disminución en la impureza de estos.
Para ello, se construyen las regiones $R_{1]$, \dots, $R_{j]$ de forma tal que minimicen la suma de cuadrados de los residuos (o por sus silabas en ingles *RSS*).  
\begin{center}

$\displaystyle \sum^{J}_{j=1} \sum_{i\  \in R_{j}} \left(y_{i} - \hat{y}_{R_{j}} \right) ^{2}$ (Hastie, 2001)

\end{center}
Siendo $\hat{y}_{R_{j}}$ el promedio de la variable respuesta en la j-ésima región.
Para lograr este cometido se utiliza una separación recursiva binaria de la siguiente forma. Se selecciona la variable $X_{j}$ y el número $s$ dividendo el espacio en dos regiones $R_{1}(j,s) = \lbrace{ X : X_{j} < s \rbrace}$ y $R_{2}(j,s) = \lbrace{ X: X_{j} \geq s \rbrace}$  de forma tal que se haga mínimo 

\begin{center}

$\displaystyle \sum_{i:x_{i} \in R_{1}(j,s)} \left(y - \hat{y}_{R_{1}}\right)^{2} + \sum_{i:x_{i} \in R_{2}(j,s)} \left(y - \hat{y}_{R_{2}}\right)^{2}$

\end{center}

Una vez encontrada la mejor partición se separan los datos en las regiones resultantes y se repite el proceso en cada región. Es decir, se busca nuevamente la mejor variable y el mejor punto de corte de forma se incremente la disminución de la impureza en los nodos hijos.
El proceso continúa hasta que se satisfaga algún criterio de parada (como por ejemplo que los nodos terminales tengan cierto número de observaciones).
Luego de definido el criterio de construcción de las regiones, se procede a realizar un proceso de poda en el árbol obtenido basado en un criterio de *costo-complejidad*. Esto en la medida de que si se deja crecer el árbol de forma indefinida se obtiene un modelo con un alto grado de sobre ajuste (*overfitting*). Por su contraparte, un árbol muy "pequeño", posiblemente no logre capturar la estructura del conjunto de datos. *(Hastie, 2001)*
El proceso de poda realizado, consiste en dejar crecer el árbol hasta que los nodos terminales tengan cierto número de observaciones (dicho árbol se denota como $T_{0}$). Luego se elige aquel subárbol el cual posee un menor error de predicción en el conjunto de testeo. En la medida de que un procedimiento de *cross-validation* aplicado en cada posible subárbol es muy costoso en términos de "tiempo computacional", surge como alternativa el método basado en un criterio de *costo-complejidad*.

En dicho método se define a $T_{\alpha}$ como un subárbol obtenido al podar a $T_{0}$. De esta forma, para cada $\alpha$ se busca $T_{\alpha}$ que minimice la siguiente expresión:

\begin{center}

$C_{\alpha}(T)=\displaystyle \sum^{|T|}_{m=1} N_{m} Q_{m}(T) + \alpha|T|$ (Hastie, 2001)

\end{center}

Donde se tiene que $|T|$ es igual número de nodos terminales del árbol $T$, mientras que $N_{m}$ es el número de observaciones en la región $R_{m}$. Por otro lado, la expresión $Q_{m}(T)$ consiste la medida de impureza.

En cuanto al parámetro $\alpha$, el mismo consiste en un parámetro de penalización aplicado a la complejidad (tamaño) del árbol. Donde valores altos de este, penalizan a árboles de gran tamaño. De esta forma, controla el compromiso entre la complejidad y la bondad de ajuste del modelo. Dicho parámetro se estima mediante *cross-validation*.

\textbf{Bagging – Random Forest}

A pesar de que los árboles de regresión poseen un alto grado de interpretabilidad, estos poseen la gran limitante de ser inestables. Esto en el sentido de que pequeñas variaciones en el conjunto de entrenamiento y testeo generan grandes cambios en las estimaciones.
Por lo tanto, con el fin de realizar predicciones del precio de oferta de los apartamentos a la venta, se procedió a emplear diferentes métodos con el fin de obtener un método de predicción estable.
En primer lugar, se aplicó el método *Bagging* desarrollado por *Breiman* en 1994. Este método consiste en construir un estimador combinando distintas versiones de estimadores.
En este contexto, estas nuevas versiones se construyen generando nuevos conjuntos de entrenamiento, mediante la técnica de remuestreo *bootstrap*. La cual consiste en la generación de varias muestras con reemplazo, del conjunto de datos de entrenamiento, donde a cada observación se le asigna el mismo peso ($\frac{1}{n}$, siendo $n$ el número de observaciones). Al número de muestras *bootstrap* se le suele denotar con la letra $B$.
A la hora de utilizar este método en problemas de regresión, se procede a tomar varias muestras *bootstrap*, donde a partir de cada una de ellas se construye un estimador. Luego, se le asigna a la observación el promedio de las respuestas de los estimadores construidos en cada muestra. 

Este método, en el contexto de árboles de regresión, consiste en la creación de $B$ árboles, cada uno mediante un nuevo conjunto de entrenamiento obtenido mediante una muestra *bootstrap*. Se destaca que a estos árboles no se le realiza un proceso de poda. Por lo que estos mismos presentan una gran varianza, pero bajo sesgo. Sin embargo, al predecir mediante un promedio de los $B$ árboles, se logra una reducción considerable en la varianza del estimador y de esta forma mejorando la precisión de la predicción.
Luego, como una generalización del método *Bagging* aplicado a árboles de decisión, surge el segundo método empleado con el fin obtener un modelo estable a la hora de realizar predicciones. Este se denomina *Random Forest*, donde al igual que el método anterior, consiste en la creación de varios arboles de decisión (en este caso de regresión) mediante la generación de muestras *bootstrap*.
Sin embargo, el método *Random Forest*, no considera en cada división el total de variables, sino un subconjunto de estas elegido de forma aleatoria. “A pesar de que a la hora de elegir el número de variables existen diferentes aproximaciones, en este informe se procedió a utilizar la parte entera de $\sqrt{p}$, siendo $p$ el número de variables. Esto en la medida de que es el número de variables que utiliza por defecto (en problemas de clasificación) la función *randomForest()* del paquete *randomForest*” CAMBIAR ESTO.
Este método se destaca sobre el método *Bagging* principalmente cuando se tiene que una variable es muy influyente. Esto en la medida de que si se consideran todas las variables a la hora de construir los diferentes $B$ árboles, en la medida de que se tiene una variable muy influyente, posiblemente dichos árboles no difieran mucho entre sí. Esta limitante no se presenta en *Random Forest* en el sentido de que selecciona de forma aleatoria un subconjunto de los predictores en cada iteración.
En los dos métodos anteriormente mencionados, se tiene que cada observación posee una probabilidad de aproximadamente $\frac{2}{3}$ de ser seleccionada en cada remuestra realizada. De esta forma, se cuenta con un conjunto de observaciones las cuales no son utilizadas para construir el estimador.
Este conjunto de observaciones se denomina como *out of bag observations* (*OOB*). Por lo tanto, en cada iteración se procede a predecir dichas observaciones, mediante el estimador obtenido. Repitiendo este procedimiento para las $n$ observaciones, se calcula el *error OOB*. Dicha medida se procedió a utilizar como una primera aproximación en cuanto a la performance predictiva de ambos modelos.
A pesar de que los métodos anteriormente mencionados logran solucionar el problema de la inestabilidad por parte de los árboles de decisión, estos métodos se caracterizan por presentar una baja interpretabilidad. 
Sin embargo, en la medida de que se construyen varios árboles, es posible obtener cierta medida de la importancia de cada predictor. En los algoritmos *Bagging* y *Random Forest* se calcula la reducción de la medida de impureza en las divisiones de una variable dada promediando en todos los árboles obtenidos. De esta forma, si la reducción es "grande" la variable se considera "importante".

\textbf{Cross – validation}

Por último, a la hora de evaluar la performance de los diferentes modelos planteados, se realizó un procedimiento de *cross-validation*, particularmente *k-folds*.
El algoritmo consiste en dividir la muestra en $k$ submuestras de igual tamaño. Luego $k-1$ submuestras se usan como datos de entrenamiento y la muestra restante $k$ se usa para testear los datos.
A continuación, se procede a ajustar los datos de esa muestra con el modelo construido con las $k-1$ muestras. Donde el proceso se repite $k$ veces, con cada una de las $k$ muestras. De tal forma que cada $k$ muestras es utilizada una sola vez como datos de testeo. 
De esta forma, todas las observaciones se usan tanto para train como para test. A su vez, cada observación se usa para test una sola vez y para train $k-1$ veces. Los errores obtenidos en cada etapa se promedian para producir una sola estimación (error medio obtenido de los $k$ análisis realizados).
Con el fin de medir el error de predicción del modelo en los modelos planteados anteriormente se consideró la siguiente medida:

\begin{center}

$RMSE= \displaystyle \frac{1}{k} \sum_{k=1}^{K} RMSE_{k}$

\end{center}

Donde $RMSE_{k}$ es la raiz del error cuadratico medio en la k-ésima muestra.

\begin{center}

$RMSE_k= \displaystyle \sqrt{ \frac{1}{n_k} \sum_{j = 1}^{n_k} (y_j - \hat{y}_j)^{2}}$

\end{center}

dónde $n_k$ es la cantidad de observaciones en la k-ésima muestra. 

\chapter{Datos \label{cap:Antec}}

\section{Descripción de los datos utilizados \label{sec:desc}}

\section{Obtención \label{sec:obtencion}}

Los datos de precio de oferta de los apartamentos a la venta en Montevideo y todas las covariables utilizadas para modelizar fueron obtenidas a través de la Interfaz para acceder a la página web (API) de Mercado Libre \url{https://www.mercadolibre.com.uy/}. Para ello, es necesario registrarse en la web \url{https://developers.mercadolibre.com.uy/} y desde allí crear una aplicación. Una vez realizado este paso es posible obtener una clave (token) válida por 6 hs que mercado libre proporciona para la conexión a su API.

De ésta manera y como fue mencionado, una vez que se obtiene el token es posible consultar la API. Para obtener la información de todos los inmuebles tanto a la venta como para el alquier en Uruguay es necesario realizar la consulta filtrando la categoría MLU1459. Sobre esta categoría existen distintas subcategorías. A los efectos del interés del presente trabajo fue consultada la API filtrando según la categoría MLU1474 que corresponde a todos los apartamentos a la venta en Uruguay.

Ahora bien, para obtener la información sobre los apartamentos a la venta en el departamento de Montevideo, se consultó la API filtrando según categoría MLU1474 y además filtrando según el id (identificador) de cada uno de los barrios en Montevideo.

Esto permitió obtener numerosos datos pero no fue suficiente. Para poder acceder a los atributos específicos de cada publicación (ítem) fue necesario realizar consultas filtrando específicamente en cada id de cada publicación. Es decir una vez btenida la información disponible consultando la categoría MLU1474 y filtrando los barrios en Montevideo, se utilizaron los id entonces obtenidos de cada una de las publicaicones y se realizaron consultas por publicación.

De ésta manera fue posible obtener la información de todos los apartamentos a la venta en Montevideo disponible en la API de mercado libre, de manera automatizada. El programa tarda aproximadamente 3 hs en obtener la totalidad de los datos, pudiendo variar según la máquina utilizada.

Para mayor información es posible consultar el código del script funcion api barrios. Disponible en el repositotio público en Github en el siguiebte link: \url{https://github.com/alvarovalinio/TFG/tree/main/mercado_libre/API/funciones}. 

\section{Procesamiento y criterios de limpieza\label{sec:procesamiento}}

La obtención y limpieza de los datos fue una parte muy importante de éste trabajo.
En lo que respecta a la limpieza, el Anexo X contiene detalles específicos de los criterios de limpieza seleccionados para cada una de las variable en la base de datos.

En lo que respecta a las variables cuantitativas como precio de oferta del inmueble (price), gastos comunes (maintainance fee), áera cubierta (covered area), etre otras, un punto importante en la limpieza fue tratar de reconocer valores sin sentido, por ejemplo 11111, 5555555. Para ello fue construida una función auxiliar que es capaz de detectar cuándo un valor tiene 3 o más números iguales repetidos. En ese caso se considera que el dato es erróneo y en el caso de la variable precio de oferta del inmueble la observación completa es eliminada, mientras que en el caso de gastos comunes se le imputa NA. También se eliminan las observaciones cuyo precio de oferta es inferior al valor del percentil 75\% entre las observaciones con precio inferior a USD 40.000 ya que se consideran datos erróneos.

De manera similar y debido a la elevada presencia de valores atípicos, se eliminan todas las observaciones cuyos valores de la variable price superan el percentil 95\% de la misma.

En lo que respecta a las variables área total (totalarea) y área cubierta (coveredarea) se decidió asignar NA a todos los valores superiores a 1000 metros cuadrados, ya que se consideran datos erróneos. 

Existe un conjunto de variables que toman valor Si, No, NA. Para todas ellas se asume que los NA son No y se realiza la recodificación correspondiente en función de ello.

Valores de latitud y longitud en las georreferencias de los inmuebles que no corresponden a coordenadas geográficas dentro de Montevdeo son recodificadas como NA ya que son datos incorrectos y no se conocen las georreferencias exactas.

Asimismo, se decidió dejar fuera del análisis algunas de las variables con gran porcentaje de NA. Entre ellas cantidad de habitaciones (rooms), garages (parkinglots), unidad (unirfloors), piso (floor), condición nuevo o usado (itemcondition), property age, entre otras.

Por mayor información respecto a los criterios de limpieza seleccionados para cada una de las variables disponibles en la base de daros se recomienda dirigirse al Anexo X.

\section{Variables geoespaciales\label{sec:geo}}

La base de datos obtenida contiene información sobre la latitud y longitud dónde está ubicado cada apartamento lo cuál implica tener la georreferencia específica. Esto motivó la elaboración de variables geoespaciales como distancia al shopping más cercano, ubicación respecto a la calle avenida italia en continuación con la calle 18 de Julio, distancia a la Rambla.

\subsection{Zona respecto a avenida italia \label{subsec:zona}}

Toma valor Norte o Sur según el centroide del barrio en el cuál se encuentra disponible el apartamento se encuentra al Norte o al Sur de la calle avenida italia o 18 de Julio. 

\subsection{Distancia al shopping más cercano \label{subsec:shop}}

Fue contruida utilizando la distancia en metros cuadrados entre el centroide del barrio en el cuál se encuentra disponible el apartamento y el shopping más cercano. Si bien se cuenta con la información de latitud y longitud específivo, debido a que existen georreferencias incorrectas en los datos para calcular la distancia al shopping más cercano se definió considerar la georreferencia del centroide del barrio al que pertenece. Para ello, fue necesaria la obtención de las coordenadas de todos los shoppings ubicados en montevideo y elaboración propia del archivo shapefile. 

Luego se decidió realizar la siguiente recodificación: 
  
\begin{itemize}    
\item Menos de 1 km: distancia menor o igual de 1 km,
\item Entre 1 km y 5 km: distancia superior a 1 km e inferior 5 km, 
\item Más de 5 km: distancia superior a 5 km.
\end{itemize}

\subsection{Distancia a la rambla \label{subsec:rambla}}

Fue construida utilizando la distancia (en m2) a la rambla este de Montevideo. Para selccionar qué zona de la rambla es la adecuada para diferenciar precio fue llevado a cabo un análisis de árbol de regresión.

Es importante mencionar que valores de latitud de la ubicación del inmueble inferiores a -35 y superiores a -34.7 así como valores de longitud inferiores a -56.5 y superiores a -56 se consideran datos erróneos y se les imputa la latitud del centroide del barrio al cual pertenece.  

\section{Encuesta contínua de hogares\label{sec:ech}}

La variable ingresomedioECH fue construida utilizando la información de la encuesta contínua de hogares (ECH) del año 2020. En particular se utilizó la variable HT11: Ingreso total del hogar con valor locativo sin servicio doméstico (en pesos uruguayos). Se calculó el ingreso medio por barrio de los hogares de Montevideo y luego se recodificaron los valores según los percentiles: 

\begin{itemize}      
\item Bajo: barrios con ingreso igual o inferior al primer cuartil (percentil 25\%), 
\item Medio - Bajo: barrios con ingresos superior al primer cuartil e inferior o igual al segundo cuartil (percentil 50\%),  
\item Medio - Alto: barrios con ingresos superior al segundo cuartil e inferior o igual al tercer cuartil (percentil 75\%), 
\item Alto: barrios con ingreso superior al tercer cuartil (percentil 75\%).
\end{itemize}

\section{Fuentes externas de información \label{sec:fuentes}}

Para la construcción de las variables geoespaciales fue necesario recurrir a fuentes externas de información. Asímismo y como fue mencionado, en la construcción de la variable ingresomedio ECH fue utilizada también la encuesta contínua de hogares 2020. 

La herramienta utilizada para construir los archivos .kml que permite georreferenciar los shoppings en Montevideo y la calle avenida italia en continuación con 18 de julio fue Google My Maps, el cual es un servicio puesto en marcha por Google en abril del 2007, que permite a los usuarios crear mapas personalizados para uso propio o para compartir. Los usuarios pueden añadir puntos, líneas y formas sobre Google Maps. fuente wikipedia \url{https://es.wikipedia.org/wiki/Google_My_Maps}.

De ésta forma, fueron construidos los archivos .kml que contienen las georreferencias de los shoppings de montevideo y de la calle avenida italia en contrinuación con 18 de julio.

Los shoppings georreferenciados son:

\begin{itemize}      
\item Montevideo shopping center
\item Punta Carretas shopping
\item Tres cruces shopping
\item Nuevocentro shopping
\item Portones shopping
\end{itemize}      

Una vez obtenidos los archivos .kml los mismos son transformados a archivos ESRI Shapefile utilizando QGis la cual es un Sistema de Información Geográfica SIG).

El formato ESRI Shapefile (SHP) es un formato de archivo informático propietario de datos espaciales desarrollado por la compañía ESRI, quien crea y comercializa software para Sistemas de Información Geográfica como Arc Info o ArcGIS. Originalmente se creó para la utilización con su producto ArcView GIS, pero actualmente se ha convertido en formato estándar de facto para el intercambio de información geográfica entre Sistemas de Información Geográfica por la importancia que los productos ESRI tienen en el mercado SIG y por estar muy bien documentado.

Un shapefile es un formato vectorial de almacenamiento digital donde se guarda la localización de los elementos geográficos y los atributos asociados a ellos. No obstante carece de capacidad para almacenar información topológica. Es un formato multiarchivo, es decir está generado por varios ficheros informáticos. El número mínimo requerido es de tres y tienen las extensiones siguientes:
      
\begin{itemize}       
\item  shp es el archivo que almacena las entidades geométricas de los objetos.
\item  shx es el archivo que almacena el índice de las entidades geométricas.
\item  dbf es la base de datos, en formato dBASE, donde se almacena la información de los atributos de los objetos.
\end{itemize}       

(Fuete wikipedia https://es.wikipedia.org/wiki/Shapefile)

A continuación s presentan el mapa de Montevideo con la georreferencia de los shoppings y de la calle avenida italia en continuación con 18 de julio. Es importente mencionar que la geometría del departamento de Montevideo fue construida con los archivos shapefile disponibles en la página web del Instituto Nacional de Estadística (INE) en la siguiente dirección: \url{https://www.ine.gub.uy/}.


<<echo=FALSE, message=FALSE, warning=FALSE>>=
# Vectoria INE
mapa_barrio <- st_read(here("mercado_libre/API/scripts_aux/Mapas", "vectorial_INE_barrios/ine_barrios"))
mapa_barrio <- st_transform(mapa_barrio, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

#Puntos shoppings
mall <- st_read(here("mercado_libre/API/scripts_aux/Mapas","puntos_googlemaps/shoppings"))
mall <- st_transform(mall, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
mall <- mall %>% select(Name, geometry)

# Líneas avd_italia
avd_italia <- st_read(here("mercado_libre/API/scripts_aux/Mapas","lineas_googlemaps/avditalia_18"))
avd_italia <- st_transform(avd_italia, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
avd_italia <- avd_italia %>% select(Name, geometry)
@

Mapa del departamento de Montevideo, Uruguay, y georreferencia de los shoppings y calle avenida italia en continuación con 18 de julio

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
      geom_sf() +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
      stat = "sf_coordinates", min.segment.length = 0,
      colour = "black", segment.colour = "black",
      size = 3, alpha = 0.8) +
      xlab('Longitud') +
      ylab('Latitud') 
@


A continuación se presenta el mapa según zona avditalia asignada a cada barrio.



<<echo=FALSE>>=
# Centroide barrios

#devuleve geometría con el centroide de cada barrios
centroide_barrios <- st_centroid(mapa_barrio)

# Extrae coordenadas (longitud y latitud) degeometría del centroide
centroide_barrios <- centroide_barrios %>%
      mutate(lon_barrio = st_coordinates(centroide_barrios$geometry)[,1],
             lat_barrio = st_coordinates(centroide_barrios$geometry)[,2])

# Pasa latitud y longitud del centroide a objeto sf
centroide_barrios_sf <- centroide_barrios %>% 
      st_as_sf(coords = c("lat_barrio","lon_barrio"), crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Tranforma coordenadas a formato long lat
centroide_barrios_sf_t <- st_transform(centroide_barrios_sf,crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

centroide_barrios <- centroide_barrios %>% 
      mutate(aux_lon = NA,
             zona_avditalia = NA)

# Extrae latitud y longitud de puntos en avd_italia (geometria)

puntos_avditalia <- st_coordinates(avd_italia)

puntos_avditalia <- as_tibble(puntos_avditalia) %>% select(-Z, -L1) %>%
      rename('lon_avditalia' = 'X',
             'lat_avditalia' = 'Y')

# Avd italia se conforma en total de 60 puntos
# Para cada barrios tomamos el punto en avd.italia con menor diferencia de longitud
# min {longitud centroide - longitud avd_italia }
# luego comparamos las latitudes del centroide y el punto de avd italia con mínima diferencia en cuanto a longitud
# si latitud avd italia < lat centroide barrio -> NORTE 
# si latitud avd italia >= lat centroide barrio -> NORTE 

for (i in 1:nrow(centroide_barrios)) {
      centroide_barrios$aux_lon[i] <- which.min(abs(centroide_barrios$lon_barrio[i] - 
                                                          puntos_avditalia$lon_avditalia))
      centroide_barrios$zona_avditalia[i] <- ifelse(
            puntos_avditalia$lat_avditalia[centroide_barrios$aux_lon[i]] < 
                  centroide_barrios$lat_barrio[i], 'Norte', 'Sur')
}


centroide_barrios <- centroide_barrios %>% 
      data.frame() %>% 
      select(NOMBBARR, zona_avditalia)

mapa_barrio <- mapa_barrio %>% left_join(centroide_barrios, by = 'NOMBBARR')
@

Mapa del departamento de Montevideo según zona respecto la callle avenida italia en constinuación con 18 de julio

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
                                stat = "sf_coordinates", min.segment.length = 0,
                                colour = "black", segment.colour = "black",
                                size = 3, alpha = 0.8) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@


<<echo=FALSE>>=

load(here("mercado_libre/API/ECH/RDATA_junio2021/HyP_2020_Terceros.RData"))


f <- f %>% select(numero, nper, hogar, nombarrio, HT11, ht13, YHOG, YSVL, lp_06, pobre_06,
             i228, i174, i259, i175, h155, h155_1, h156, h156_1, pesomen) %>%
      filter(hogar == 1)

# Para considerar pesos, multiplicar ingreso hogar i* peso hogar i
# Sum ingreso hogar i * peso hogar i / sum

f <- f %>% 
      group_by(nombarrio) %>%
      summarise(media_ingbarr = sum(pesomen*HT11, na.rm = TRUE) / 
                      sum(pesomen, na.rm = TRUE))

f <- f %>% rename('NOMBBARR' = 'nombarrio')

# quitamos espacios en blanco al final de nombbarr
f$NOMBBARR <- trimws(f$NOMBBARR, which = "right", whitespace = "[ \t\r\n]")

f$NOMBBARR <- recode(as.factor(f$NOMBBARR), 
                 'Malvín' = 'Malvin',
                  'Malvín Norte' = 'Malvin Norte')

mapa_barrio <- mapa_barrio %>% left_join(f, by = 'NOMBBARR')
@

Mapa del ingreso promedio de los hogares por barrio de Montevideo

<<echo=FALSE, fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = media_ingbarr/1000 )) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_gradient(low = 'firebrick', high = 'darkgreen', name = "Ingreso promedio \n por mil ECH",labels = comma)
@




%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAFíA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apa}
\bibliography{TFGbiblo}


%%%%%%%%%%%%%%%%%%%%%%%%%%% ANEXO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{appendix}

\chapter{Anexos} 

\section{siete \label{ane:ind}}


\section{ocho \label{ane:var}}

\end{appendix}


\end{document}
