\documentclass[12pt,twoside,spanish,a4paper]{book}
\usepackage{geometry}\geometry{top=3cm,bottom=3cm,left=3cm,right=3cm}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
%\usepackage{mathrsfs}
\usepackage{longtable}
\usepackage{tocbibind}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{boxedminipage}
\usepackage[utf8]{inputenc}
%\usepackage[all,2cell,dvips]{xy}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage[spanish,es-tabla]{babel}
%\usepackage{parskip}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{verbatim}
\usepackage{hyperref}
%\usepackage{fancyvrb}
\usepackage[authoryear]{natbib}
\captionsetup[table]{position=bottom} 

\fancyhf{} 
\fancyhead[LE]{\leftmark} 
\fancyhead[RO]{\nouppercase{\rightmark}} 
%\fancyfoot[LE,RO]{\thepage} 
\rfoot{\thepage} 
\pagestyle{fancy} 

\topmargin 2mm
\oddsidemargin 2mm
\evensidemargin 2mm

\makeindex
\setcounter{secnumdepth}{3}

\linespread{1.6}

\begin{document}
\SweaveOpts{concordance=TRUE, echo = FALSE}

<<echo=FALSE>>=
options(scipen = 999)  
@


<<librerias>>=
library(knitr)
library(tidyverse)
library(sf)
library(scales)
library(here)
library(gridExtra)
library(data.table)
library(magrittr)
library(ggcorrplot)
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(caret)
library(doParallel)
library(rpart)
library(rpart.plot)
library(rattle)
library(ranger)
library(missRanger)
library(kableExtra)
library(xtable)
library(RColorBrewer)
library(lmtest)
library(nortest)
library(glmnet)
library(GGally)
library(latex2exp)
library(gbm)
library(kernlab)
@


<<funciones, echo=FALSE, message=FALSE, warning=FALSE>>=
source(here("mercado_libre/API/funciones","funcion_imput_media.R"))
@

<<Datos_imputmedia>>=
options(scipen = 999)

#### DATOS

aptos_yearmonth <- list.files(path = here("mercado_libre/API/datos/limpios/apt"), 
                              pattern = "*.csv", full.names = T)

yearmonth <- c('aptos_202106','aptos_202107',"aptos_202108", "aptos_202109", "aptos_202110" )

aptos <- sapply(aptos_yearmonth, FUN=function(yearmonth){
      read_csv(file=yearmonth)}, simplify=FALSE) %>% bind_rows()


aptos <- aptos %>% group_by(id) %>% 
      arrange(desc(fecha_bajada)) %>%
      slice(1) %>% ungroup()

aptos <- aptos %>% mutate_if(is.character, as.factor)

# Filtramos por el criterio en price - eliminamos obs. con price superior al percentil 95%

aptos_todos <- aptos

aptos <- aptos %>% filter(price <= quantile(aptos$price,.95))

# Perdemos esta cantidad de registros

# nrow(aptos_todos) - nrow(aptos)

# vemos prop de NA
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),4)) %>% data.frame() %>% 
      rename(prop_na=".") %>% arrange(desc(prop_na))

#### Definimos variables Sin na imputamos por la media

aptos_sin_na <- imput_media(aptos,p=.1)

aptos_mr <- read_csv(here("mercado_libre/API/datos/limpios/apt/aptos_mr","aptos_mr.csv")) 
aptos_mr <- aptos_mr %>% mutate_if(is.character, as.factor)

############ train - test 

set.seed(1234)
ids <- sample(nrow(aptos_sin_na), 0.8*nrow(aptos_sin_na))

train <- aptos_sin_na[ids,]
test <- aptos_sin_na[-ids,]

train_mr <- aptos_mr[ids,]
test_mr <- aptos_mr[-ids,]

############## Cargamos los modelos

#### Arbol

load(here("mercado_libre/modelos/ARBOL","arbol_train.RDS")) #levanta objeto con nombre arbol

load(here("mercado_libre/modelos/ARBOL","arbol_prune_train.RDS")) #arbol.p

load(here("mercado_libre/modelos/ARBOL","arbol_train_mr.RDS")) #levanta objeto con nombre arbol
load(here("mercado_libre/modelos/ARBOL","arbol_prune_train_mr.RDS")) #arbol.p

# Arbol para dist rambla

load(here("mercado_libre/modelos/ARBOL","arbol_prune_lat_lon.RDS")) 


#### RF

load(here("mercado_libre/modelos/RF","RF_train.RDS")) # rf_TRAIN

load(here("mercado_libre/modelos/RF","RF_train_mr.RDS")) # rf_train_mr


#### Boosting

load(here("mercado_libre/modelos/BOOSTING","boosting_train.RDS")) 

load(here("mercado_libre/modelos/BOOSTING","boosting_train_mr.RDS")) 

#### SVR

load(here("mercado_libre/modelos/SVR","SVR_train.RDS"))

load(here("mercado_libre/modelos/SVR","SVR_train_mr.RDS"))

#### Caret

#### RF

load(here("mercado_libre/modelos/RF/Caret","RF_caret.RDS")) 

load(here("mercado_libre/modelos/RF/Caret","RF_caret_mr.RDS")) 

load(here("mercado_libre/modelos/RF/Caret","RF_caret_tunning.RDS")) 

load(here("mercado_libre/modelos/RF/Caret","RF_caret_tunning_mr.RDS")) 

#### Boosting

load(here("mercado_libre/modelos/BOOSTING/Caret","Boosting_caret.RDS")) 

load(here("mercado_libre/modelos/BOOSTING/Caret","Boosting_caret_mr.RDS")) 

load(here("mercado_libre/modelos/BOOSTING/Caret","Boosting_caret_tunning.RDS")) 

load(here("mercado_libre/modelos/BOOSTING/Caret","Boosting_caret_tunning_mr.RDS")) 
@


<<mapas>>=
# Vectoria INE
mapa_barrio <- st_read(here("mercado_libre/API/scripts_aux/Mapas", "vectorial_INE_barrios/ine_barrios"), quiet = TRUE)
mapa_barrio <- st_transform(mapa_barrio, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

#Puntos shoppings
mall <- st_read(here("mercado_libre/API/scripts_aux/Mapas","puntos_googlemaps/shoppings"), quiet = TRUE)
mall <- st_transform(mall, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
mall <- mall %>% select(Name, geometry)

# Líneas avd_italia
avd_italia <- st_read(here("mercado_libre/API/scripts_aux/Mapas","lineas_googlemaps/avditalia_18"), quiet = TRUE)
avd_italia <- st_transform(avd_italia, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
avd_italia <- avd_italia %>% select(Name, geometry)

# Rambla Este - MVD
rambla <- st_read(here("mercado_libre/API/scripts_aux/Mapas/lineas_googlemaps", "ramblaeste_MVD"), quiet = TRUE)
rambla <- st_transform(rambla, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
rambla <- rambla %>% select(Name, geometry)
@


<<>>=
# Centroide barrios

#devuleve geometría con el centroide de cada barrios
centroide_barrios <- st_centroid(mapa_barrio)

# Extrae coordenadas (longitud y latitud) degeometría del centroide
centroide_barrios <- centroide_barrios %>%
      mutate(lon_barrio = st_coordinates(centroide_barrios$geometry)[,1],
             lat_barrio = st_coordinates(centroide_barrios$geometry)[,2])

# Pasa latitud y longitud del centroide a objeto sf
centroide_barrios_sf <- centroide_barrios %>% 
      st_as_sf(coords = c("lat_barrio","lon_barrio"), crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Tranforma coordenadas a formato long lat
centroide_barrios_sf_t <- st_transform(centroide_barrios_sf,crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

centroide_barrios <- centroide_barrios %>% 
      mutate(aux_lon = NA,
             zona_avditalia = NA)

# Extrae latitud y longitud de puntos en avd_italia (geometria)

puntos_avditalia <- st_coordinates(avd_italia)

puntos_avditalia <- as_tibble(puntos_avditalia) %>% select(-Z, -L1) %>%
      rename('lon_avditalia' = 'X',
             'lat_avditalia' = 'Y')

# Avd italia se conforma en total de 60 puntos
# Para cada barrios tomamos el punto en avd.italia con menor diferencia de longitud
# min {longitud centroide - longitud avd_italia }
# luego comparamos las latitudes del centroide y el punto de avd italia con mínima diferencia en cuanto a longitud
# si latitud avd italia < lat centroide barrio -> NORTE 
# si latitud avd italia >= lat centroide barrio -> NORTE 

for (i in 1:nrow(centroide_barrios)) {
      centroide_barrios$aux_lon[i] <- which.min(abs(centroide_barrios$lon_barrio[i] - 
                                                          puntos_avditalia$lon_avditalia))
      centroide_barrios$zona_avditalia[i] <- ifelse(
            puntos_avditalia$lat_avditalia[centroide_barrios$aux_lon[i]] < 
                  centroide_barrios$lat_barrio[i], 'Norte', 'Sur')
}


centroide_barrios <- centroide_barrios %>% 
      data.frame() %>% 
      select(NOMBBARR, zona_avditalia)

mapa_barrio <- mapa_barrio %>% left_join(centroide_barrios, by = 'NOMBBARR')
@

\pagenumbering{roman}


%%%%%%%%%%%%%%%%%%%%%%%%%% CARATULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\begin{center}

%\includegraphics[width=0.20\textwidth]{img/udelar_logo.jpg}

UNIVERSIDAD DE LA REPÚBLICA

Facultad de Ciencias Económicas y de Administración

Licenciatura en Estadística

Trabajo final de grado

\vspace{2.5cm}

\textbf{\large TÍTULO}

\vspace{1.5 cm}

\textbf{Lucia Coudet}
\textbf{Alvaro Valiño}


\end{center}


\vspace{2cm}

\noindent Tutores:\\
\noindent Natalia Da Silva\\


\vspace{1cm}

\begin{center}

\noindent Montevideo, Fecha.

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESUMEN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 'resumen.Rnw'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables


\setcounter{page}{1} 
 
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCCION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{chapter}
\chapter{Introducción \label{cap:Intro}}

El presente trabajo tiene como principal objetivo el estudio e implementación de diferentes técnicas de aprendizaje estadístico mediante las cuáles realizar predicciones de una variable de interés. En particular, se trabaja con el precio de oferta de los apartamentos a la venta en el departamento de Montevideo, Uruguay, disponibles en mercado libre.

Para ello, se plantearon 3 objetivos específicos, siendo estos: 1) obtención y procesamiento de los datos para transformarlos en información, con especial énfasis en la automatización y reproducibilidad de los mismos; 2) implementación de técnicas de aprendizaje estadístico que permitan una mayor flexibilidad en la estimación con respecto a las técnicas clásicas de estimación y 3) una primera aproximación a un análisis de interpretabilidad enfocándose en los métodos globales modelo-agnósticos (\textit{model-agnostic methods}).

En lo que respecta al primer objetivo, gran parte de los datos fueron obtenidos a través de la Interfaz de programación de aplicaciones (API) de Mercado Libre. Esto implicó la creación de un programa que permita una descarga automatizada de la información disponible. Asimismo, se trabajó con diferentes fuentes de información externas. En particular con la Encuesta Contínua de Hogares (ECH) y datos obtenidos de la plataforma Google My Maps. De esta manera, fueron construídas variables adicionales que se consideraron de interés para el problema planteado.

En cuanto al segundo objetivo, las técnicas de aprendizaje estadístico implentadas se separan en dos grupos. En primer lugar, un conjunto de técnicas aplicadas a Arboles de decisión, en particular Random Forest y Boosting (james2013introduction). En segundo lugar, se trabajó con la técnica Support Vector Regression la cual pertenece al grupo de los modelos de regresión robusta (Vapnik). En lo que respecta a las técnicas clásicas de estimación, se realizó la aplicación de un modelo lineal (Carmona).

Una vez implementados los modelos mencionados en el punto anterior, con el fin de determinar el modelo con mejor capacidad predictiva, se llevó a cabo un análisis comparativo en función de diferentes métricas. 

Por último, una vez obtenido el mejor modelo, en la medida que los modelos implementados (con excepción del modelo lineal) se denominan como modelos de caja negra (molnar - libro interp) se llevó a cabo un análisis de interpretabilidad. Esto último, principalmente mediante análisis gráfico. 

Se destaca que las técnicas utilizadas implican un alto costo computacional. De esta forma, con el fin de obtener un mayor poder de cómputo se utilizó un enfoque desde la programación en paralelo. A su vez, todas las etapas fueron realizadas de tal forma que todos los resultados sean reproducibles.

Los resultados obtenidos fueron a través del lenguaje y entorno de programación para análisis estadístico y gráfico, R, enfocándose en la optimización de todos los procesos principalmente mediante la programación en paralelo. 

El trabajo se conforma de 7 capítulos. A continuación se presenta en el capítulo 2 el detalle de la obtención y depuración de los datos utilizados. Luego, en el capítulo 3 se mencionan los principales antecedentes consultados. En lo que respecta al capítulo 4, se detalla la metodología utilizada y el sustento teórico de la misma. Posteriormente, en los capítulos 5 y 6 se presenta el análisis exploratorio de los datos y los principales resultados obtenidos respectivamente. Por su parte, en el capítulo 7 se presentan las principales conclusiones abordadas y principales líneas de trabajo para futuras investigaciones. 

\chapter{Datos \label{cap:datos}}

Como fue mencionado en el capítulo \ref{cap:Intro}, el principal objetivo del trabajo es el estudio e implementación de diferentes técnicas de aprendizaje estadístico mediante las cuáles realizar predicciones de una variable de interés. En particular se trabaja con el precio de oferta de los apartamentos a la venta en Montevideo, Uruguay.

A continuación se presenta en la sección \ref{sec:obtencion} la descripción del proceso de obtención de los datos. Luego, en la sección \ref{sec:procesamiento} se realiza un detalle de los principales criterios de depuración utilizados. Posteriormente, en la sección \ref{sec:fuentesext} se describen las principales fuentes externas de información utilizadas. Por último en la sección \ref{sec:varselab} se presentan las variables construídas en base a elaboración propia.

\section{Obtención \label{sec:obtencion}}

La base de datos utilizada fue obtenida consultando la API de Mercado Libre. Dónde con el fin de interactuar con la misma, es necesario seguir los siguientes pasos: 1) El usuario se registra en la web \url{https://developers.mercadolibre.com.uy/}; 2) se crea una aplicación con el cual se obtiene un identificador y una contraseña para interactuar con la API y 3) se obtiene una clave (token) para poder realizar la consulta. 

En particular, para obtener la información de las publicaciones de apartamentos se realiza una consulta a la API especificando en la url la categoría MLU1474. Adicionalmente, para considerar solamente los apartamentos en el departamento de Montevideo, se especifica en la url el identificador de cada uno de los barrios de Montevideo. En el anexo \ref{barriosA} se presenta la tabla los barrios de Montevideo y su identificador. 

Luego, con el fin de obtener un mayor número de variables explicativas, se accedió a los atributos específicos de cada publicación. Para ello, es necesario ingresar en la url de cada identificador de cada publicación. 

De ésta manera fue posible obtener la información de todos los apartamentos a la venta en Montevideo disponible en la API de mercado libre y considerada de interés, de manera automatizada. 

Se destaca dos aspectos relevantes en cuanto al funcionamiento de la API de Mercado Libre. 

En primer lugar, es posible obtener la información de las publicaciones vigentes a la fechas de consulta. Por lo tanto, se realizaron descargas mensuales abarcando el periodo comprendido entre los meses de junio a octubre del año 2021, ambos inclusive. Dado que las publicaciones mantienen los identificadores a través de los meses, para el caso de las publicaciones que se mantienen vigentes más de un mes, se optó por considerar los datos correspondientes al último mes de vigencia. 

En segundo lugar, la API permite extraer hasta un límite determinado de publicaciones por consulta (10.000 consultas a la fecha de realización del trabajo). Debido a que las consultas fueron realizadas de forma iterativa por barrio, fue posible sobrellevar ésta limitante. 

\section{Procesamiento y criterios de limpieza\label{sec:procesamiento}}

El proceso de depuración de los datos fue realizados diferenciando según la naturaleza de las variables.
 
En primer lugar, en lo que respecta a las variables cuantitativas el proceso de depuración se centró en el reconocimiento de valores erróneos, por ejemplo valores que repitan una secuencia de números.

Para ello se construyó una función auxiliar que es capaz de detectar cuándo un valor tiene 3 o más números iguales repetidos. En ese caso se considera que el dato es erróneo y se considera como valor faltente, con excepción de la variable Precio de oferta (price) dónde la observación completa es eliminada.

Asimismo se decidió no considerar en el análisis las observaciones cuyo precio de oferta es inferior al valor del percentil 75\% entre las observaciones con precio inferior a USD 40.000 ya que se consideran datos erróneos o de muy baja frecuencia. De manera similar y debido a la elevada presencia de valores atípicos, se eliminan todas las observaciones cuyos valores de la variable price superan el percentil 95\% de la misma. De ésta manera, no se consideraron en el análisis un X PORCIENTO de las observaciones. AGREGAR CONSULTANDO DATOS CRUDOS.

A su vez, en lo que respecta a las variables total\_area y covered\_area se decidió asignar como valor faltante a todos los valores superiores a 1000 metros cuadrados, ya que se consideran datos erróneos. 

En segundo lugar, respecto a las variables cualitativas, existe un conjunto de variables asociadas a la publicación del inmueble que toman valor Si, No, ó faltante. Debido a que en todos los casos se trata de campos no obligatorios para el usuario que realiza la publicación, no es posible diferenciar fehacientemente entre los valores faltantes y el valor No. Por lo tanto se trabajó bajo el supuesto de que los valores faltantes corresponden a el valor No y de ésta forma se realiza la recodificación correspondiente.

En tercer lugar, en cuanto a las variables latitud y longitud que permiten georreferenciar a los inmuebles, se realizó un proceso de depuración atendiendo a la factibilidad del dato. Para ello se consideraron los valores de ambas variables correspondientes a coordenadas geográficas dentro de Montevdeo . De ésta manera, valores de latitud inferiores a -35 y superiores a -34.7, al igual que valores de  longitud inferiores a -56.5 y superiores a -56 se consideraron datos erróneos. Los mismos fueron recodificados imputando los valores correspondientes al centroide del barrio donde se ubica el inmueble.

A su vez, se detectó otro tipo de error en las georreferencias siendo este el caso de aquellas georreferencias que no se ubican dentro del polígono del barrio al cual pertenece el inmueble. Para estos casos, se supone que el dato correcto es el nombre del barrio y no la georreferencia específica, y se imputa el valor de latitud y longitud correspondiente al centroide del barrio.

Dada la complejidad que implica la detección de éstas georreferencias erróneas, mediante un procedimiento de trade-off entre costo  y complejidad, se procedio de la siguiente forma: 1) se separó el mapa de Montevideo en dos regiones utilizando el corte de la calle Avenida Italia en continuación con la calle 18 de Julio; 2) se computó a qué región resultante de 1) pertenece el inmueble utilizando los valores de latitud y longitud; 3) se procedió de forma análoga a 2) considerando el centroide del barrio al cual pertenece el inmueble, 4) se evalua si 2) y 3) coinciden, en caso que difieran se le imputa al inmueble los valores de latitud y longitud correspondientes al centroide del barrio.  

En cuarto lugar, se decidió dejar fuera del análisis las variables con gran porcentaje de valores faltantes superior al 10\%. En el anexo \ref{nasA} se presenta la tabla con la proporción de valores faltantes para cada una de las variables.

Por último, en lo que respecta al tipo de cambio, todos los valores de la variable price expresados en moneda nacional fueron convertidos a dólares estadounidenses utilizando el tipo de cambio a la fecha de obtención de los datos. Sobre éste punto, se destaca que la obtención del valor del tipo de cambio se realiza de manera automatizada realizando un procedimiento de \textit{Web Scrapping} sobre la página web oficial del \textit{Instituto Nacional de Estadistica} (INE).

\section{Fuentes externas de información\label{sec:fuentesext}}

En la medida que la base de datos construida contiene información sobre la latitud y longitud dónde está ubicado cada apartamento, esto motivó la elaboración de variables geoespaciales.

La herramienta utilizada para construir mapas fue \textit{Google My Maps}, el cual es un servicio puesto en marcha por Google en abril del 2007, que permite a los usuarios crear mapas personalizados para uso propio o para compartir. Los usuarios pueden añadir puntos, líneas y formas sobre Google Maps. (\url{https://sites.google.com/mrpiercey.com/resources/geo/my-maps} - CITAR).

Una vez que se construyen los mapas desde dicha plataforma, se exportan los archivos generados. Luego, éstos son transformados a archivos ESRI Shapefile utilizando QGis el cual es un Sistema de Información Geográfica (SIG).

El formato ESRI (Environmental Systems Research Institute, Inc.) Shapefile (SHP) es un formato vectorial de almacenamiento digital donde se guarda la localización de los elementos geográficos y los atributos asociados a ellos. Es un formato multiarchivo, es decir está generado por varios ficheros informáticos. El número mínimo requerido es de tres y tienen las extensiones siguientes:
      
\begin{itemize}       
\item  shp es el archivo que almacena las entidades geométricas de los objetos.
\item  shx es el archivo que almacena el índice de las entidades geométricas.
\item  dbf es la base de datos, en formato dBASE, donde se almacena la información de los atributos de los objetos.
\end{itemize}  

(citar esri paper 1998)

En lo que respecta a la geometría del departamento de Montevideo la misma fue obtenida a partir de los archivos Shapefile disponibles en la página web del Instituto Nacional de Estadística (INE) en la siguiente dirección: \url{https://www.ine.gub.uy/}.

Por otra parte, se utilizó también información de la encuesta contínua de hogares 2020 para la construcción de variables que consideren el nivel de ingreso por barrio.

\section{Variables construidas \label{sec:varselab}}

Como se menciona en la sección \ref{sec:fuentesext}, tener la georreferencia específica en la base de datos motivó la elaboración de variables geoespaciales. En particular, las variables construídas fueron: ubicación respecto a la calle avenida italia en continuación con la calle 18 de Julio \textit{(zona\_avd)}, distancia a la Rambla Este de Montevideo \textit{(dist\_rambla)}, y distancia al shopping más cercano \textit{(dist\_shop)}.

La metodología utilizada para el cálculo de las distancias fue a través de la fórmula del Haversine la cual permite el cómputo de la distancia mínima entre dos puntos que se encuentran en un cuerpo esférico usando latitud y longitud. Para ello en particular se trabajó con la función \textit{distm} del paquete \textit{geosphere}. En el anexo \ref{harvesineA} se especifican los detalles sobre el cálculo. 

En lo que respecta a la variable \textit{(zona\_avd)}, la misma toma valor Norte o Sur según el apartamento se encuentre al Norte o al Sur del corte de la calle Avenida Italia en continuación con la calle 18 de Julio. Para ello, se tomó el punto más cercano a la ubicación del apartamento en el mapa de líneas que compone la calle Avenida Italia en continuación con la calle 18 de Julio. En particular, se toma la diferencia mínima en términos de longitud entre el apartamento y los puntos que componen el mapa. Una vez obtenido el mínimo, se comparan las latitudes. En caso que la latitud del apartamento sea mayor que la del punto más cercano seleccionado, se asigna Sur, y en otro caso se asigna Norte.

Por otra parte, la variable \textit{(dist\_shop)} fue contruida calculando la distancia en metros entre el apartamento y el centro comercial más cercano. Para ello, fue necesaria la obtención de las coordenadas geográficas (latitud y longitud) de todos los centros comerciales ubicados en Montevideo ( Montevideo shopping center, Punta Carretas shopping, Tres cruces shopping, Nuevocentro shopping y Portones shopping).

Por último, la variable \textit{(dist\_rambla)} fue construida utilizando la distancia en metros entre el apartamento y la rambla Este de Montevideo. Ésta variable fue construida a través de los resultados observados mediante el ajuste de un árbol de regresión de la variable precio en función de la latitud y la longitud. En el mismo se observó que los apartamentos con predicciones superiores toman valores de latitud entre -34.92 y -34.89, y valores de longitud mayores o iguales a -56.17. En la sección \ref{arbollatlonA} el gráfico del árbol ajustado

Se destaca que los mapas utilizados para construir las variables \textit{(zona\_avd)}, \textit{(dist\_rambla)} y \textit{(dist\_shop)} fueron obtenidos en base a elaboración propia a partir de Google My Maps, según detallado en la sección \ref{sec:fuentesext}. 

En la figura \ref{map1} se presenta el mapa del departamento de Montevideo, Uruguay, con las variables construidas.

\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
      geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'blue', size = 0.5) +
      geom_sf(data = rambla, color = 'yellow2', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
      stat = "sf_coordinates", min.segment.length = 0,
      colour = "black", segment.colour = "black",
      size = 3, alpha = 0.8) +
      xlab('Longitud') +
      ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@
\captionof{figure}{Mapa de Montevideo, Uruguay con la georreferencia de los shoppings, calle avenida italia en continuación con la calle avenida 18 de julio y rambla Este de Montevideo . La línea azul indica a la calle avenida italia en continuación con la calle avenida 18 de julio mientras que la línea amarilla a la rambla éste de Montevideo.}
\end{figure}
\label{map1}

<<echo=FALSE, eval=FALSE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
                                stat = "sf_coordinates", min.segment.length = 0,
                                colour = "black", segment.colour = "black",
                                size = 3, alpha = 0.8) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@
 
Por otra parte, la variable ingresomedio\_ECH fue construida utilizando la información de la encuesta contínua de hogares (ECH) del año 2020 disponible en \url{https://www.ine.gub.uy/}. En particular se utilizó la variable \textit{HT11}: Ingreso total del hogar con valor locativo sin servicio doméstico (en pesos uruguayos). Se calculó el ingreso medio por barrio de los hogares de Montevideo y luego se asignó a cada observación, el nivel de ingreso medio que le corresponda según el barrio dónde se encuentre el apartamento.

En la figura \re{map2} se presenta el mapa del ingreso promedio de los hogares por barrio de Montevideo.

<<>>=

load(here("mercado_libre/API/ECH/RDATA_junio2021/HyP_2020_Terceros.RData"))

f <- f %>% select(numero, nper, hogar, nombarrio, HT11, ht13, YHOG, YSVL, lp_06, pobre_06,
             i228, i174, i259, i175, h155, h155_1, h156, h156_1, pesomen) %>%
      filter(hogar == 1)

# Para considerar pesos, multiplicar ingreso hogar i* peso hogar i
# Sum ingreso hogar i * peso hogar i / sum

f <- f %>% 
      group_by(nombarrio) %>%
      summarise(media_ingbarr = sum(pesomen*HT11, na.rm = TRUE) / 
                      sum(pesomen, na.rm = TRUE))

f <- f %>% rename('NOMBBARR' = 'nombarrio')

# quitamos espacios en blanco al final de nombbarr
f$NOMBBARR <- trimws(f$NOMBBARR, which = "right", whitespace = "[ \t\r\n]")

f$NOMBBARR <- recode(as.factor(f$NOMBBARR), 
                 'Malvín' = 'Malvin',
                  'Malvín Norte' = 'Malvin Norte',
                 'Unión' = 'Union',
                 'Maroñas, Parque Guaraní' = 'Maroñas, Parque Guarani',
                 'Villa García, Manga Rur.' = 'Villa Garcia, Manga Rur.')

mapa_barrio <- mapa_barrio %>% left_join(f, by = 'NOMBBARR')
@


\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = media_ingbarr/1000 )) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_gradient(low = 'red', high = 'green', name = "Ingreso promedio \n por mil ECH",labels = comma) 
@
\captionof{figure}{Mapa del departamento de Montevideo, Uruguay, por nivel de ingreso promedio por mil obtenido a partir de la Encuesta Contínua de Hogares 2020.}
\end{figure}
\label{map2}

\chapter{Antecedentes \label{cap:Antec}}

Como fue mencionado en el capítulo \ref{cap:Intro}, el presente trabajo  tiene como principal objetivo la implementación y estudio de diferentes técnicas de aprendizaje estadístico multivariadas aplicadas sobre el precio de oferta de los apartamento a la venta en Montevideo, Uruguay.

En éste sentido, existen diversos artículos y documentos de trabajo que hablan sobre el tema y que, en particular, trabajan con el precio de los inmuebles en el mercado Uruguayo.

Ponce 2012 propone un modelo de precios de fundamentos para las viviendas el cual se basa en el hecho de que una vivienda puede ser considerada como un activo de inversión y como un activo que brinda servicios, por lo que el precio puede ser considerado como el resultado del mercado por los servicios de vivienda y como el resultado de equilibrio en un mercado de activos. Encuentra, entre otras cosas, que los precios de las viviendas fluctúan más que lo justificado por sus fundamentos lo cual implica periodos de subvaloración o sobrevaloración de los precios de las viviendas. Precio de fundamentos para las viviendas en Uruguay. Ponce 2012.

Posteriormente, Ponce y Tubio 2013 realizan una aplicación de modelos hedónicos para el mercado Uruguayo. Utilizan una base de datos con información de inmuebles ofertados a través de la web, la cual contien información de más de 500 inmobiliarias y más de 20 barrios de Montevideo, Uruguay. Se trata de una sistematización de metodologías existentes para la elaboración de índices de precios de inmuebles, en particular atendiendo a modelos que permitan evaluar el desvío de los precios corrientes con respecto a los fundamentos del mercado. Precios de inmuebles: aproximaciones metodológicas y aplicación empírica. Ponce y Tubio 2013.

Luego, Landaberry y Tubio 2015 con el fin de monitorear el mercado de viviendas en Uruguay proponen una serie de índices de precios. En particular para el caso de Montevideo proponen índices desde un enfoque hedónico ya que, entre otras cosas, permiten estimar precios sombras para los atributos de las viviendas. Estimación de índice de precios de inmuebles en Uruguay, Landaberry y Tubio 2015.

En el año 2017 Goyeneche et al trabajaron en la construcción de un modelo predictivo del valor contado de un determinado inmueble, entendido como valor contado aquel que es asignado por el tasador. En línea con esto, implementaron técnicas de modelado autorregresivas para considerar la información temporal así como también la información espacial, a la vez que las variables hedónicas fueron modeladas mediante un modelo de regresión lineal dinámico, dónde los coeficientes de la regresión son función de la ubicación del inmueble. Recurrieron a la metodología de Stacking con el fin de lograr predicciones más precisas. En particular trabajaron con información de inmuebles en Montevideo, Uruguay. La base de datos utilizada fue proporcionada por el Banco Hipotecario del Uruguay BHU Predicción del valor de un inmueble mediante técnicas agregativas, Goyeneche et al 2017. 

Por último, en 2019 Picardo presenta modelos predictivos para el precio de las viviendas. En particular implementa modelos de regresión lineal, árboles de regresión, y random forest para inmuebles ubicados en Montevideo, Uruguay. La base de datos utilizada corresponde a elaboración propia a través de web scraping y registros administrativos de transacciones de la Dirección General de Registros DGR.

En lo que respecta a las técnicas estadísticas utilizadas, se utilizaron diferentes literaturas. 
En éste sentido, entre las bibliografías destcadas se encuentra el libro publicado por Gareth James et al  dónde realizan una introducción al aprendizaje estadístico con aplicaciones en R. 

Por otra parte, en lo que respecta al detalle metodológico del modelo líneal, fue consultado principalmente el trabajo de Francesc Carmona. Las pruebas de heteroscedasticidad y normalidad fueron consultadas bla bla.

Para el análisis de interpretabilidad de los modelos de aprendizaje automático fueron consultados los trabajos de bla bla.

\chapter{Marco teórico y metodología\label{cap:MT}}

\section{Supervisado – aprendizaje automatico\label{sec:machinelearning}}

Con el fin de obtener predicciones del precio de oferta de los inmuebles, fueron implementadas diferentes técnicas de aprendizaje automático. Estas consisten en modelar y analizar conjuntos de datos, mediante el aprendizaje de ejemplos, con el fin de predecir y estimar resultados en forma automática.

En este contexto, se realizó un análisis supervisado, en la medida de que se cuenta con una variable de salida ($Y$) y varias variables de entrada ($X$). Por lo tanto, se tiene que los posibles modelos son de la forma:

\begin{center}

$Y=f(x)+\epsilon$

\end{center}

Siendo $f$ una función desconocida y $\epsilon$ un error aleatorio independiente de $X$ e $Y$ con media 0. Se denota a la matriz $X$ de dimensión $n \times p$ a la matriz de datos, donde se tiene $n$ observaciones de entrenamiento y $p$ variables. 

La i-ésima fila se corresponde a la i-ésima observación (perteneciente al conjunto de entrenamiento) siendo de la forma $x_{i}=(x_{i1},\dots,x_{ip})^{T}$, con $x_{i}\in\mathbb R^{p}$. Por otro lado, se denota una nueva observación (o pertenciente al conjunto de testeo) como $x^{*}=(x_{i1}^{*},\dots,x^{*}_{ip})^{T}$, donde está es un vector p-dimensional (al igual que $x_{i}$). 

Se destaca que al ajustar los diferentes modelos se tomó como conjunto de entrenamiento a aproximadamente el $80\%$ de las observaciones. A la hora de estimar $f$, se realizó mediante métodos paramétricos y no paramétricos. 

En el primer caso, se asumió la forma funcional de $f$ y se procedió a estimar sus respectivos parámetros. Por otro lado, en los métodos no paramétricos, no se asumió la forma funcional de $f$.

\subsection{Modelo lineal de precios hedónicos \label{subsec:mlMT}}

El modelo lineal de precios hedónicos parte del supuesto de que los precios observados de los productos se pueden desglosar en una suma de cantidades específicas de determinadas características asociadas al bien. De esta manera se define un set implícito de precios, también conocidos como \textit{precios hedónicos}.

De ésta forma, el precio del bien es regresado sobre las características del mismo, y utilizando técnicas clásicas de estimación se obtienen los anteriormente mencionados precios hedónicos.

Formalmente, el modelo se especifíca como:

$$y = f(x) + \epsilon$$

dónde 

$$f(x) = \mathbf{X'}\boldsymbol{\beta}$$

$$\mathbf{X'}\boldsymbol{\beta} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k$$

$\epsilon$ es el vector de errores del modelo, con $\epsilon_i$ independientes e identicamente distribuidos $N(0, \sigma^2)$. (Carmona, 2003).

Siendo $(X_1, \cdots, X_k)$ el vector de las $k$ características asociadas al bien y $(\beta_1, \cdots, \beta_k)$ el vector de los precios hedónicos. Es importante observar que el vector de precios hedónicos asociados a las características coincide con el vector de parámetros de un modelo líneal clásico.

La estimación de los parámetros $\boldsymbol{\beta} = (\beta_1, \cdots, \beta_k)$ se realiza por el método de los mínimos cuadrados. Para ello, se intenta hallar los valores $\boldsymbol{\hat{\beta}} = (\hat{\beta_1}, \cdots, \hat{\beta_k})$ que minimicen la suma de cuadrados de los residuos $\boldsymbol{\epsilon'\epsilon} = \boldsymbol{(Y - X'\beta)'(Y - X'\beta)}$.

De ésta manera, se obtiene que la estimación del vector  $\mathbf{\beta}$ que minimiza la suma de los cuadrados de los residuos es solución de la siguiente ecuación:

$$\mathbf{X'X\beta=X'Y} (Carmona)$$

Y luego

$$\hat{y} = \hat{f(x)} =  \boldsymbol{X'\hat{\beta}}$$

En lo que respecta a la bondad de ajuste del modelo, se utilizan el coeficiente de determinación ($R^2$) y coeficiente de determinación ajustado ($R^2_a$).

Se define al coeficiente de determinación ($R^2$) como sigue:

$$R^2 = 1 - \frac{SCR}{SCT}$$

en dónde:

\begin{itemize}
\item $SCR = \sum_{i=1}^n (y_i - \hat{y_i})^2$
\item $SCT = \sum_{i=1}^n (y_i - \bar{y})^2$
\item $n$ el número de observaciones.
\end{itemize}

Por otro lado, se define el coeficiente de determinación ajustado como sigue:

$$R^2_a = 1 - \big(\frac{n-1}{n-k-1}\big)(1-R^2)$$

en dónde

\begin{itemize}
\item n es el número de observaciones
\item k es el número de variables explicativas del modelo
\item $R^2$ es el coeficiente de determinación.
\end{itemize}


En lo que respecta a la significación global del modelo, se utiliza la prueba \textit{F de Fischer} (Prueba F) en dónde la hipotesis nula implica la no significación global del modelo. La misma se rechaza cuando $F > F_{k, n-k-1}$ Siendo $F$ el estadístico de prueba (Carmona).

Con el fin de testear los supuestos de normalidad y homocedasticidad de los residuos del modelo existen diferentes pruebas. 

En partircular, en este trabajo se realizó la prueba de normalidad de \textit{Lilliefors} (1976) sobre los residuos del modelo. Ésta prueba utiliza el estadístico kolmogorov-smirnov para el caso en que la media y la varianza poblacional son desconocidos. Para ésto, se utilizó la función \textit{lillie.test} del paquete \textit{nortest}.

Por otro lado, se aplicó la prueba de heteroscedasticidad de \textit{Breush-Pagan} sobre los residuos del modelo, utilizando la función textit{bptest} del paquete \textit{lmtest}.Ésta prueba consiste en ajustar un modelo líneal para los residuos del modelo de regresión líneal. Se rechaza la hipótesis nula de heteroscedasticidad en el caso en que mucha varianza sea explicada por las variables explicativas. 

Por defecto, se utilizan todas las variables explicativas del modelo inicial.(Breush-Pagan 1979).

En particular, éste modelo puede aplicarse a los precios de los bienes inmuebles.
Entre las características asociadas al bien pueden considerarse características que son propias del mismo así como también características asociadas a la geolocaliación, entre otras. (De Bruyne, K., Van Hove, J., 2013).

Una de las ventajas más importantes de éste tipo de modelos es la fácil interpretación. No obstante suelen tener una mala performance predictiva en comparación a otros enfoques ya que puden presentar problemas de heteroscedasticidad, multicolinealidad, y variables omitidas.

Por otra parte, el modelo de precios hedónicos puede ser generalizado para el caso no lineal, lo cual no ha sido implementado en el presente trabajo.

\subsection{Árboles de regresión\label{subsec:arbol}}

Luego de relizar la implemetación del modelo lineal de precios hedónicos, se procedió a modelar mediante un árbol de decisión. En la medida de que se cuenta con una variable de salida continua, se construyó un árbol de regresión.

A pesar de que en la literatura existen diversos enfoques para la construcción de estos modelos, se trabajó con el método \textit{CART} el cual fue propuesto por \textit{Breiman}, \textit{Friedman}, \textit{Olshen} y \textit{Stone} en 1984.

Éste método se caracteriza por la realización de particiones binarias recursivas del espacio de las variables de entrada. Mediante las mismas, se conforma una organización jerárquica en forma de árbol, donde en cada nodo interior se tiene una pregunta (dicotómica) sobre una variable de entrada y en cada nodo terminal (denominado "hoja") una decisión.

De está forma, se procede a dividir el conjunto de los valores posibles de $X_{1}\dots,X_{p}$ (variables de entrada) en $J$ regiones disjuntas $R_{1},\dots,R_{J}$.(James, 2013)

Luego para cada observación que se encuentra en la región $R_{j}$ se realiza la misma predicción. Siendo está, en el contexto de árboles de regresión, el promedio de la variable respuesta en dicha región.

En el momento de la construcción de las regiones ($R_{1},\dots,R_{J}$) se realiza de tal forma que en cada subconjunto resultante (denominados como "nodos hijos") en cada iteración implique una disminución en la impureza de estos.
Para ello, se construyen las regiones $R_{1]$, \dots, $R_{j]$ de forma tal que minimicen la suma de cuadrados de los residuos (o por sus silabas en ingles \textit{RSS}).  
\begin{center}

$\displaystyle \sum^{J}_{j=1} \sum_{i\  \in R_{j}} \left(y_{i} - \hat{y}_{R_{j}} \right) ^{2}$ (Hastie, 2001)

\end{center}

Siendo $\hat{y}_{R_{j}}$ el promedio de la variable respuesta en la j-ésima región.
Para lograr este cometido se utiliza una separación recursiva binaria de la siguiente forma. Se selecciona la variable $X_{j}$ y el número $s$ dividiendo el espacio en dos regiones $R_{1}(j,s) = \lbrace{ X : X_{j} < s \rbrace}$ y $R_{2}(j,s) = \lbrace{ X: X_{j} \geq s \rbrace}$  de forma tal que se haga mínimo 

\begin{center}

$\displaystyle \sum_{i:x_{i} \in R_{1}(j,s)} \left(y - \hat{y}_{R_{1}}\right)^{2} + \sum_{i:x_{i} \in R_{2}(j,s)} \left(y - \hat{y}_{R_{2}}\right)^{2}$

\end{center}

Una vez encontrada la mejor partición se separan los datos en las regiones resultantes y se repite el proceso en cada región. Es decir, se busca nuevamente la mejor variable y el mejor punto de corte de forma se incremente la disminución de la impureza en los nodos hijos.

El proceso continúa hasta que se satisfaga algún criterio de parada. Un criterio de para puede ser por ejemplo que los nodos terminales tengan cierto número de observaciones.

Luego de definido el criterio de construcción de las regiones y el criterio de parada, se procede a realizar un proceso de poda en el árbol obtenido basado en un criterio de \textit{costo-complejidad}. Esto en la medida de que si se deja crecer el árbol de forma indefinida se obtiene un modelo con un alto grado de sobre ajuste (\textit{overfitting}). Por su contraparte, un árbol muy "pequeño", posiblemente no logre capturar la estructura del conjunto de datos. (Hastie, 2001).

El proceso de poda realizado, consiste en dejar crecer el árbol hasta que los nodos terminales tengan cierto número de observaciones (dicho árbol se denota como $T_{0}$). Luego se elige aquel subárbol el cual posee un menor error de predicción en el conjunto de testeo. En la medida de que un procedimiento de \textit{cross-validation} aplicado en cada posible subárbol es muy costoso en términos de "tiempo computacional", surge como alternativa el método basado en un criterio de \textit{costo-complejidad}.

En dicho método se define a $T_{\alpha}$ como un subárbol obtenido al podar a $T_{0}$. De esta forma, para cada $\alpha$ se busca $T_{\alpha}$ que minimice la siguiente expresión:

\begin{center}

$C_{\alpha}(T)=\displaystyle \sum^{|T|}_{m=1} N_{m} Q_{m}(T) + \alpha|T|$ (Hastie, 2001)

\end{center}

Donde se tiene que $|T|$ es igual número de nodos terminales del árbol $T$, mientras que $N_{m}$ es el número de observaciones en la región $R_{m}$. Por otro lado, la expresión $Q_{m}(T)$ consiste la medida de impureza.

En cuanto al parámetro $\alpha$, el mismo consiste en un parámetro de penalización aplicado a la complejidad (tamaño) del árbol. Donde valores altos de este, penalizan a árboles de gran tamaño. De esta forma, controla el compromiso entre la complejidad y la bondad de ajuste del modelo. Dicho parámetro se estima mediante \textit{cross-validation}.

\subsection{Bagging - Ranfom Forest\label{subsec:rf}}

A pesar de que los árboles de regresión poseen un alto grado de interpretabilidad, estos poseen la gran limitante de ser inestables. Esto en el sentido de que pequeñas variaciones en el conjunto de entrenamiento y testeo generan grandes cambios en las estimaciones.
Por lo tanto, se emplearon diferentes métodos alternativos buscando estabilidad en las predicciones.

En primer lugar, se aplicó el método \textit{Random Forest} desarrollado por \textit{Breiman} en 1994. Este método consiste en construir un estimador combinando distintas versiones de estimadores.
En este contexto, estas nuevas versiones se construyen generando nuevos conjuntos de entrenamiento, mediante la técnica de remuestreo \textit{bootstrap}. Ésta técnica consiste en la generación de varias muestras con reemplazo, del conjunto de datos de entrenamiento, donde a cada observación se le asigna el mismo peso ($\frac{1}{n}$, siendo $n$ el número de observaciones). Al número de muestras \textit{bootstrap} se le suele denotar con la letra $B$.
A la hora de utilizar este método en problemas de regresión, se procede a tomar varias muestras \textit{bootstrap}, donde a partir de cada una de ellas se construye un estimador. Luego, se le asigna a la observación el promedio de las respuestas de los estimadores construidos en cada muestra. 

Este método, en el contexto de árboles de regresión, consiste en la creación de $B$ árboles, cada uno mediante un nuevo conjunto de entrenamiento obtenido mediante una muestra \textit{bootstrap}. Se destaca que a estos árboles no se le realiza un proceso de poda. Por lo que estos mismos presentan una gran varianza, pero bajo sesgo. Sin embargo, al predecir mediante un promedio de los $B$ árboles, se logra una reducción considerable en la varianza del estimador y de esta forma se mejora la precisión de la predicción.

A su vez, el algoritmo a la hora de construir los diferentes estimadores (árboles), no considera en cada división el total de variables, sino un subconjunto de estas elegido de forma aleatoria. Como primera aproximación se procedió a utilizar la parte entera de $\sqrt{p}$, siendo $p$ el número de variables. Esto en la medida de que es el número de variables que utiliza por defecto la función \textit{ranger()} del paquete {ranger}. En etapas posteriores del análisis, se modifico el valor del mismo con el fin de obtener un mejor poder predictivo.

Este último punto es lo que diferencia al algoritmo con su versión más simple denominada \textit{Bagging} (también desarrollada por \textit{Breiman}). En este último se considera en cada división el total de las variables, por lo que resulta ser un caso particular del método \textit{Random Forest}.

Se optó por trabajar con el método \texit{Random Forest} ya que se destaca sobre el método \textit{Bagging} principalmente cuando se tiene que una variable es muy influyente. Esto se debe a que si se consideran todas las variables a la hora de construir los diferentes $B$ árboles, en la medida de que se tiene una variable muy influyente, posiblemente dichos árboles no difieran mucho entre sí. Esta limitante no se presenta en \textit{Random Forest} en el sentido de que selecciona de forma aleatoria un subconjunto de los predictores en cada iteración.

Una caracteristica relevante del método \textit{Random Forest} (al igual que en \textit{Bagging}), es que cada observación posee una probabilidad de aproximadamente $\frac{2}{3}$ de ser seleccionada en cada remuestra realizada. De esta forma, se cuenta con un conjunto de observaciones las cuales no son utilizadas para construir el estimador.

Este conjunto de observaciones se denomina como \textit{out of bag observations} (\textit{OOB}). Por lo tanto, en cada iteración se procede a predecir dichas observaciones, mediante el estimador obtenido. Repitiendo este procedimiento para las $n$ observaciones, se calcula el \textit{error OOB}. Dicha medida se procedió a utilizar como una primera aproximación en cuanto a la performance predictiva del modelo.

A pesar de que el método anteriormente mencionado logra solucionar el problema de la inestabilidad por parte de los árboles de decisión, este método se caracteriza por presentar una baja interpretabilidad. 

Sin embargo, en la medida de que se construyen varios árboles, es posible obtener cierta medida de la importancia de cada predictor. En los algoritmos \textit{Bagging} y \textit{Random Forest} se calcula la reducción de la medida de impureza en las divisiones de una variable dada promediando en todos los árboles obtenidos. De esta forma, si la reducción es  "grande" la variable se considera "importante".

\subsection{Boosting\label{subsec:boosting}}

Como segunda alternativa para obtener un modelo de predicción estable, se trabajo con el procedimiento \textit{Boosting} aplicado nuevamente a árboles de decisión. Este método, al igual que los anteriores (\textit{Random Forest} y \textit{Bagging}), consiste en la combinación de la salida de varios estimadores con el fin de producir un estimador más preciso.

Sin embargo, el mismo difiere con los métodos anteriores en la forma de realizar este proceso. Mientras que en \textit{Random Forest} (\textit{Bagging}) se procede a construir varios árboles mediante diferentes conjuntos de entrenamiento y combinando la predicción de cada uno, en \textit{Boosting} se trabaja mediante un enfoque secuencial. 

En este método, se construye una sucesión de estimadores, los cuales surgen de forma iterativa usando una modificación del conjunto de datos realizada a partir de la performance del estimador en el paso anterior. De esta forma, en cada iteración, se toma como variable de salida los residuos del modelo anterior y no a la variable de respuesta original ($Y$). Esto último, con el fin de realizar un proceso de actualización de los residuos del modelo y por lo tanto mejorando la predicción del estimador en áreas donde el mismo no realiza un buen ajuste. (Hastie, 2001)

Generalmente, en \textit{Boosting} cada arbol suele estar conformado por pocas particiones, por lo que el procedimiento de aprendizaje suele ser "lento". De forma resumida, el álgoritmo consiste en aplicar los siguientes pasos de forma iterativa:

1. Se establece $\hat{f}(x) = 0$ y $r_i = y_i$ en el conjunto de entrenamiento.

2. Para cada $b = 1, 2, \cdots, B$ repite:

a. Se ajusta un árbol $\hat{f}^b(x)$ con $d$ particiones (es decir, $d+1$ nodos terminales) en los datos de entrenamiento (X,r).

b. Se actualiza $\hat{f}$ agregando el nuevo árbol en una versión reducida:

$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$

c. Se actualizan los residuos 

$$r_i \leftarrow r_i - \lambda \hat{f}^b(x_i) $$

3. Se genera el modelo

$$\hat{f}(x) =  \displaystyle \sum_{b = 1}^B \lambda \hat{f}^b(x)$$
Donde, la letra $B$ denota el número de árboles utilizados en el algortimo. Se destaca que, a diferencia de \textit{Random Forest}, \textit{Boosting} puede generar un sobreajuste a los datos en caso que $B$ sea grande, a pesar de que éste sobreajuste ocurra de manera lenta. 

Por otro lado, el parámetro $\lambda$ controla la tasa a la que aprende el algoritmo. Donde $\lambda$ suele ser un número positivo pequeño, usualmente 0.01 o 0.001. A su vez, se tiene que generalemente, cuánto menor el valor de $\lambda$, mayor el número de arboles ($B$) necesarios. (Hastie, 2001)

Por último, la letra $d$ denota el número de particiones en cada árbol, el cual dicho parámetro controla la complejidad de cada estimador. Se observa que en el caso $d=1$ implica que cada árbol tenga solamente una partición y de esta forma se cuenta con un modelo aditivo (cada término involucra una sola variable). 

A su vez, el parámetro $d$ puede ser intepretado también como el parámetro que controla el orden de interacción entre los modelos, ya que las $d$ particiones pueden involucrar a los sumo $d$ variables. (Hastie, 2001)

Dichos parámetros ($B$, $\lambda$ y $d$) se estiman mediante una metodologia de validación cruzada.

\subsection{Support Vector Regression (SVR)\label{subsec:svr}}


Los modelos denominados \textit{Support Vector Regression} (SVR), surgen como una generalización aplicada a problemas de regresión de los modelos \textit{Support Vector Machine} (SVM). 

Por lo tanto, al ser una generalización de los SVM (en problemas de clasificación), poseen características muy similares, principalmente la robustez en cuanto a observaciones atípicas. De esta forma, se tiene que los SVR pertenecen al "grupo"  denominado \textit{robust regression}, donde en estos métodos se busca minimizar el efecto de observaciones atípicas en la ecuación de regresión. (Kunh-Johnson, 2013)

Estos métodos surgen como altenartiva a los modelos de regresión lineal, ya que estos ultimos a la hora de estimar los parámetros buscan minimizar la suma de cuadrados residuales (SSE). Lo cual conlleva que una observación que no sigue la tendencia del resto, puede ser muy influyente. (Kunh-Johnson, 2013)

A pesar de que existen varios enfoques para llevar acabo SVR en este trabajo se centró en el denominado $\epsilon$-\textit{insensitive regression} (Kunh-Johnson, 2013). En este contexto, a la hora de obtener las estimaciones de los parámetros del modelo, se define una nueva función de perdida denominada $\epsilon$-\textif{insensitive loss function}, siendo de la forma:

\begin{center}

$L(y,f(x,\alpha))=L(|y-f(x,\alpha)|_{\epsilon})$ (Vapnik, 2000)

\end{center}


\begin{center}

$|y-f(x,\alpha)|_{\epsilon} = \begin{cases} 0, & \mbox{si } |y-f(x,\alpha)| \leq \epsilon \mbox{} \\ |y-f(x,\alpha)|-\epsilon, & \mbox{en otro caso } \mbox{} \end{cases}$     (Vapnik, 2000)

\end{center}

En función a la ecuación anterior, se tiene que la perdida es igual a 0 si la discrepancia entre los predicho y lo observado es menor a $\epsilon$, siendo $\epsilon$ un limite fijado de antemano. Por lo tanto se tiene que tanto los outliers, como las observaciones que poseen un buen ajuste (residuos pequeños), no tienen efecto en la ecuación de regresión. 

En este contexto, para estimar los parámetros del modelo, SVR utiliza la función de perdida anteriormente definida, pero a su vez considerando un parámetro de penalización. En dicho método se busca obtener los coeficientes que minimizan la siguiente expresión:

\begin{center}

$C \displaystyle \sum_{i=1}^{n}L(|y_i-f(x_i,\beta)|_{\epsilon}) + \displaystyle \sum_{j=1}^{P} \beta^{2}_{j}$ (Kunh-Johnson, 2013)


\end{center}

Donde el parámetro $C$ es un parámetro de penalización, el cual generalmente se estima mediante cross-validation. En este contexto el parámetro $C$ cumple un rol de indicar la complejidad del modelo. Conforme aumenta el valor de éste el modelo obtiene mayor flexibilidad, en la medida que el efecto de los errores es aumentado. Por otro lado, al disminuir este parámetro el modelo se vuelve más rígido y con menor posibilidad de sobre ajustar a las observaciones.

Luego, se tiene que la solución al problema de minimización anteriormente mencionado, involucra el producto escalar entre las observaciones y no a las observaciones en si (Hastie, 2017). De esta forma, se puede re expresar a la función de regresión mediante la siguiente expresión:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} \langle x^{*},x_{i}\rangle$ (Kunh-Johnson, 2013) 

\end{center}

De esta forma, se tiene que para evaluar $f(x)$ es necesario el cálculo del producto escalar entre la nueva observación ($x^{*}$) y cada una de las observaciones pertenecientes al conjunto de entrenamiento. A su vez, se cuenta con $n$ parámetros $\alpha_{i}$ con $i=1,\dots,n$, donde cada uno corresponde a una observación de entrenamiento.

Sin embargo, en SVR se tiene la propiedad de que solo un subconjunto de los datos tiene un rol activo en la predicción de una nueva observación. Esto en la medida de que los parámetros $\alpha_{i}$ asociados a las observaciones de entrenamiento las cuales se encuentran a $\pm\ \epsilon$ de la recta de regresión (es decir se encuentran dentro del intervalo de longitud $2 \epsilon$ alrededor de la recta de regresión) son iguales a $0$. (Kunh-Johnson, 2013)

A las observaciones las cuales determinan a la recta de regresión se les denomina support vectors. Además, en la medida de que el predictor se encuentra sujeto al producto escalar entre la nueva observacion y las observaciones de entrenamiento (en particular solo aquellas que sean support vectors), se puede generalizar con el fin de captar relaciones no lineales entre las variables. 

Para ello se utiliza una función denominada \textit{kernel} la cual permite agrandar el espacio original de las variables, con el fin de obtener relaciones lineales en un nuevo espacio de mayor dimensión (James, 2013). Esta función es una generalización del producto escalar y se denota de la siguiente forma:


\begin{center}

$K(x_{i},x_{j})$ (James, 2013)

\end{center}

De esta forma el predictor queda expresado como:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} K(x^{*},x_{i})$ (Kunh-Johnson, 2013)

\end{center}

A la hora de aplicar SVR existen diferentes kernels lo cuales se podrían utilizar. Uno de lo más utilizados en la bibliografía se denomina \textit{radial kernel}. Este es de la forma:

\begin{center}

$K(x^{*},x_{i})=exp \left(-\gamma \displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}  \right)$, $\gamma>0$ (James, 2013)

\end{center}

En donde si la observación $x^{*}$ se encuentra lejos de la observación $x_{i}$ en terminos de distancia euclidia, entonces se tiene que $\displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}$ es una cantidad grande y por consecuente $K(x^{*},x_{i})$ es pequeño. Por lo tanto, $x_{i}$ no va a tener un rol activo a la hora de predecir el valor de $x^{*}$.

Esto significa que el radial kernel posee un comportamiento local, en el sentido de que  las observaciones de entrenamiento cercanas tienen un mayor efecto en la predicción del valor de una nueva observación.

Por otro lado, se tiene que $\gamma$ es un parámetro de escala, el cual afecta la varianza en la estimación. Al igual que $C$, dicho parámetro generalmente se estima mediante cross-validation.

Luego, se destacan dos aspectos de los modelos SVR. En primer lugar, en el caso de que la relación entre las variables sea realmente lineal (problemas de regresión), se recomienda usar un linear kernel (producto escalar) sobre un radial kernel. (Kunh-Johnson, 2013)
 
A su vez, en la medida de la ecuación de regresión ($f(x)$) se expresa a través del producto escalar entre las observaciones, se recomienda estandarizar las mismas con el fin de tener una misma unidad de medida. (Kunh-Johnson, 2013)


\section{Cross validation e Hyperparameter tuning \label{sec:cv}}

A la hora de evaluar la performance de los diferentes modelos planteados, se realizó un procedimiento de \textit{cross-validation}, particularmente \textit{k-folds}.
El algoritmo consiste en dividir la muestra en $k$ submuestras de igual tamaño. Luego $k-1$ submuestras se usan como datos de entrenamiento y la muestra restante $k$ se usa para testear los datos.
A continuación, se procede a ajustar los datos de esa muestra con el modelo construido con las $k-1$ muestras. Donde el proceso se repite $k$ veces, con cada una de las $k$ muestras. De tal forma que cada $k$ muestras es utilizada una sola vez como datos de testeo. 
De esta forma, todas las observaciones se usan tanto para train como para test. A su vez, cada observación se usa para test una sola vez y para train $k-1$ veces. Los errores obtenidos en cada etapa se promedian para producir una sola estimación (error medio obtenido de los $k$ análisis realizados).
Con el fin de medir el error de predicción del modelo en los modelos planteados anteriormente se consideró la siguiente medida:

\begin{center}

$RMSE= \displaystyle \frac{1}{k} \sum_{k=1}^{K} RMSE_{k}$

\end{center}

Donde $RMSE_{k}$ es la raiz del error cuadratico medio en la k-ésima muestra.

\begin{center}

$RMSE_k= \displaystyle \sqrt{ \frac{1}{n_k} \sum_{j = 1}^{n_k} (y_j - \hat{y}_j)^{2}}$

\end{center}

dónde $n_k$ es la cantidad de observaciones en la k-ésima muestra. 

Asimismo, como medida de la performance predictiva se utilizó también el error absoluto medio:

\begin{center}

$MAE= \displaystyle \frac{1}{k} \sum_{k=1}^{K} MAE_{k}$

\end{center}

Donde $MAE_{k}$ es el error absoluto medio en la k-ésima muestra.

\begin{center}

$MAE_k= \displaystyle \sqrt{ \frac{1}{n_k} \sum_{j = 1}^{n_k} (|y_j - \hat{y}_j|)}$

\end{center}

dónde $n_k$ es la cantidad de observaciones en la k-ésima muestra. 

Con el fin de obtener el modelo con la mejor performance predictiva, se realizó un proceso de \textit{hyperparameter tuning}. El mismo consiste en obtener la mejor combinación de hiperparámetros posibles mediante una metodología de cross-validation.

Un hyperparámetro es una valor necesario para ajustar un modelo el cual no se determina a partir 
de los datos sino que, por el contrario, es necesario que sea especificado previamente a la realización del ajuste. Dependiendo del algoritmo con el que se trabaje, el rol de los mismos puede variar. Por ejemplo, como fue mencionado para el caso de Random Forest se tiene 2 hyperparámetros de gran importancia: la cantidad de variables que son seleccionadas para ajustar cada arbol y la cantidad de árboles. 

\section{Interpretabilidad \label{sec:cv}}

Una vez implementados los diferentes algoritmos de aprendizaje automático y realizado el proceso de hiperparameter tunning, se procedió a obtener una medida de interpretabilidad de los mismos. Para ello, se trabajó con métodos globales modelo - agnosticos de interpretación, aplicados a aquel modelo con mejor performance predictiva y haciendo hincapié en el análisis gráfico.

Estos métodos de interpretación para modelos de caja negra, consiste en describir el compartamiento promedio del modelo. Donde, los mismos generalmente se expresan mediante un valor esperado, basado en la distribución de los datos. (Molnar, 2021)

A pesar de que en la literatura existen diferentes métodos para llevar a cabo el análsis, se trabajó mediante dos aproximaciones. Como primera aproximación, se realizó el análisis mediante los gráficos denominados \texit{Partial Dependece Plot} (PDP). Luego, con el fin de realizar un análisis con mayor profundidad y en la medida de que la metodología anterior presenta ciertas limitantes, se trabajó mediante el estudio de \texit{Accumulated local effect plots} (ALE).

En las siguientes subsecciones se detallan los principales aspectos teóricos de ambas metodologías, al igual que sus ventajas y limitantes.

\subsection{Partial Dependence Plot (PDP) \label{subsec:pdp}}

Los gráficos denominados partial dependence plot (PDP) permiten observar el efecto marginal que una o dos variables tienen sobre la predicción de la variable de respuesta obtenida a través de un algoritmo de machine learning (Molnar, 2021). Estos gráficos pueden detectar cuando la relación entre la variable de respuesta y la variable predictora de interés es líneal, monótona, o bien cuando se trata de una forma funcional más compleja. 

La función \textit{partial dependence function} para el caso de regresión se define como:

$$\begin{align*}\hat{f}_{x_S,PDP}(x_S)&=E_{X_C}\left[\hat{f}(x_S,X_C)\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C)d{}x_C\end{align*}$$


En dónde $X_s$ son las variables para las cuales se quiere conocer el efecto sobre la predicción, mientras que $X_C$ corresponde al resto de las variables utilizadas en el algoritmo de machine learning $\hat{f}$.

Ahora bien, la estimación para la función anterior se obtiene mediante la métodología Monte Carlo promediando sobre la muestra de entrenamiento:

$$\hat{f}_{x_S, PDP}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$

en dónde $X_C^{(i)}$ son los valores en la base de datos para las variables en las cuáles no estamos interesados, y $n$ el número de observaciones.

Es importante mencionar que PDP es un método que permite determinar de manera global la relación entre una o dos variables predictoras sobre la variable de respuesta. Asimismo, interesa destacar que uno de los supuestos de PDP es que las variables en $X_C$ y $X_S$ no están correlacionadas (Molnar, 2021).

De manera similar se obtiene una estimación PDP para el caso de variables categóricas en dónde, para cada categoría, se realiza la estimación PDP forzando a que todas las observaciones tomen el valor correspondiente a dicha categoría.

Una de las ventajas principales de PDP es que es un método intuitivo y en el caso de variables incorrelacionadas, tiene una interpretación clara. El gráfico PDP permite observar cómo cambia la predicción promedio cuando la h-ésima variable predictora cambia. No obstante éste resultado no es tan claro en el caso de correlación entre las variables con las que se construye el gráfico, ya que se construyen conbinaciones de las variables que son irreales o con probabilidad muy baja. Esto ocurrre ya que la función PDP en un punto en particular se obtiene forzando a que todos las observaciones tomen dicho valor en particular, lo cual puede no tener sentido en algunos casos. 

Una solución a éste problema es a través de la utilización de los denominados Accumulated Local Effect plots (ALE plots) que trabajan con distribuciones condicionales en lugar de con distribuciones marginales como es el caso de PDP. 

\subsection{Accumulated Local Effects (ALE) Plot \label{subsec:aleplot}}

Como fue mencionado en la sección anterior, una forma de evitar el cálculo de una predicción promedio en base a casos irreales o de baja probabilidad, es trabajar con distribuciones condicionales en lugar de distribuciones marginales. Es el caso de los conocidos M-Plots (Marginal Plots). 

La estimación de la predicción promedio para el caso de M-Plots se realiza de la siguiente manera:

$$\begin{align*}\hat{f}_{x_S,M}(x_S)&=E_{X_C|X_S}\left[\hat{f}(X_S,X_C)|X_S=x_s\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C|x_S)d{}x_C\end{align*}$$


M-plots logra superar el problema de los casos irreales o de baja probabilidad en la medida que se promedian predicciones solamente para observaciones que tienen valores cercanos al valor particular de la variable para el cual se está midiendo el efecto.

No obstante, no resuelve el problema en el caso de variables correlacionadas ya que igualmente se está estimando el efecto combinado de las variables correlacionadas.

ALE plots resuelve el problema promediando al igual que M-Plot sobre distribuciones marginales pero calculando diferencias en la predicción en lugar de promedios. 

La forma matemática es como sigue:

$$\begin{align*}\hat{f}_{x_S,ALE}(x_S)=&\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\left[\hat{f}^S(X_s,X_c)|X_S=z_S\right]dz_S-\text{constant}\\=&\int_{z_{0,1}}^{x_S}\int_{x_C}\hat{f}^S(z_s,x_c)\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\text{constant}\end{align*}$$

Dónde $\hat{f}^S(z_s,x_c)$ representa el cambio en la predicción lo cuál se define como la derivada parcial:

$$\hat{f}^S(x_s,x_c)=\frac{\partial\hat{f}(x_S,x_C)}{\partial{}x_S}$$

Esto permite aislar el efecto de la variable de interés sobre las predicciones.

Luego, se resta uns constante lo cual centra el gráfico de tal manera que el efecto sobre los datos sea cero. Mas adelante se hará énfasis en éste punto.

Ahora bien, a pesar de que no todos los modelos trabajan con gradientes, esto no es un problema dado que la metodología de computación de ALE plots utiliza intervalos en lugar de gradientes.

\subsubsection{Estimación \label{est}}

ALE plots, al igual que PDP y M-plots, puede ser estimado para una o dos variables de interés. En éste apartado se presenta el caso de una sola variable.

Para estimar los efectos locales de la variable $z_j$, se divide el recorrido de la misma en diferentes intervalos, y se calcula la diferencia en las predicciones en cada uno de dichos intervalos. Este procedimiento por lo tanto, funciona para el caso de modelos que no tienen derivadas parciales.

El primer paso es estimar el efecto como sigue:

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{j}^{(i)}\in{}N_j(k)}\left[f(z_{k,j},x^{(i)}_{\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]$$

Dónde $N_j(k)$ corresponde al k-ésimo intervalo de la variable de interés $z_j$.

Luego, el efecto calculado anteriormente se centra de manera que el efecto medio tome valor 0:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x^{(i)}_{j})$$

Esto se realiza para poder tener una medida del efecto de la variable de interés en un determinado punto en comparación a la predicción promedio.

En lo que respecta a los intervalos, los mismos son definidos utilizando los cuantiles de la distribución de la variable de interés, lo cual asegura que cada intervalo va a contener el mismo número de observaciones. La desventaja principal de utilizar los cuantiles para definir los intervalos es que los mismos pueden tener longitud muy diferente, haciendo que el gráfico sea muy irregular.

\section{Tratamiento de valores faltantes \label{sec:nas}}

La base de datos construida contiene variables con diferentes grado de datos faltantes. 

Éste problema fue abordado siguiendo dos estrategias de imputación. En primera instancia se entrenan diferentes modelos imputando solamente a las variables numéricas que tienen proporción de valores faltantes inferior a 0.15, y se procede en ésta primera instancia a realizar imputación por la media, lo cual es equivalente a asumir que el proceso generador de datos de éstos valores es un proceso aleatorio. Es decir, que los datos faltantes son generados al azar.

Posteriormente, se realizó un proceso de imputación de valores faltantes mediante un análisis supervisado. Para ello se trabajó de la siguiente manera:

1. Se ajustó un modelo tomando como variable de salida cada variable con datos faltantes.
2. En cada uno de ellos, se consideró como variables de entrada todas aquellas que originalmente no presentan datos faltantes, pero excluyendo la variable precio.
3. En cada variable, se sustituyó cada dato faltante por su predicción utilizando el modelo ajustado.

En particular, el algoritmo utilizado para ajustar los modelos fue Random Forest, mediante el paquete missRanger. En lo que respecta a hyperparameter tuning, se destaca que debido al tiempo computacional que conlleva fueron utilizados en todos los casos los valores por defecto.

Como fue mencionado anteriormente el criterio genérico para seleccionar las variables a imputar fue según la proporción de valores faltantes (proporción inferior a 0.15). Sin embargo, ésta metodología de imputación, a diferencia de la anterior, permite realizar el proceso para variables de tipo cualitativas. De esta forma, se incluyó en el análisis la variables item condition. No obstante, si bien existen otras variables que podrían ser incluídas, se decidió dejarlas fuera del análisis en la medida que no se consideran relevantes para el mismo (principalmente por no estar balancedas).

\chapter{Análisis exploratorio de  datos \label{cap:EDA}}

En éste capítulo se presenta los principales resultados obtenidos a la hora de realizar el análisis exploratorio de los datos, luego de realizar el proceso de depuración de los mismos detallado en la sección \ref{procesamiento}. De esta forma, la base de datos está conformada por un total de \Sexpr{paste(dim(aptos)[1])} observaciones y \Sexpr{paste(dim(aptos)[2])} variables. 

Por otra parte, una vez realizado el proceso de tratamiento de datos faltantes según se especifica en la sección \ref{nas} se mantiene un total de \Sexpr{paste(dim(aptos_sin_na)[2])} variables en el caso de imputación de valores faltantes por la media, y \Sexpr{paste(dim(aptos_mr)[2])} variables en el caso de imputación de valores faltantes por Random Forest. 

En la tabla AGREGAR REF A TABLA disponible en la sección del anexo \ref{varsA} se detalla el listado de variables utilizadas para la implementación de los diferentes modelos en etapas posteriores del análisis. 

Una vez obtenida la base de datos luego de realizar los procesos mencionados anteriormente, se procedió a analizar el comportamiento de la variable de salida Precio de oferta en dólares estadounidenses. A continuación se presenta en la tabla REF las principales medidas de resumen: 

<<>>=
sum.table <- aptos %>% summarise(Min = round(min(price),0),
                    Q1=round(quantile(price,.25),0),
                    Mediana = round(median(price),0),
                    Media = round(mean(price),0),
                    Q3 = round(quantile(price,.75),0),
                    Max = round(max(price),0),
                    Desvio = round(sd(price),0),
                    CV = round(Desvio/Media,2))

library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
kable(sum.table, caption = 'Medidas de resumen de la variable precio de oferta en dólares estadounidenses. En la medida que la media es superior a la mediana, se observa una asimetría hacia la derecha. Por otro lado, se observa un desvío de 76.788 dólares estadounidenses y un coeficiente de variación de 0.47.', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@

<<eval = FALSE>>=
aptos %>%
      ggplot(aes(x=price)) + 
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black', fill = 'navyblue') +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
             axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            legend.position = 'none') +
      ylab('') +
      scale_x_continuous(labels = comma) +
      geom_vline(xintercept = mean(aptos$price, na.rm = TRUE), 
                 colour = 'red', linetype = 'dashed') 
@

En la figura \ref{fig1} se presenta gráficamente la distribución de la variable mediante un gráfico de histograma.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
aptos %>% ggplot(aes(x=price)) + 
       geom_histogram(fill = 'navyblue', alpha = 0.9, bins = 40) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_text(face = 'bold', size = 12),
            axis.text.y = element_text(size = 12)) +
      labs(x = 'Precio de oferta en dolares estadounidenses') +
      scale_fill_manual(values = c('navyblue')) +
      geom_vline(xintercept = mean(aptos$price, na.rm = TRUE), 
                 colour = 'firebrick') +
      annotate("text", x = mean(aptos$price, na.rm = TRUE) + 72000, y = 5500, label = paste0("Media = USD ",round(mean(aptos$price, na.rm = TRUE))), color = 'firebrick') +
      scale_x_continuous(label = comma)
@
\captionof{figure}{Histograma del precio de oferta en dolares estadounidenses (price) de los apartamentos a la venta en Montevideo. El precio promedio es USD \Sexpr{paste(round(mean(aptos$price, na.rm = TRUE)))}}
\end{figure}
\label{fig1}

En lo que respecta a las variables de entrada, en primer lugar se presenta análisis de las variables cualítativas que se consideraron más relevantes. A continuación se presentan los gráficos de barras con el fin de observar la distribución de los niveles en los datos.

<<>>=
niveles <- aptos %>% group_by(bedrooms) %>% summarise(n = n())
colores <- data.frame(colores= c("gold1","darkviolet","green4", "dodgerblue2", "firebrick"))
niveles <- niveles %>% arrange(n)
niveles$id <- 1:nrow(niveles)
colores$id <- 1:nrow(niveles)
niveles <- left_join(niveles,colores,by="id")

niveles <- niveles %>% arrange(bedrooms)

p1 <- aptos %>% ggplot() +
  geom_bar(aes(x = bedrooms, y = (..count..)/sum(..count..), fill = bedrooms)) +
  theme(axis.ticks.x = element_blank(),
        legend.position = 'none',
        axis.title.y = element_blank(),
        axis.title.x = element_text(face = 'bold', size = 10),
        axis.text.x = element_text(face = 'bold')) + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(x = as.factor(bedrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
  labs(x = 'Cantidad de dormitorios') +
  scale_fill_manual(values = niveles$colores)

p2 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(full_bathrooms)), y = (..count..)/sum(..count..), fill = full_bathrooms)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(full_bathrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Cantidad de baños completos') +
      scale_fill_manual(values = c('aquamarine3','pink3'))

p3 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(zona_avditalia)), y = (..count..)/sum(..count..), fill = zona_avditalia)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(zona_avditalia),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Zona respecto a avenida italia') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))

p4 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(has_swimming_pool)), y = (..count..)/sum(..count..), fill = has_swimming_pool)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(has_swimming_pool),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'El edificio tiene piscina') +
      scale_fill_manual(values = c('mediumpurple2', 'salmon3'))

@

\begin{figure}[h!]
\centering
<<fig = TRUE, fig.pos = 'h'>>=
grid.arrange(p1, p2, p3, p4, ncol = 2)
@
\captionof{figure}{Gráficos de barras para diferentes variables cualitativas. En panel superior se encuentran a la izquierda la variable \text{bedrooms} y a la derecha la variable \text{full\_bathrooms}. Por otro lado, en el panel inferior a la izquierda se encuentra la variable \text{zona\_avditalia} y a la derecha \text{has\_swimming\_pool}}
\end{figure}

Con el fin de observar la distribución de algunas de las variables graficadas anteriormente a la vez que evaluar si las variables podrían ser candidatas a variables explicativas del modelo, se presentan a continuación algunos gráficos de violín y gráficos de caja.

<<>>=
p5 <- aptos %>% 
      ggplot(aes(x=as.character(zona_avditalia), y=price,
                 fill=as.character(zona_avditalia))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('Precio de publicación') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('orangered2', 'springgreen4'))

p6 <- aptos %>%
      filter(!is.na(full_bathrooms)) %>%
      ggplot(aes(x=as.character(full_bathrooms), y=price, 
                 fill=as.character(full_bathrooms))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('yellow4', 'palevioletred'))
@


\begin{figure}[h!]
\centering
<<fig = TRUE, width = 6, height=3>>=
grid.arrange(p5, p6, ncol =3)
@
\captionof{figure}{Gráficos de violín y gráficos de caja. A la izquierda se encuentra el gráfico de price según \text{zona\_avditalia} mientras que a la derecha se encuentra el gráfico de price según \text{full\_bathrooms}. Es posible observar que los gráficos sugieren la existencia de una diferencia en media de price según \text{zona\_avditalia}, teniendo una media superior los apartamentos que se encuentran al Sur de avenida italia. A su vez, los apartamentos con dos o más baños completos tienen una media superior respecto a los apartamentos con 1 solo baño completo.}
\end{figure}

Ahora bien, con el objetivo de evaluar posibles interacciones sobre las variables anteriormente graficadas, se presenta el gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}.


\begin{figure}[h!]
\centering
<<fig = TRUE,width = 6, height=3>>=
aptos %>% ggplot(aes(y=price, group = full_bathrooms, fill = full_bathrooms)) + 
       geom_boxplot() +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_blank()) +
      labs(x = 'Precio de oferta') +
      scale_fill_manual(values = c('orangered2', 'springgreen4')) +
      facet_grid(full_bathrooms~zona_avditalia) +
      scale_y_continuous(label = comma)
@
\captionof{figure}{Gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}. El gráfico sugiere que las diferencias en media observardas se mantienen cuando condicionamos en \text{full\_bathrooms}. En particular, entre los apartamentos con 2 o más baños completos. se observa una media superior para los apartamentos ubicados al sur respecto a los apartamentos ubicados al Norte. En lo que respecta a los apartamentos con un sólo baño completo, el gráfico sugiere que los apartamentos al Sur poseen una media superior respecto a los ubicados al Norte, si bien la diferencia es menos en éste último caso.}
\end{figure}

En lo que respecta a la variables \text{dist\_shop},se realizó la siguiente discretización para poder tener una aproximacón de su efecto sobre la variable price:

- Menos de 1 km: para volores inferiores a 1000
- Entre 1 km y 5 km para valores en el intervalo [1000, 5000)
- Mayor a 5 km para valores mayores a 5000

a continuación se presentá el gráfico de cajas del precio de oferta según \text{dist\_shop} por \text{zona\_avditalia} utilizando la discretización mencionada.

\begin{figure}[h!]
\centering
<<fig = TRUE, width = 6, height=3>>=
aptos %>% 
      mutate(dist_shop = factor(case_when(dist_shop < 1000 ~ 'Menos de 1 km',
      dist_shop < 5000 ~ 'Entre 1 km y 5 km',
      TRUE ~ 'Más de 5 km'))) %>%
      ggplot(aes(y=price,x=dist_shop,fill=zona_avditalia)) + 
      geom_boxplot() +      
      theme(axis.ticks.x = element_blank(),
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text()) +
      labs(y = 'Precio de oferta', x = 'Distancia al shopping más cercano') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))  +
      scale_y_continuous(label = comma)      
@
\captionof{figure}{Gráfico de caja de \text{dist\_shop} segun \text{zona\_avditali}a. En todos los casos los apartementos ubicados al Sur tienen una media superior. Asimismo, el gráfico sugiere que entre los apartamentos ubicados al Norte, los apartamentos ubicados a menos de 1 km de un shopping tienen una media superior.}
\end{figure}


\begin{figure}[h!]
\centering
<<fig = TRUE>>=
aptos_cor <- aptos %>% select(price, dist_shop, dist_rambla, covered_area,
                              total_area, no_covered_area) 

corr<-round(cor(aptos_cor, use='pairwise.complete.obs') , 2)

ggcorrplot(corr, method = c("square"),type=c("upper"), 
           ggtheme = ggplot2::theme_gray,lab=TRUE)
@
\captionof{figure}{Mapa de correlación. Como es de esperar, precio y distancia a la ramble éste de montevideo estan correlacionadas negativamente.}
\end{figure}

Por último, se presentan algunos gráficos que permiten observar la relación entre el precio de los apartamentos, el área cubierta, y la variable zona_avditalia.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
ggpairs(aptos, c(3,14), mapping = ggplot2::aes(color = zona_avditalia, alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.5)))
@
\captionof{figure}{Gráficos de price según \text{covered\_area}. En el panel superior izquierdo se encuentra la densidad de price segú \text{convered\_area}. En el panel superior derecho se encuentra la correlación total entre price y \text{covered\_area} y la correlación por grupos según \text{zona\_avditalia}. En el panel inferior izquierdo se encuentra el gráfico de dispersión de price según \text{covered\_area} por \text{zona\_avditalia}, mientras que en el panel inferior derecho se encuentra la densidad de \text{covered\_area} según \text{zona\_avditalia}.}
\end{figure}


\chapter{Resultados \label{cap:Resultados}}

En ésta sección se presentan los principales resultados obtenidos en el presente trabajo.
Como se menciona en la sección marco metodológico, en primer lugar se presentan los resultados del modelo lineal, seguido por las siguientes técnicas de aprendizaje estadístico: random forest, boosting, y support vector regression. Para estas últimas, a su vez se presentan los resultados obtenidos en el proceso de hyperparameter tuning.

Por último, en forma de resumen, se presenta un análisis comparativo de los diferentes modelos en función de su performance predictiva. A partir del mismo se selecciona el mejor modelo y a éste se le realiza un análisis gráfico de interpretabilidad.

Se destaca que todas las etapas anteriores se aplicaron considerando las dos técnicas de imputación mencionadas en la sección marco metodológico.

\section{Modelo lineal \label{sec:resml}}

A continuación se presentan los principales resultados del modelo de regresión lineal, con la variable price como variable explicada.

<<>>=
lm <- lm(price ~ ., data = train)
RMSE_lm <- sqrt(mean((test$price - predict(lm,test))^2))

lm_mr <- lm(price ~ ., data = train_mr)
RMSE_lm_mr <- sqrt(mean((test_mr$price - predict(lm,test_mr))^2))

lmt <- rbind(broom::glance(lm), broom::glance(lm))
lmt <- lmt %>% select(-"sigma" , - "AIC", - "BIC",
                  - "deviance", -"df.residual", -"nobs", -"df", -"logLik", -"p.value")

lmt <- lmt %>% mutate(p.value = "< 2.2e-16")

met <- tribble(
  ~Imput, ~RMSE,
  "Media", round(RMSE_lm), 
  "Random Forest",  round(RMSE_lm_mr)
)

lmt <- cbind(met, lmt)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(lmt, caption = c("Principales resultados de los modelos lineales ajustados según el método de imputación utilizados"), escape = F, booktabs = T, align = "c", format.args = list(big.mark = "."), col.names = c('Método', 'RMSE', "$R^2$", "$R^2_a$","Estadístico F", "P-Valor" )) %>% kable_styling(latex_options = "hold_position") %>% row_spec(0,bold=TRUE)
@


No se observa que el método de imputación genere diferencias sustanciales en término de RMSE. Por lo tanto se realiza el diagnóstico sobre los residuos del modelo solamente para el caso de imputación por la media.

\begin{figure}[h!]
\centering
<<fig = TRUE, width=6, height=3>>=
lm1 <- ggplot(as_tibble(lm$residuals) %>% 
             mutate(id = seq(1, length(lm$residuals), 1))) + 
      geom_point(aes(x = id, y = value), color = 'red', alpha = 1/5) +
      theme(axis.title.y = element_text(face = 'bold', size = 14),
            axis.title.x = element_text(),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(x = 'Id.', y = TeX("$\\hat{\\epsilon}$"))

mediana_reslm <- quantile(lm$residuals, probs = 0.5)
media_reslm <- mean(lm$residuals)

lm2 <- ggplot(as_tibble(lm$residuals)) + 
      geom_density(aes(value), color = 'red') +
      theme(axis.title = element_text(face = 'bold', size = 14),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(y = TeX("$\\hat{f}_{\\epsilon}$"), x = TeX("$\\hat{\\epsilon}$"))

grid.arrange(lm1, lm2, ncol = 2)
@
\captionof{figure}{Residuos del modelo líneal. En el panel izquiero se encuentra el gráfico de dispersión mientras que en el pane izquierdo se encuentra la densidad estimada.}
\end{figure}

Como fue mencionado en la sección \ref{subsec:mlMT} en primer lugar se realizó la prueba de normalidad de Lilliefors (1976) sobre los residuos del modelo. El p-valor de la prueba para los residuos del modelo lineal es menor a 0.5 con lo cual se rechaza la hipótesis nula de normalidad.

Por otro lado, en lo que respecta a la prueba de hetoroscedasticidad de Breuch-Pagan también se rechaza la hipótes nula de homoscedasticidad.

Estos resultados se mantienen incambiados según se utilice imputación de valores faltantes por la media o por Random Forest.

En el anexo \ref{mlA} se encuentran las tablas con los resultados de las pruebas aplicadas.

\section{Árbol de regresión \label{sec:resarbol}}

A continuación se presentan los resultados del árbol de regresión ajustado sobre la variable price utilizando las metodologías de imputación mencionadas en la sección marco metodológico. 

En primer lugar se presenta, para cada modelo ajustado, el gráfico luego de realizar el proceso de poda. Respecto a éste punto, en el \ref{arbolA} se presenta la tabla con los diferentes valores del parámetro de costo-complejidad, su error mediante un proceso de cross-validation asociado, al igual que el tamaño del árbol que dicho valor del parámetro implica y otras métricas de interés.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
##### Arbol de regresion

# summary(arbol)

# Proceso de poda

#broom::tidy(arbol$cptable)

# Grafico de la evolucion del error

cp_error <- data.frame(arbol$cptable)

# Obtengamos el cp

cp_opt <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"CP"]

npart <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"nsplit"]

#cp_error %>% ggplot(aes(x=CP,y=xerror))+geom_point(color="red")+geom_line()

# Otra forma

# Grafico

rpart.plot(arbol.prune,roundint = T,digits = -3)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable price mediante la base de datos construida en base a información obtenida de Mercado Libre,  una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt}. El método de imputación sobre valores faltantes es en éste caso imputación por la media. El árbol se conforma por \Sexpr{npart+1} nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart} particiones.}
\end{figure}

En función a lo observado en la figura, se tiene que la primera variable en realizar una partición binaria es la variable \text{full\_bathrooms}. Las variables \text{dist\_rambla}, \text{total\_area}, \text{bedrooms} e \text{ingresomedio\_ech} son utilizadas en las siguientes particiones.

El gráfico de árbol realizando imputación de valores faltantes por random forest se encuentra en el anezo debido a que no presenta diferencias sustanciales con el árbol realizando imputación por la media presentado e la figura anterior.

Una vez observada las principales características de los modelos, se procedió a obtener cierta medida de la performance predictiva de los mismos. Para ello, se calculó la raíz error cuadrático medio (RMSE). 

A continuación se presenta la tabla con los resultados para las metodologías de imputación utilizadas.

<<>>=
# RMSE test
RMSE_arbol <- sqrt(mean((test$price-predict(arbol.prune,test))^2))
RMSE_arbol_mr <- sqrt(mean((test_mr$price-predict(arbol.prune.mr,test_mr))^2))
 
r2_arbol <- sum((predict(arbol.prune,test)-mean(test$price))^2) /
                        sum((test$price - mean(test$price))^2)

r2_arbol_mr <- sum((predict(arbol.prune.mr,test_mr)-mean(test_mr$price))^2) /
                        sum((test_mr$price - mean(test_mr$price))^2)

mae_arbol <- mean(abs((test$price-predict(arbol.prune,test))))
mae_arbol_mr <- mean(abs((test_mr$price-predict(arbol.prune.mr,test_mr))))

res_arbol <- tribble(
  ~Imput, ~RMSE, ~R2, ~MAE, 
  "Media",   round(RMSE_arbol), r2_arbol, round(mae_arbol),
  "Random Forest", round(RMSE_arbol_mr), r2_arbol_mr, round(mae_arbol_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(res_arbol, caption = 'Principales medidas de resumen de los árboles de regresión, ajustados según metodologías de imputación pr la media y por Random Forest. Se presentan el RMSE, R2, y MAE', booktabs = T, align = "c", col.names = c('Método', 'RMSE', "$R^2$", "MAE"), escape = F, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@

\section{Random Forest \label{sec:resRF}}

En éste sección se presentan los resultados del modelo de Random Forest ajustado. Como fue mencionado anteriormente, en primer lugar se realizó el ajuste sobre los datos dónde las variables cuantitativas con proporción de datos faltantes inferior a 0.1 fueron estimadas por la media. Posteriormente, se ajustó el modelo utilizando un procesi de imputación mediante Random Forest. 

A continuación se presenta la tabla con los errores cuadráticos medios de los modelos mencionados. No se observa una diferencia entre los valores lo cual puede deberse a que las variables imputadas fueron seleccionadas con un bajo porcentaje de valores faltantes.

<<>>=
# RMSE test

predictions_rf <- predictions(predict(rf_train,test))
predictions_rf_mr  <- predictions(predict(rf_train_mr,test_mr))

RMSE_rf <- sqrt(mean((test$price-predictions_rf)^2))
RMSE_rf_mr <- sqrt(mean((test_mr$price - predictions_rf_mr)^2))

r2_rf <- sum((predictions_rf-mean(test$price))^2) /
      sum((test$price - mean(test$price))^2)

r2_rf_mr <- sum((predictions_rf_mr-mean(test_mr$price))^2) /
      sum((test_mr$price - mean(test_mr$price))^2)

mae_rf <- mean(abs((test$price-predictions_rf)))
mae_rf_mr <- mean(abs((test_mr$price-predictions_rf_mr)))

res_rf <- tribble(
      ~Imput, ~RMSE, ~R2, ~MAE, 
      "Media",   round(RMSE_rf), r2_rf, round(mae_rf),
      "Random Forest", round(RMSE_rf_mr), r2_rf_mr, round(mae_rf_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(res_rf, caption = 'Error cuadrático medio de los modelos ajustados por Random Forest', booktabs = T, align = "c", col.names = c('Método', 'RMSE', "$R^2$", "MAE"), escape = F, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@

\begin{figure}[h!]
\centering
<<fig=TRUE, width=6, height=3>>=
############ RF 

# Importancia de las variables

importancia_rf <- data.frame(rf_train$variable.importance)

colnames(importancia_rf) <- c("importance")

importancia_rf$variables <- row.names(importancia_rf)

#### Veamos imputación por missranger

# Importancia de las variables

importancia_rf_mr <- data.frame(rf_train_mr$variable.importance)

colnames(importancia_rf_mr) <- c("importance")

importancia_rf_mr$variables <- row.names(importancia_rf_mr)

p1 <- importancia_rf %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="",x="") +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_text(face = 'bold'),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 12),
            axis.title.y = element_blank()) +
      scale_fill_manual(values = c('navy', 'orangered3', 'darkgreen', 'slateblue3', 'brown3', 'gold3', 'springgreen3', 'pink3', 'royalblue4', 'firebrick'))

p2 <- importancia_rf_mr %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="",x="")+
      theme(axis.text.x = element_blank(),
            axis.text.y = element_text(face = 'bold'),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 12),
            axis.title.y = element_blank()) +
      scale_fill_manual(values = c('navy', 'orangered3', 'darkgreen', 'slateblue3', 'brown3', 'gold3', 'springgreen3', 'pink3', 'royalblue4', 'firebrick'))

grid.arrange(p1, p2, ncol = 2)
@
\captionof{figure}{Importancia de las variables en los modelos de random forest ajustados. Se observa que \text{total\_area} es la variable con mayor importancia en ambos modelos. A su vez, todas las variables geoespacialesconstruidas aparecen entre las más importantes de ambos modelos.}
\end{figure}


\section{Boosting \label{sec:resboosting}}

<<>>=
predictions_boost <- gbm::predict.gbm(boosting_train, test)
predictions_boost_mr <- gbm::predict.gbm(boosting_train_mr, test_mr)

RMSE_boosting <- RMSE(predictions_boost, test$price)
RMSE_boosting_mr <- RMSE(predictions_boost_mr , test_mr$price)


r2_boost <- sum((predictions_boost-mean(test$price))^2) /
      sum((test$price - mean(test$price))^2)

r2_rf_boost <- sum((predictions_boost_mr-mean(test_mr$price))^2) /
      sum((test_mr$price - mean(test_mr$price))^2)

mae_boost <- mean(abs((test$price-predictions_boost)))
mae_boost_mr <- mean(abs((test_mr$price-predictions_boost_mr)))

res_boost <- tribble(
      ~Imput, ~RMSE, ~R2, ~MAE, 
      "Media",   round(RMSE_boosting), r2_boost, round(mae_boost),
      "Random Forest", round(RMSE_boosting_mr), r2_rf_boost, round(mae_boost_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(res_boost, caption = 'Error cuadrático medio de los modelos ajustados por Boosting', booktabs = T, align = "c", col.names = c('Método', 'RMSE', "$R^2$", "MAE"), escape = F,  format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@

\section{Support vector regression \label{sec:ressvr}}

<<>>=
# RMSE test

predictions_svr <- kernlab::predict(SVR_train, test)
predictions_svr_mr <- kernlab::predict(SVR_train_mr, test_mr)

RMSE_svr <- RMSE(kernlab::predict(SVR_train, test), test$price)
RMSE_svr_mr <- RMSE(kernlab::predict(SVR_train_mr, test_mr), test_mr$price)

r2_svr <- sum((predictions_svr-mean(test$price))^2) /
                        sum((test$price - mean(test$price))^2)

r2_svr_mr <- sum((predictions_svr_mr-mean(test_mr$price))^2) /
                        sum((test_mr$price - mean(test_mr$price))^2)

mae_svr <- mean(abs((test$price-predictions_svr)))
mae_svr_mr <- mean(abs((test_mr$price-predictions_svr_mr)))

res_arbol <- tribble(
  ~Imput, ~RMSE, ~R2, ~MAE, 
  "Media",   round(RMSE_svr), r2_svr, round(mae_svr),
  "Random Forest", round(RMSE_svr_mr), r2_svr_mr, round(mae_svr_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<eval = FALSE, results=tex>>=
kable(res_arbol, caption = 'Principales medidas de resumen de los árboles de regresión, ajustados según metodologías de imputación pr la media y por Random Forest. Se presentan el RMSE, R2, y MAE', booktabs = T, align = "c", col.names = c('Método', 'RMSE', "$R^2$", "$MAE$"), escape = F, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


A continuación se presenta la tabla con los valores del RMSE para cada uno de los modelos implementados.

<<eval = FALSE>>=

lmt

# rmse_tot <- tribble(
#   ~Modelo, ~imput, ~RMSE,
#   "Modelo lineal", "Media", round(RMSE_lm),
#   "Arbol de regresion", "Media", round(RMSE_arbol),
#   "Random Forest", "Media", round(RMSE_rf),
#    "Support Vector Regression", "Media",0,
#   "Modelo lineal", "Random Forest", round(RMSE_lm_mr),
#   "Arbol de regresion",  "Random Forest", round(RMSE_arbol_mr),
#   "Random Forest",  "Random Forest", round(RMSE_rf_mr),
#    "Support Vector Regression",  "Random Forest", 0
# )

summary(lm_mr)

rmse_tot <- rbind(rmse_media,rmse_mr)

library(knitr)
options(knitr.table.format = "latex")
@

<<eval=FALSE>>=
kable(rmse_tot, caption = 'Raíz del error cuadrático medio (RMSE) de los modelos ajustados', booktabs = T, align = "c", col.names = c("Modelo", "Método de imputación","RMSE"), format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@


\section{Hyperparameter tuning \label{sec:hypt}}

En ésta sección se presentan los resultados de aplicar los procedimientos de hyperparameter tuning para cada uno de los modelos de aprendizaje automático presentados anteriormente.

La tabla que se presenta a continuación contiene los resultados del mejor modelo, en términos de 


<<>>=
#### Caret

#### RF

RF_caret_tunning_best <-RF_caret_tunning$results[which.min(RF_caret_tunning$results$RMSE), ] %>%
      select(-RMSESD, -RsquaredSD, -MAESD)

RF_caret_tunning_mr_best <-RF_caret_tunning_mr$results[which.min(RF_caret_tunning_mr$results$RMSE), ] %>% 
      select(-RMSESD, -RsquaredSD, -MAESD) 

#### Boosting

Boosting_caret_tunning_best <-Boosting_caret_tunning$tunning[which.min(Boosting_caret_tunning$tunning$RMSE), ]

Boosting_caret_tunning_mr_best <-Boosting_caret_tunning$tunning[which.min(Boosting_caret_tunning$tunning$RMSE), ]

# Best tunes

best_tunes_rf <- rbind(RF_caret_tunning_best, RF_caret_tunning_mr_best) %>% arrange(RMSE)

best_tunes_boost <- rbind(Boosting_caret_tunning_best, Boosting_caret_tunning_mr_best) %>% arrange(RMSE)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(best_tunes_rf, caption = 'Mejores modelos RF', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(best_tunes_boost, caption = 'Mejores modelos Boosting', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@


\section{Interpretabilidad \label{sec:cv}}


\section{Selección del mejor modelo e interpretabilidad \label{sec:cv}}

A continuación se presenta la tabla con los errores cuadráticos medios de los modelos implementados.

%%%%%%%%%%%%%%%%%%%%%%%%%%% ANEXO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{chapter}
\setcounter{section}
\setcounter{subsection}
\setcounter{subsubsection}

\begin{appendix}

\chapter{Anexo} 

\section{Variables \label{varsA}} 

<<>>=
vars <- readxl::read_excel(here('mercado_libre/API/datos', 'criterios_de_limpieza.xlsx'),
                                sheet = 'anexo_vars') %>% arrange(Fuente)
n <- vars %>% group_by(Fuente) %>% summarise(conteo = n())


aux <- as_tibble(matrix(rep(" ",4), ncol = 4, nrow = 1))
colnames(aux) <- colnames(vars)
vars <- bind_rows(aux, vars)

library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
kable(vars %>% select(-Fuente), caption = 'Descripción de las variables previo al proceso de depuración', booktabs = T, align = "c", longtable = TRUE, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = c("hold_position"), font_size = 10) %>% row_spec(0,bold=TRUE) %>%
      pack_rows("Fuente: API ML", 2, as.numeric(n[1,2]))  %>% 
      pack_rows("Fuente: Elaboración propia", as.numeric(n[1,2] + 1), nrow(vars)) 
@


<<>>=
########## Obtenemos variables que usamos
  
  
aux_vars <- vars %>% filter(Nombre %in% names(aptos_mr))
  
aux_vars <- aux_vars %>% mutate(`Imputación` = case_when(
    Nombre == 'no_covered_area' ~ 'Media - Random Forest',
    Nombre == 'total_area' ~ 'Media - Random Forest',
    Nombre == 'covered_area' ~ 'Media - Random Forest',
    Nombre == 'item_condition' ~ 'Random Forest',
    TRUE ~ 'No Requiere'
  ))

  
  # Tipo de datos
  
t_data <- sapply(aptos_mr,class) %>% data.frame() %>% rename(tipo = ".") 
      
t_data <- t_data %>% mutate(Nombre = rownames(t_data))
  
aux_vars <- aux_vars %>% select(Nombre,`Descripción`,`Imputación`) %>% 
    full_join(t_data,by="Nombre") %>% mutate(Naturaleza = ifelse(tipo =="factor",
                                                           "cualitativa",
                                                           "cuantitativa")) %>% 
relocate(Nombre,`Descripción`,Naturaleza,`Imputación`)

 aux_vars$`Descripción`[which(aux_vars$Nombre=="price")] <- "Precio de oferta en dolares estadounidenses"

library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
kable(aux_vars, caption = 'Variables utilizadas en los diferentes modelos implementados:nombre en la base de datos, descripción, naturaleza y método de imputación de valores faltantes utilizado.', booktabs = T, align = "c", 
        longtable = TRUE, format.args = list(big.mark = ".")) %>% 
    kable_styling(latex_options = c("hold_position"), font_size = 10) %>% 
    row_spec(0,bold=TRUE)
@


\section{Barrios de Montevideo \label{barriosA}}


<<>>=
barrios <- readr::read_csv(here('mercado_libre/Api/Datos', 'mapeo_barrios.csv'))
                          
library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
kable(barrios, caption = 'Barrios de Montevideo. En la columna identificador se especifica el código de cada barrio en la API de Mercado libre. En la columna Nombre ML se especifica el nombre asociado a cada identificador en MErcado Libre. En la columna Nombre INE se especifica el nombre INE asociado a cada nombre en Mercado Libre.', booktabs = T, align = "c", col.names = c('Identificador', 'Nombre ML', 'Nombre INE'), longtable = TRUE, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = c("hold_position"), font_size = 10) 
@

\section{Fórmula de Haversine para el cálculo de distancias \label{harvesineA}}

La fórmula de Haversine tiene la siguiente expresión:

$$d = 2 \, r \, \text{sen}^{-1} \bigg(\sqrt{ \text{sen}^{2} \, \frac{\phi_2 - \phi_1}{2} + \text{cos} (\phi_1) \, \text{cos}(\phi_2) \, \text{sen}^{2} \,  \frac{\psi_2 - \psi_1}{2}} \bigg)$$ (PONER AUTOR, Y FECHA) 

dónde d es la distancia entre dos puntos de longitud y latitud $(\psi_1, \phi_1)$ y $(\psi_2, \phi_2)$ respectivamente, y r el radio de la tierra.

\section{Proporción de valores faltantes por variable \label{nasA}} 

pegar

\section{Arbol de regresión de la variable precio en funcion de latitud y longitud \label{arbollatlonA}}

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
rpart.plot(arbol.prune.lat.lon,roundint = FALSE,digits = 4)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable price mediante la base de datos construida en base a información obtenida de Mercado Libre, considerando como variables de entrada latitud y longitud del apartamento. A dicho árbol se le realizó el correspondiente proceso de poda, el cual queda conformado por 7 nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. Donde a mayor intensidad del color azul, mayor la predicción en cuanto al precio de oferta del apartamento.}
\end{figure}


\section{Proporción de valores faltantes por variable\label{naA}}

<<>>=
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),3)) %>% data.frame() %>% 
   rename(prop_na=".") %>% arrange(desc(prop_na))
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(p_na %>% filter(p_na > 0), caption = 'Proporción de valores faltantes por variable', booktabs = T, align = "c", col.names = c("Proporción por variable"), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@


\section{Modelo lineal \label{mlA}} 

A continuación se presentan los resultados de las pruebas de normalidad y de homocedasticidad mencionadas en la sección \ref{sec:resml}. Esto considerando ambos métodos de imputación de valores faltantes.

<<>>=
bp <- lmtest::bptest(lm) # se rechaza h0 de varianza constante 

bp_mr <- lmtest::bptest(lm_mr) # se rechaza h0 de varianza constante 

bp_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media",   "< 2.2e-16", "Se rechaza H0",
  "Random Forest", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(bp_t, caption = 'Resultados de la prueba de Breush-Pagam para el análisis de homoscedasticidad de los residuos', booktabs = T, align = "c", col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@


<<>>=
lt <- lillie.test(residuals(lm)) #se rechaza h0 de normalidad

lt_mr <- lillie.test(residuals(lm_mr)) #se rechaza h0 de normalidad

lt_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media", "< 2.2e-16", "Se rechaza H0",
  "Random Forest", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(lt_t, caption = 'Resultados de la prueba de Llliefors para el análisis de normalidad de los residuos', booktabs = T, align = "c", col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@
\section{Arbol de regressión \label{arbolA}} 

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
##### Arbol de regresion

# Grafico de la evolucion del error

cp_error_mr <- data.frame(arbol_mr$cptable)

# Obtengamos el cp

cp_opt_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"CP"]

npart_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"nsplit"]

# Grafico

rpart.plot(arbol.prune.mr,roundint = T,digits = -3)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable price mediante la base de datos construida en base a información obtenida de Mercado Libre,  una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt_mr}. El método de imputación sobre valores faltantes es en éste caso imputación por Random Forest. El árbol se conforma por \Sexpr{npart_mr+1} nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart_mr} particiones.}
\end{figure}

A continuación, a modo de resumen se presenta la tabla de los modelos mencionados en la sección \ref{sec:resarbol}. La misma contiene diferentes valores del parámetro de costo-complejidad, su error mediante un proceso de cross-validation asociado, y el tamaño del árbol que dicho valor del parámetro implica.

Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro
de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation.

<<>>=

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(cp_error %>% select(-rel.error, -xstd), caption = 'Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation.   La metodología de imputación de valores  faltantes utilizada en éste caso es imputación por la media.', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@

<<>>=

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(cp_error_mr %>% select(-rel.error, -xstd), caption = 'Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation. La metodología de imputación de valores  faltantes utilizada en éste caso es imputación por Random Forest.', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


\section{Hyperparameter Tuning \label{hyptA}}

<<>>=
#### Caret

#### RF

RF_caret_res <- RF_caret$results %>% 
      select(-RMSESD, -RsquaredSD, -MAESD) %>% arrange(RMSE)

RF_caret_mr_res <-RF_caret_mr$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% arrange(RMSE)

RF_caret_tunning_res <-RF_caret_tunning$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% arrange(RMSE)

RF_caret_tunning_mr_res <-RF_caret_tunning_mr$results %>% 
      select(-RMSESD, -RsquaredSD, -MAESD) %>% arrange(RMSE)

#### Boosting

Boosting_caret_res <- Boosting_caret$tunning %>% arrange(RMSE)
Boosting_caret_tunning_res <-Boosting_caret_tunning$tunning  %>% arrange(RMSE)
@

<<>>=
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(RF_caret_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(RF_caret_mr_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(RF_caret_tunning_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(RF_caret_tunning_mr_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(Boosting_caret_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

<<results=tex>>=
kable(Boosting_caret_tunning_res, caption = 'Resumen', booktabs = T, align = "c", format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE) 
@

\end{appendix}

\end{document}




