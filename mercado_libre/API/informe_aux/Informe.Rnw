\documentclass[12pt,twoside,spanish,a4paper]{book}
\usepackage{geometry}\geometry{top=3cm,bottom=3cm,left=3cm,right=3cm}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
%\usepackage{mathrsfs}
\usepackage{longtable}
\usepackage{tocbibind}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{boxedminipage}
\usepackage[utf8]{inputenc}
%\usepackage[all,2cell,dvips]{xy}
\usepackage{graphicx}
\usepackage{float}
\usepackage[spanish,es-tabla]{babel}
%\usepackage{parskip}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{verbatim}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage{hyperref}
%\usepackage{fancyvrb}
\usepackage[authoryear]{natbib}
\usepackage{booktabs}
\usepackage{caption}
\fancyhf{} 
\fancyhead[LE]{\leftmark} 
\fancyhead[RO]{\nouppercase{\rightmark}} 
%\fancyfoot[LE,RO]{\thepage} 
\rfoot{\thepage} 
\pagestyle{fancy} 
\usepackage{caption}
\captionsetup[table]{position=bottom} 

\topmargin 2mm
\oddsidemargin 2mm
\evensidemargin 2mm

\makeindex
\setcounter{secnumdepth}{3}

\linespread{1.6}

\begin{document}
\SweaveOpts{concordance=TRUE,cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align = "c", caption = TRUE, comment = FALSE, fig.pos = "h"}

<<echo=FALSE>>=
options(scipen = 999, cache = TRUE, fig.pos = "h")  
@

<<librerias>>=
library(knitr)
library(tidyverse)
library(sf)
library(scales)
library(here)
library(gridExtra)
library(data.table)
library(magrittr)
library(ggcorrplot)
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(caret)
library(doParallel)
library(rpart)
library(rpart.plot)
library(rattle)
library(ranger)
library(missRanger)
library(kableExtra)
library(xtable)
library(RColorBrewer)
library(lmtest)
library(nortest)
library(glmnet)
library(GGally)
library(latex2exp)
library(gbm)
library(kernlab)
@


<<funciones, echo=FALSE, message=FALSE, warning=FALSE>>=
source(here("mercado_libre/API/funciones","funcion_imput_media.R"))
@

<<Datos_imputmedia>>=
options(scipen = 999)

#### DATOS

aptos_yearmonth <- list.files(path = here("mercado_libre/API/datos/limpios/apt"), 
                              pattern = "*.csv", full.names = T)

yearmonth <- c('aptos_202106','aptos_202107',"aptos_202108", "aptos_202109", "aptos_202110" )

aptos <- sapply(aptos_yearmonth, FUN=function(yearmonth){
      read_csv(file=yearmonth)}, simplify=FALSE) %>% bind_rows()


aptos <- aptos %>% group_by(id) %>% 
      arrange(desc(fecha_bajada)) %>%
      slice(1) %>% ungroup()

aptos <- aptos %>% mutate_if(is.character, as.factor)

# Filtramos por el criterio en price - eliminamos obs. con price superior al percentil 95%

aptos_todos <- aptos

aptos <- aptos %>% filter(price <= quantile(aptos$price,.95))

# Perdemos esta cantidad de registros

# nrow(aptos_todos) - nrow(aptos)

# vemos prop de NA
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),4)) %>% data.frame() %>% 
      rename(prop_na=".") %>% arrange(desc(prop_na))

#### Definimos variables Sin na imputamos por la media

aptos_sin_na <- imput_media(aptos,p=.1)

aptos_mr <- read_csv(here("mercado_libre/API/datos/limpios/apt/aptos_mr","aptos_mr.csv")) 
aptos_mr <- aptos_mr %>% mutate_if(is.character, as.factor)

############ train - test 

set.seed(1234)
ids <- sample(nrow(aptos_sin_na), 0.8*nrow(aptos_sin_na))

train <- aptos_sin_na[ids,]
test <- aptos_sin_na[-ids,]

train_mr <- aptos_mr[ids,]
test_mr <- aptos_mr[-ids,]

############## Cargamos los modelos

#### Arbol

load(here("mercado_libre/modelos/ARBOL","arbol_train.RDS")) #levanta objeto con nombre arbol

load(here("mercado_libre/modelos/ARBOL","arbol_prune_train.RDS")) #arbol.p

load(here("mercado_libre/modelos/ARBOL","arbol_train_mr.RDS")) #levanta objeto con nombre arbol
load(here("mercado_libre/modelos/ARBOL","arbol_prune_train_mr.RDS")) #arbol.p

#### RF

load(here("mercado_libre/modelos/RF","RF_train.RDS")) # rf_TRAIN

load(here("mercado_libre/modelos/RF","RF_train_mr.RDS")) # rf_train_mr


#### Boosting

load(here("mercado_libre/modelos/BOOSTING","boosting_train.RDS")) 

load(here("mercado_libre/modelos/BOOSTING","boosting_train_mr.RDS")) 

#### SVR

load(here("mercado_libre/modelos/SVR","SVR_train.RDS"))

load(here("mercado_libre/modelos/SVR","SVR_train_mr.RDS"))
@


<<mapas>>=
# Vectoria INE
mapa_barrio <- st_read(here("mercado_libre/API/scripts_aux/Mapas", "vectorial_INE_barrios/ine_barrios"), quiet = TRUE)
mapa_barrio <- st_transform(mapa_barrio, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

#Puntos shoppings
mall <- st_read(here("mercado_libre/API/scripts_aux/Mapas","puntos_googlemaps/shoppings"), quiet = TRUE)
mall <- st_transform(mall, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
mall <- mall %>% select(Name, geometry)

# Líneas avd_italia
avd_italia <- st_read(here("mercado_libre/API/scripts_aux/Mapas","lineas_googlemaps/avditalia_18"), quiet = TRUE)
avd_italia <- st_transform(avd_italia, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
avd_italia <- avd_italia %>% select(Name, geometry)

# Rambla Este - MVD
rambla <- st_read(here("mercado_libre/API/scripts_aux/Mapas/lineas_googlemaps", "ramblaeste_MVD"), quiet = TRUE)
rambla <- st_transform(rambla, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
rambla <- rambla %>% select(Name, geometry)
@


<<>>=
# Centroide barrios

#devuleve geometría con el centroide de cada barrios
centroide_barrios <- st_centroid(mapa_barrio)

# Extrae coordenadas (longitud y latitud) degeometría del centroide
centroide_barrios <- centroide_barrios %>%
      mutate(lon_barrio = st_coordinates(centroide_barrios$geometry)[,1],
             lat_barrio = st_coordinates(centroide_barrios$geometry)[,2])

# Pasa latitud y longitud del centroide a objeto sf
centroide_barrios_sf <- centroide_barrios %>% 
      st_as_sf(coords = c("lat_barrio","lon_barrio"), crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Tranforma coordenadas a formato long lat
centroide_barrios_sf_t <- st_transform(centroide_barrios_sf,crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

centroide_barrios <- centroide_barrios %>% 
      mutate(aux_lon = NA,
             zona_avditalia = NA)

# Extrae latitud y longitud de puntos en avd_italia (geometria)

puntos_avditalia <- st_coordinates(avd_italia)

puntos_avditalia <- as_tibble(puntos_avditalia) %>% select(-Z, -L1) %>%
      rename('lon_avditalia' = 'X',
             'lat_avditalia' = 'Y')

# Avd italia se conforma en total de 60 puntos
# Para cada barrios tomamos el punto en avd.italia con menor diferencia de longitud
# min {longitud centroide - longitud avd_italia }
# luego comparamos las latitudes del centroide y el punto de avd italia con mínima diferencia en cuanto a longitud
# si latitud avd italia < lat centroide barrio -> NORTE 
# si latitud avd italia >= lat centroide barrio -> NORTE 

for (i in 1:nrow(centroide_barrios)) {
      centroide_barrios$aux_lon[i] <- which.min(abs(centroide_barrios$lon_barrio[i] - 
                                                          puntos_avditalia$lon_avditalia))
      centroide_barrios$zona_avditalia[i] <- ifelse(
            puntos_avditalia$lat_avditalia[centroide_barrios$aux_lon[i]] < 
                  centroide_barrios$lat_barrio[i], 'Norte', 'Sur')
}


centroide_barrios <- centroide_barrios %>% 
      data.frame() %>% 
      select(NOMBBARR, zona_avditalia)

mapa_barrio <- mapa_barrio %>% left_join(centroide_barrios, by = 'NOMBBARR')
@

\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%% CARATULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\begin{center}

%\includegraphics[width=0.20\textwidth]{img/udelar_logo.jpg}

UNIVERSIDAD DE LA REPÚBLICA

Facultad de Ciencias Económicas y de Administración

Licenciatura en Estadística

Trabajo final de grado

\vspace{2.5cm}

\textbf{\large TÍTULO}

\vspace{1.5 cm}

\textbf{Lucia Coudet}
\textbf{Alvaro Valiño}


\end{center}


\vspace{2cm}

\noindent Tutores:\\
\noindent Natalia Da Silva\\


\vspace{1cm}

\begin{center}

\noindent Montevideo, Fecha.

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESUMEN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 'resumen.Rnw'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables


\setcounter{page}{1} 
 
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCCION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introducción \label{cap:Intro}}

El presente trabajo tiene como principal objetivo la implementación y estudio de diferentes técnicas de aprendizaje estadístico multivariadas, mediante las cuáles poder realizar predicciones de una variable de interés. En particular, se trbajó con el precio de oferta de los apartamentos a la venta en el departamento de Montevideo, Uruguay.

Con el fin de llevar a cabo éste objetivo, fueron realizadas todas las etapas necesarias para realizar un análisis estadístico, desde la obtención de los datos, limpieza y pre-procesamiento, hasta la generación de nuevas variable y modelado. 

De ésta manera, uno de los puntos de gran importancia de éste trabajo es la obtención y procesamiento de los datos. Gran parte de los datos fueron obtenidos través de la Interfaz para acceder a la página web (API) de Mercado Libre. Para ello fue necesario la creación de un código que permita una descarga automatizada de la información disponible. Asimismo, a partir de los datos obtenidos fueron construídas variables adicionales que se consideraron de interés para el problema planteado.

En lo que respecta a la técnicas de aprendizaje estadístico, se hizo especial énfasis en \textit{árboles de decisión}. Los mismos fueron utilizados no solamente para la predicción de la variable de respuesta sino también para la implementación de técnicas de imputación de valores faltantes.

Asimismo, con el fin de mejorar el desempeño predictivo de los modelos fue realizado un proceso de entraniento y validación de los resultados. 

En la medida que existe un \textit{trade off} entre interpretación del modelo y poder predictivo, fueron tomados dos enfoques. 

En primer lugar, se trabajó con técnicas de estimación clásicas tales como el modelo lineal de precios hedónicos. Por otro lado, se implentaron técnicas de aprendizaje automático. 

El modelo líneal clásico de precios hedónicos permite una interpretación directa de la relación entra la variable de respuesta y las covariables, pero supone la existencia de una relación lineal entre l variable de respuesta y las variables explicativas, el cuál es un supuesto que puede no ajustar a la realidad. 

Una de las ventajas de las técnicas de aprendizaje automático supervisado es que permiten captar relaciones no lineales. Por su parte, las metodologías como \textit{random forest} suelen tener un desempeño predictivo superior pero a expensas de una pérdida en el grado de interpretabilidad. Debido a ésto, se suele denominar a éstos modelos como \textit{modelos de caja negra}.

Sin embargo, con el fin de atenuar ésta limitante, se realizó un análisis de interpretabilidad de los modelos. Principalmente, mediante métodos globales modelo-agnósticos. Estos métodos permiten describir el comportamiento medio de los modelos de aprendizaje automático y son particularmente útiles cuando se quiere entender los mecanismos generales en los datos.

Se destaca que los resultados obtenidos fueron a través del lenguaje y entorno de programación para análisis estadístico y gráfico, \textit{R}, enfocándose en la optimización de todos los procesos principalmente mediante la programación en paralelo. 

Todas las etapas fueron realizando preservando la reproducibilidad de todos los resultados. 

\chapter{Datos \label{cap:datos}}

\section{Descripción de los datos utilizados \label{sec:desc}}

\section{Obtención \label{sec:obtencion}}

Los datos de precio de oferta de los apartamentos a la venta en Montevideo y gran parte de las variables utilizadas para modelizar fueron obtenidas a través de la Interfaz para acceder a la página web (API) de Mercado Libre \url{https://www.mercadolibre.com.uy/}. Para ello, es necesario registrarse en la web \url{https://developers.mercadolibre.com.uy/} y desde allí crear una aplicación. Una vez realizado este paso es posible obtener una clave (token) válida por un determinado lapso de tiempo que mercado libre proporciona para la conexión a su API.

De ésta manera y como fue mencionado, una vez que se obtiene el token es posible consultar la API. Para obtener la información de todos los inmuebles tanto a la venta como para el alquier en Uruguay es necesario realizar la consulta filtrando la categoría MLU1459. Sobre esta categoría existen distintas subcategorías. A los efectos del interés del presente trabajo fue consultada la API filtrando según la categoría MLU1474 que corresponde a todos los apartamentos a la venta en Uruguay.

Ahora bien, para obtener la información sobre los apartamentos a la venta en el departamento de Montevideo, se consultó la API filtrando según categoría MLU1474 y además filtrando según el id (identificador) de cada uno de los barrios en Montevideo.

Esto permitió obtener numerosos datos. No obstante, fue necesario acceder a los atributos específicos de cada publicación (ítem) filtrando específicamente en cada id de cada publicación. Es decir una vez obtenida la información disponible consultando la categoría MLU1474 y filtrando los barrios en Montevideo, se utilizaron los id entonces obtenidos de cada una de las publicaciones y se realizaron consultas por publicación.

De ésta manera fue posible obtener la información de todos los apartamentos a la venta en Montevideo disponible en la API de mercado libre y considerada de interés, de manera automatizada. El programa tarda aproximadamente 3 hs en obtener la totalidad de los datos, pudiendo variar según la máquina utilizada.

Para mayor información es posible consultar el código del script funcion api barrios. Disponible en el repositotio público en Github en el siguiebte link: \url{https://github.com/alvarovalinio/TFG/tree/main/mercado_libre/API/funciones}.


\section{Procesamiento y criterios de limpieza\label{sec:procesamiento}}

La obtención y limpieza de los datos fue una parte muy importante de éste trabajo.
En lo que respecta a la limpieza, el Anexo X contiene detalles específicos de los criterios de limpieza seleccionados para cada una de las variables en la base de datos.

En lo que respecta a las variables cuantitativas como precio de oferta del inmueble (price), áera cubierta (area\_total), áera cubierta (covered\_area), etre otras, un punto importante en la limpieza fue tratar de reconocer valores sin sentido, por ejemplo valores que repitan una secuencia de números como 11111, 5555555. 

Para ello fue construida una función auxiliar que es capaz de detectar cuándo un valor tiene 3 o más números iguales repetidos. En ese caso se considera que el dato es erróneo. Para el caso de la variable price la observación completa es eliminada, para todos los otros casos se imputa NA. 

Asimismo se eliminan las observaciones cuyo precio de oferta es inferior al valor del percentil 75\% entre las observaciones con precio inferior a USD 40.000 ya que se consideran datos erróneos o de muy baja frecuencia. De manera similar y debido a la elevada presencia de valores atípicos, se eliminan todas las observaciones cuyos valores de la variable price superan el percentil 95\% de la misma.

En lo que respecta a las variables total\_area y covered\_area se decidió asignar NA a todos los valores superiores a 1000 metros cuadrados, ya que se consideran datos erróneos. 

Existe un conjunto de variables que toman valor Si, No, NA. Para todas ellas se asume que los NA son No y se realiza la recodificación correspondiente en función de ello.

Valores de latitud y longitud en las georreferencias de los inmuebles que no corresponden a coordenadas geográficas dentro de Montevdeo (latitudes inferiores a -35 y superiores a -34.7 , y longitudes inferiores a -56.5 y superiores a -56 ) son considerados datos erróneos. De esta forma se optó por recodificarlos imputando la coordenada del centroide del barrio donde está ubicado el apartamento. 

Más aún, se pudo detectar la existencia de georreferencias incorrectas ya que en algunos casos la misma no se encontraba dentro del polígono del barrio dónde se ubica el apartamento. Para estos casos, asumimos que el dato correcto es el nombre del barrio y no la georreferencia específica, y se le imputa el valor de latitud y longitud correspondiente al centroide del barrio.

Dada la complejidad que implica la detección de éstas georreferencias erróneas, mediante un procedimiento de trade-off entre costo  y complejidad, se optó por utilizar el corte avenida italia en continuación con 18 de Julio para detectar éstos datos erróneos. En particular, se comparó la ubicación de la georreferencia respecto a avenida italia y 18 de julio (norte o sur) y la del centroide del barrrio, en caso que no coincidan, se imputó la georreferencia del baricentro de dicho barrio.

Asimismo, se decidió dejar fuera del análisis algunas de las variables con gran porcentaje de NA. Entre ellas cantidad de habitaciones (rooms), garages (parking\_lots), unidad (unit\_floors), piso (floor), condición nuevo o usado (item\_condition), antiguedad de la propiedad (property age), entre otras.

En lo que respecta al tipo de cambio, los valores del precio de oferta expresados en moneda nacional fueron convertidos a dólares estadounidenses utilizando el tipo de cambio de fecha de bajada de los datos. La obtención del valor del tipo de cambio se realiza de manera automatizada haciendo web scrapping sobre la págine del \textit{Instituto Nacional de Estadistica} (INE).

De manera similar, los gastos comunes expresados en dólares estadounidenses fueron convertidos a pesos uruguayos.

Por mayor información respecto a los criterios de limpieza seleccionados para cada una de las variables disponibles en la base de datos se recomienda dirigirse al Anexo X.

En el anexo \ref{varsA} se encuentra el detalle de las variables disponibles en la base de datos construida.

\section{Construcción de variables a partir de las georreferencias \label{sec:geo}}

La base de datos obtenida contiene información sobre la latitud y longitud dónde está ubicado cada apartamento lo cuál implica tener la georreferencia específica. Esto motivó la elaboración de variables geoespaciales como distancia al shopping más cercano, ubicación respecto a la calle avenida italia en continuación con la calle 18 de Julio, y distancia a la Rambla éste de Montevideo.

La metodología utilizada para el cálculo de las distancias fue a través de la fórmula del Haversine la cual permite el cómputo de la distancia mínima entre dos puntos que se encuentran en un cuerpo esférico usando latitud y longitud. Para ello en particular se trabajó con la función \textit{distm} del paquete \textit{geosphere}. En el anexo \ref{harvesineA} se encuentra el detalle de cálculo. 

\subsection{Zona respecto a avenida italia \label{subsec:zona}}

Toma valor Norte o Sur según el apartamento encuentre al Norte o al Sur de la calle avenida italia o 18 de Julio. Para ello, se fueron comparando las georreferencia de cada apartamento con respecto a la georreferencia del punto más cercano en el mapa de líneas de avenida italia en continuación con 18 de Julio. El mapa de líneas de avenida italia en continuación con 18 de julio fue construido en base a elaboración propia. 

\subsection{Distancia al shopping más cercano \label{subsec:shop}}

Fue contruida calculando la distancia en metros entre el apartamento y el shopping más cercano. Para ello, fue necesaria la obtención de las coordenadas geográficas (latitud y longitud) de todos los shoppings ubicados en montevideo y elaboración propia del archivo shapefile, la cual se detalle en la sección Fuentes externas de información.

Los shoppings georreferenciados son:

\begin{itemize}      
\item Montevideo shopping center
\item Punta Carretas shopping
\item Tres cruces shopping
\item Nuevocentro shopping
\item Portones shopping
\end{itemize}      

\subsection{Distancia a la rambla \label{subsec:rambla}}

Fue construida utilizando la distancia en metros entre el apartamento y la rambla este de Montevideo. Para selccionar qué zona de la rambla es la adecuada para diferenciar precio fue llevado a cabo un análisis de árbol de regresión.

A continuación se presenta el mapa del departamento de Montevideo, Uruguay, con las variables creadas.

\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
      geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'red', size = 0.5) +
      geom_sf(data = rambla, color = 'yellow2', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
      stat = "sf_coordinates", min.segment.length = 0,
      colour = "black", segment.colour = "black",
      size = 3, alpha = 0.8) +
      xlab('Longitud') +
      ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@
\captionof{figure}{Mapa de Montevideo, Uruguay con la georreferencia de los shoppings, calle avenida italia en continuación con la calle avenida 18 de julio y rambla éste de Montevideo . La línea roja indica a la calle avenida italia en continuación con la calle avenida 18 de julio mientras que la línea amarilla a la rambla éste de Montevideo.}
\end{figure}

<<echo=FALSE, eval=FALSE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'yellow', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
                                stat = "sf_coordinates", min.segment.length = 0,
                                colour = "black", segment.colour = "black",
                                size = 3, alpha = 0.8) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_manual(name = 'Zona avenida italia \n 18 de julio', values = c('orangered2', 'springgreen4'))
@

\section{Encuesta contínua de hogares\label{sec:ech}}

La variable ingresomedioECH fue construida utilizando la información de la encuesta contínua de hogares (ECH) del año 2020. En particular se utilizó la variable \textit{HT11}: Ingreso total del hogar con valor locativo sin servicio doméstico (en pesos uruguayos). Se calculó el ingreso medio por barrio de los hogares de Montevideo y luego se asignó a cada observación, el nivel de ingreso medio que le corresponda según el barrio dónde se encuentre el apartamento.

<<>>=

load(here("mercado_libre/API/ECH/RDATA_junio2021/HyP_2020_Terceros.RData"))

f <- f %>% select(numero, nper, hogar, nombarrio, HT11, ht13, YHOG, YSVL, lp_06, pobre_06,
             i228, i174, i259, i175, h155, h155_1, h156, h156_1, pesomen) %>%
      filter(hogar == 1)

# Para considerar pesos, multiplicar ingreso hogar i* peso hogar i
# Sum ingreso hogar i * peso hogar i / sum

f <- f %>% 
      group_by(nombarrio) %>%
      summarise(media_ingbarr = sum(pesomen*HT11, na.rm = TRUE) / 
                      sum(pesomen, na.rm = TRUE))

f <- f %>% rename('NOMBBARR' = 'nombarrio')

# quitamos espacios en blanco al final de nombbarr
f$NOMBBARR <- trimws(f$NOMBBARR, which = "right", whitespace = "[ \t\r\n]")

f$NOMBBARR <- recode(as.factor(f$NOMBBARR), 
                 'Malvín' = 'Malvin',
                  'Malvín Norte' = 'Malvin Norte')

mapa_barrio <- mapa_barrio %>% left_join(f, by = 'NOMBBARR')
@

A continuación se presenta el mapa del ingreso promedio de los hogares por barrio de Montevideo.

\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = media_ingbarr/1000 )) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_gradient(low = 'firebrick', high = 'darkgreen', name = "Ingreso promedio \n por mil ECH",labels = comma)
@
\captionof{figure}{Mapa del departamento de Montevideo, Uruguay, por nivel de ingreso promedio por mil obtenido a partir de la Encuesta Contínua de Hogares 2020.}
\end{figure}

\section{Fuentes externas de información\label{sec:fuentes}}

Para la construcción de las variables geoespaciales fue necesario recurrir a fuentes externas de información. Asímismo y como fue mencionado, en la construcción de la variable ingresomedio ECH fue utilizada también la encuesta contínua de hogares 2020. 

La herramienta utilizada para construir los archivos .kml que permite georreferenciar los shoppings en Montevideo, la calle avenida italia en continuación con 18 de julio, y la ramla éste de Montevideo, fue \textit{Google My Maps}, el cual es un servicio puesto en marcha por Google en abril del 2007, que permite a los usuarios crear mapas personalizados para uso propio o para compartir. Los usuarios pueden añadir puntos, líneas y formas sobre Google Maps. (Wikipedia, \url{https://es.wikipedia.org/wiki/Google_My_Maps}).

De ésta forma, fueron construidos los archivos .kml que contienen las georreferencias de los shoppings de montevideo y de la calle avenida italia en contrinuación con 18 de julio.

Una vez obtenidos los archivos .kml los mismos son transformados a archivos ESRI Shapefile utilizando QGis la cual es un Sistema de Información Geográfica SIG).

De manera similar fueron construidos los archivos que guardan la información necesaria para la construcción de la variable avenida italia en constinuación con 18 de Julio.

\subsection{Formato ESRI Shapefile \label{subsec:shapefile}}

El formato ESRI (Environmental Systems Research Institute, Inc.) Shapefile (SHP) es un formato de archivo informático propietario de datos espaciales desarrollado por la compañía ESRI, quien crea y comercializa software para Sistemas de Información Geográfica como Arc Info o ArcGIS. Originalmente se creó para la utilización con su producto ArcView GIS, pero actualmente se ha convertido en formato estándar de facto para el intercambio de información geográfica entre Sistemas de Información Geográfica por la importancia que los productos ESRI tienen en el mercado SIG y por estar muy bien documentado.

Un shapefile es un formato vectorial de almacenamiento digital donde se guarda la localización de los elementos geográficos y los atributos asociados a ellos. No obstante carece de capacidad para almacenar información topológica. Es un formato multiarchivo, es decir está generado por varios ficheros informáticos. El número mínimo requerido es de tres y tienen las extensiones siguientes:
      
\begin{itemize}       
\item  shp es el archivo que almacena las entidades geométricas de los objetos.
\item  shx es el archivo que almacena el índice de las entidades geométricas.
\item  dbf es la base de datos, en formato dBASE, donde se almacena la información de los atributos de los objetos.
\end{itemize}       

(Wikipedia, https://es.wikipedia.org/wiki/Shapefile)

A continuación s presentan el mapa de Montevideo con la georreferencia de los shoppings y de la calle avenida italia en continuación con 18 de julio. Es importente mencionar que la geometría del departamento de Montevideo fue construida con los archivos shapefile disponibles en la página web del Instituto Nacional de Estadística (INE) en la siguiente dirección: \url{https://www.ine.gub.uy/}.

\chapter{Antecedentes \label{cap:Antec}}

\chapter{Marco teórico y metodología\label{cap:MT}}

\section{Supervisado – aprendizaje automatico\label{sec:machinelearning}}

Con el fin de obtener predicciones del precio de oferta de los inmuebles, fueron implementadas diferentes técnicas de aprendizaje automático. Estas consisten en modelar y analizar conjuntos de datos, mediante el aprendizaje de ejemplos, con el fin de predecir y estimar resultados en forma automática.

En este contexto, se realizó un análisis supervisado, en la medida de que se cuenta con una variable de salida ($Y$) y varias variables de entrada ($X$). Por lo tanto, se tiene que los posibles modelos son de la forma:

\begin{center}

$Y=f(x)+\epsilon$

\end{center}

Siendo $f$ una función desconocida y $\epsilon$ un error aleatorio independiente de $X$ e $Y$ con media 0. Se denota a la matriz $X$ de dimensión $n \times p$ a la matriz de datos, donde se tiene $n$ observaciones de entrenamiento y $p$ variables. 

La i-ésima fila se corresponde a la i-ésima observación (perteneciente al conjunto de entrenamiento) siendo de la forma $x_{i}=(x_{i1},\dots,x_{ip})^{T}$, con $x_{i}\in\mathbb R^{p}$. Por otro lado, se denota una nueva observación (o pertenciente al conjunto de testeo) como $x^{*}=(x_{i1}^{*},\dots,x^{*}_{ip})^{T}$, donde está es un vector p-dimensional (al igual que $x_{i}$). 

Se destaca que al ajustar los diferentes modelos se tomó como conjunto de entrenamiento a aproximadamente el $80\%$ de las observaciones. A la hora de estimar $f$, se realizó mediante métodos paramétricos y no paramétricos. 

En el primer caso, se asumió la forma funcional de $f$ y se procedió a estimar sus respectivos parámetros. Por otro lado, en los métodos no paramétricos, no se asumió la forma funcional de $f$.

\subsection{Modelo lineal de precios hedónicos \label{subsec:mlMT}}

El modelo lineal de precios hedónicos parte del supuesto de que los precios observados de los productos se pueden desglosar en una suma de cantidades específicas de determinadas características asociadas al bien. De esta manera se define un set implícito de precios, también conocidos como \textit{precios hedónicos}.

De ésta forma, el precio del bien es regresado sobre las características del mismo, y utilizando técnicas clásicas de estimación se obtienen los anteriormente mencionados precios hedónicos.

Formalmente, el modelo se especifíca como:

$$y = f(x) + \epsilon$$

dónde 

$$f(x) = \mathbf{X'\beta} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p$$

$\epsilon$ es el vector de errores del modelo, con $\epsilon_i$ independientes e identicamente distribuidos $N(0, \sigma^2)$.

Siendo $(X_1, \cdots, X_p)$ el vector de las $p$ características asociadas al bien y $(\beta_1, \cdots, \beta_p)$ el vector de los precios hedónicos. Es importante observar que el vector de precios hedónicos asociados a las características coincide con el vector de parámetros de un modelo líneal clásico.

En particular, éste modelo puede aplicarse a los precios de los bienes inmuebles.
Entre las características asociadas al bien pueden considerarse características que son propias del mismo así como también características asociadas a la geolocaliación, entre otras. (De Bruyne, K., Van Hove, J., 2013).

Una de las ventajas más importantes de éste tipo de modelos es la fácil interpretación. No obstante suelen tener una mala performance predictiva en comparación a otros enfoques ya que puden presentar problemas de heteroscedasticidad, multicolinealidad, y variables omitidas.

Por otra parte, el modelo de precios hedónicos puede ser generalizado para el caso no lineal, lo cual no ha sido implementado en el presente trabajo.

Con el fin de testear los supuestos de normalidad y homocedasticidad de los residuos del modelo existen diferentes pruebas. 

En partiruclar, en este trabajose realizó la prueba de normalidad de Lilliefors (1976) sobre los residuos del modelo.Ésta prueba utiliza el estadístico kolmogorov-smirnov para el caso en que la media y la varianza poblacional son desconocidos. Para ésto, se utilizó la función \textit{lillie.test} del paquete \textit{nortest}.

Por otro lado, se aplicó la prueba de heteroscedasticidad de Breush-Pagan sobre los residuos del modelo, utilizando la función textit{bptest} del paquete \textit{lmtest}.

La prueba de hetoroscedasticidad de Breush-Pagan consiste en ajustar un modelo líneal para los residuos del modelo de regresión líneal y rechaza la hipótesis nula de heteroscedasticidad en el caso que mucha varianza sea explicada por las variables explicativas. Por defecto, se utilizan todas las variables explicativas del modelo inicial.(Breush-Pagan 1979)

\subsection{Árboles de regresión\label{subsec:arbol}}

Luego de relizar la implemetación del modelo lineal de precios hedónicos, se procedió a modelar mediante un árbol de decisión. En la medida de que se cuenta con una variable de salida continua, se construyó un árbol de regresión.

A pesar de que en la literatura existen diversos enfoques para la construcción de estos modelos, se trabajó con el método \textit{CART} el cual fue propuesto por \textit{Breiman}, \textit{Friedman}, \textit{Olshen} y \textit{Stone} en 1984.

Éste método se caracteriza por la realización de particiones binarias recursivas del espacio de las variables de entrada. Mediante las mismas, se conforma una organización jerárquica en forma de árbol, donde en cada nodo interior se tiene una pregunta (dicotómica) sobre una variable de entrada y en cada nodo terminal (denominado "hoja") una decisión.

De está forma, se procede a dividir el conjunto de los valores posibles de $X_{1}\dots,X_{p}$ (variables de entrada) en $J$ regiones disjuntas $R_{1},\dots,R_{J}$.(James, 2013)

Luego para cada observación que se encuentra en la región $R_{j}$ se realiza la misma predicción. Siendo está, en el contexto de árboles de regresión, el promedio de la variable respuesta en dicha región.

En el momento de la construcción de las regiones ($R_{1},\dots,R_{J}$) se realiza de tal forma que en cada subconjunto resultante (denominados como "nodos hijos") en cada iteración implique una disminución en la impureza de estos.
Para ello, se construyen las regiones $R_{1]$, \dots, $R_{j]$ de forma tal que minimicen la suma de cuadrados de los residuos (o por sus silabas en ingles \textit{RSS}).  
\begin{center}

$\displaystyle \sum^{J}_{j=1} \sum_{i\  \in R_{j}} \left(y_{i} - \hat{y}_{R_{j}} \right) ^{2}$ (Hastie, 2001)

\end{center}

Siendo $\hat{y}_{R_{j}}$ el promedio de la variable respuesta en la j-ésima región.
Para lograr este cometido se utiliza una separación recursiva binaria de la siguiente forma. Se selecciona la variable $X_{j}$ y el número $s$ dividiendo el espacio en dos regiones $R_{1}(j,s) = \lbrace{ X : X_{j} < s \rbrace}$ y $R_{2}(j,s) = \lbrace{ X: X_{j} \geq s \rbrace}$  de forma tal que se haga mínimo 

\begin{center}

$\displaystyle \sum_{i:x_{i} \in R_{1}(j,s)} \left(y - \hat{y}_{R_{1}}\right)^{2} + \sum_{i:x_{i} \in R_{2}(j,s)} \left(y - \hat{y}_{R_{2}}\right)^{2}$

\end{center}

Una vez encontrada la mejor partición se separan los datos en las regiones resultantes y se repite el proceso en cada región. Es decir, se busca nuevamente la mejor variable y el mejor punto de corte de forma se incremente la disminución de la impureza en los nodos hijos.

El proceso continúa hasta que se satisfaga algún criterio de parada. Un criterio de para puede ser por ejemplo que los nodos terminales tengan cierto número de observaciones.

Luego de definido el criterio de construcción de las regiones y el criterio de parada, se procede a realizar un proceso de poda en el árbol obtenido basado en un criterio de \textit{costo-complejidad}. Esto en la medida de que si se deja crecer el árbol de forma indefinida se obtiene un modelo con un alto grado de sobre ajuste (\textit{overfitting}). Por su contraparte, un árbol muy "pequeño", posiblemente no logre capturar la estructura del conjunto de datos. (Hastie, 2001).

El proceso de poda realizado, consiste en dejar crecer el árbol hasta que los nodos terminales tengan cierto número de observaciones (dicho árbol se denota como $T_{0}$). Luego se elige aquel subárbol el cual posee un menor error de predicción en el conjunto de testeo. En la medida de que un procedimiento de \textit{cross-validation} aplicado en cada posible subárbol es muy costoso en términos de "tiempo computacional", surge como alternativa el método basado en un criterio de \textit{costo-complejidad}.

En dicho método se define a $T_{\alpha}$ como un subárbol obtenido al podar a $T_{0}$. De esta forma, para cada $\alpha$ se busca $T_{\alpha}$ que minimice la siguiente expresión:

\begin{center}

$C_{\alpha}(T)=\displaystyle \sum^{|T|}_{m=1} N_{m} Q_{m}(T) + \alpha|T|$ (Hastie, 2001)

\end{center}

Donde se tiene que $|T|$ es igual número de nodos terminales del árbol $T$, mientras que $N_{m}$ es el número de observaciones en la región $R_{m}$. Por otro lado, la expresión $Q_{m}(T)$ consiste la medida de impureza.

En cuanto al parámetro $\alpha$, el mismo consiste en un parámetro de penalización aplicado a la complejidad (tamaño) del árbol. Donde valores altos de este, penalizan a árboles de gran tamaño. De esta forma, controla el compromiso entre la complejidad y la bondad de ajuste del modelo. Dicho parámetro se estima mediante \textit{cross-validation}.

\subsection{Bagging - Ranfom Forest\label{subsec:rf}}

A pesar de que los árboles de regresión poseen un alto grado de interpretabilidad, estos poseen la gran limitante de ser inestables. Esto en el sentido de que pequeñas variaciones en el conjunto de entrenamiento y testeo generan grandes cambios en las estimaciones.
Por lo tanto, se emplearon diferentes métodos alternativos buscando estabilidad en las predicciones.

En primer lugar, se aplicó el método \textit{Random Forest} desarrollado por \textit{Breiman} en 1994. Este método consiste en construir un estimador combinando distintas versiones de estimadores.
En este contexto, estas nuevas versiones se construyen generando nuevos conjuntos de entrenamiento, mediante la técnica de remuestreo \textit{bootstrap}. Ésta técnica consiste en la generación de varias muestras con reemplazo, del conjunto de datos de entrenamiento, donde a cada observación se le asigna el mismo peso ($\frac{1}{n}$, siendo $n$ el número de observaciones). Al número de muestras \textit{bootstrap} se le suele denotar con la letra $B$.
A la hora de utilizar este método en problemas de regresión, se procede a tomar varias muestras \textit{bootstrap}, donde a partir de cada una de ellas se construye un estimador. Luego, se le asigna a la observación el promedio de las respuestas de los estimadores construidos en cada muestra. 

Este método, en el contexto de árboles de regresión, consiste en la creación de $B$ árboles, cada uno mediante un nuevo conjunto de entrenamiento obtenido mediante una muestra \textit{bootstrap}. Se destaca que a estos árboles no se le realiza un proceso de poda. Por lo que estos mismos presentan una gran varianza, pero bajo sesgo. Sin embargo, al predecir mediante un promedio de los $B$ árboles, se logra una reducción considerable en la varianza del estimador y de esta forma se mejora la precisión de la predicción.

A su vez, el algoritmo a la hora de construir los diferentes estimadores (árboles), no considera en cada división el total de variables, sino un subconjunto de estas elegido de forma aleatoria. Como primera aproximación se procedió a utilizar la parte entera de $\sqrt{p}$, siendo $p$ el número de variables. Esto en la medida de que es el número de variables que utiliza por defecto la función \textit{ranger()} del paquete {ranger}. En etapas posteriores del análisis, se modifico el valor del mismo con el fin de obtener un mejor poder predictivo.

Este último punto es lo que diferencia al algoritmo con su versión más simple denominada \textit{Bagging} (también desarrollada por \textit{Breiman}). En este último se considera en cada división el total de las variables, por lo que resulta ser un caso particular del método \textit{Random Forest}.

Se optó por trabajar con el método \texit{Random Forest} ya que se destaca sobre el método \textit{Bagging} principalmente cuando se tiene que una variable es muy influyente. Esto se debe a que si se consideran todas las variables a la hora de construir los diferentes $B$ árboles, en la medida de que se tiene una variable muy influyente, posiblemente dichos árboles no difieran mucho entre sí. Esta limitante no se presenta en \textit{Random Forest} en el sentido de que selecciona de forma aleatoria un subconjunto de los predictores en cada iteración.

Una caracteristica relevante del método \textit{Random Forest} (al igual que en \textit{Bagging}), es que cada observación posee una probabilidad de aproximadamente $\frac{2}{3}$ de ser seleccionada en cada remuestra realizada. De esta forma, se cuenta con un conjunto de observaciones las cuales no son utilizadas para construir el estimador.

Este conjunto de observaciones se denomina como \textit{out of bag observations} (\textit{OOB}). Por lo tanto, en cada iteración se procede a predecir dichas observaciones, mediante el estimador obtenido. Repitiendo este procedimiento para las $n$ observaciones, se calcula el \textit{error OOB}. Dicha medida se procedió a utilizar como una primera aproximación en cuanto a la performance predictiva del modelo.

A pesar de que el método anteriormente mencionado logra solucionar el problema de la inestabilidad por parte de los árboles de decisión, este método se caracteriza por presentar una baja interpretabilidad. 

Sin embargo, en la medida de que se construyen varios árboles, es posible obtener cierta medida de la importancia de cada predictor. En los algoritmos \textit{Bagging} y \textit{Random Forest} se calcula la reducción de la medida de impureza en las divisiones de una variable dada promediando en todos los árboles obtenidos. De esta forma, si la reducción es  "grande" la variable se considera "importante".

\subsection{Boosting\label{subsec:boosting}}

Como segunda alternativa para obtener un modelo de predicción estable, se trabajo con el procedimiento \textit{Boosting} aplicado nuevamente a árboles de decisión. Este método, al igual que los anteriores (\textit{Random Forest} y \textit{Bagging}), consiste en la combinación de la salida de varios estimadores con el fin de producir un estimador más preciso.

Sin embargo, el mismo difiere con los métodos anteriores en la forma de realizar este proceso. Mientras que en \textit{Random Forest} (\textit{Bagging}) se procede a construir varios árboles mediante diferentes conjuntos de entrenamiento y combinando la predicción de cada uno, en \textit{Boosting} se trabaja mediante un enfoque secuencial. 

En este método, se construye una sucesión de estimadores, los cuales surgen de forma iterativa usando una modificación del conjunto de datos realizada a partir de la performance del estimador en el paso anterior. De esta forma, en cada iteración, se toma como variable de salida los residuos del modelo anterior y no a la variable de respuesta original ($Y$). Esto último, con el fin de realizar un proceso de actualización de los residuos del modelo y por lo tanto mejorando la predicción del estimador en áreas donde el mismo no realiza un buen ajuste. (Hastie, 2001)

Generalmente, en \textit{Boosting} cada arbol suele estar conformado por pocas particiones, por lo que el procedimiento de aprendizaje suele ser "lento". De forma resumida, el álgoritmo consiste en aplicar los siguientes pasos de forma iterativa:

1. Se establece $\hat{f}(x) = 0$ y $r_i = y_i$ en el conjunto de entrenamiento.

2. Para cada $b = 1, 2, \cdots, B$ repite:

a. Se ajusta un árbol $\hat{f}^b(x)$ con $d$ particiones (es decir, $d+1$ nodos terminales) en los datos de entrenamiento (X,r).

b. Se actualiza $\hat{f}$ agregando el nuevo árbol en una versión reducida:

$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$

c. Se actualizan los residuos 

$$r_i \leftarrow r_i - \lambda \hat{f}^b(x_i) $$

3. Se genera el modelo

$$\hat{f}(x) =  \displaystyle \sum_{b = 1}^B \lambda \hat{f}^b(x)$$
Donde, la letra $B$ denota el número de árboles utilizados en el algortimo. Se destaca que, a diferencia de \textit{Random Forest}, \textit{Boosting} puede generar un sobreajuste a los datos en caso que $B$ sea grande, a pesar de que éste sobreajuste ocurra de manera lenta. 

Por otro lado, el parámetro $\lambda$ controla la tasa a la que aprende el algoritmo. Donde $\lambda$ suele ser un número positivo pequeño, usualmente 0.01 o 0.001. A su vez, se tiene que generalemente, cuánto menor el valor de $\lambda$, mayor el número de arboles ($B$) necesarios. (Hastie, 2001)

Por último, la letra $d$ denota el número de particiones en cada árbol, el cual dicho parámetro controla la complejidad de cada estimador. Se observa que en el caso $d=1$ implica que cada árbol tenga solamente una partición y de esta forma se cuenta con un modelo aditivo (cada término involucra una sola variable). 

A su vez, el parámetro $d$ puede ser intepretado también como el parámetro que controla el orden de interacción entre los modelos, ya que las $d$ particiones pueden involucrar a los sumo $d$ variables. (Hastie, 2001)

Dichos parámetros ($B$, $\lambda$ y $d$) se estiman mediante una metodologia de validación cruzada.

\subsection{Support Vector Regression (SVR)\label{subsec:svr}}


Los modelos denominados \textit{Support Vector Regression} (SVR), surgen como una generalización aplicada a problemas de regresión de los modelos \textit{Support Vector Machine} (SVM). 

Por lo tanto, al ser una generalización de los SVM (en problemas de clasificación), poseen características muy similares, principalmente la robustez en cuanto a observaciones atípicas. De esta forma, se tiene que los SVR pertenecen al "grupo"  denominado \textit{robust regression}, donde en estos métodos se busca minimizar el efecto de observaciones atípicas en la ecuación de regresión. (Kunh-Johnson, 2013)

Estos métodos surgen como altenartiva a los modelos de regresión lineal, ya que estos ultimos a la hora de estimar los parámetros buscan minimizar la suma de cuadrados residuales (SSE). Lo cual conlleva que una observación que no sigue la tendencia del resto, puede ser muy influyente. (Kunh-Johnson, 2013)

A pesar de que existen varios enfoques para llevar acabo SVR en este trabajo se centró en el denominado $\epsilon$-\textit{insensitive regression} (Kunh-Johnson, 2013). En este contexto, a la hora de obtener las estimaciones de los parámetros del modelo, se define una nueva función de perdida denominada $\epsilon$-\textif{insensitive loss function}, siendo de la forma:

\begin{center}

$L(y,f(x,\alpha))=L(|y-f(x,\alpha)|_{\epsilon})$ (Vapnik, 2000)

\end{center}


\begin{center}

$|y-f(x,\alpha)|_{\epsilon} = \begin{cases} 0, & \mbox{si } |y-f(x,\alpha)| \leq \epsilon \mbox{} \\ |y-f(x,\alpha)|-\epsilon, & \mbox{en otro caso } \mbox{} \end{cases}$     (Vapnik, 2000)

\end{center}

En función a la ecuación anterior, se tiene que la perdida es igual a 0 si la discrepancia entre los predicho y lo observado es menor a $\epsilon$, siendo $\epsilon$ un limite fijado de antemano. Por lo tanto se tiene que tanto los outliers, como las observaciones que poseen un buen ajuste (residuos pequeños), no tienen efecto en la ecuación de regresión. 

En este contexto, para estimar los parámetros del modelo, SVR utiliza la función de perdida anteriormente definida, pero a su vez considerando un parámetro de penalización. En dicho método se busca obtener los coeficientes que minimizan la siguiente expresión:

\begin{center}

$C \displaystyle \sum_{i=1}^{n}L(|y_i-f(x_i,\beta)|_{\epsilon}) + \displaystyle \sum_{j=1}^{P} \beta^{2}_{j}$ (Kunh-Johnson, 2013)


\end{center}

Donde el parámetro $C$ es un parámetro de penalización, el cual generalmente se estima mediante cross-validation. En este contexto el parámetro $C$ cumple un rol de indicar la complejidad del modelo. Conforme aumenta el valor de éste el modelo obtiene mayor flexibilidad, en la medida que el efecto de los errores es aumentado. Por otro lado, al disminuir este parámetro el modelo se vuelve más rígido y con menor posibilidad de sobre ajustar a las observaciones.

Luego, se tiene que la solución al problema de minimización anteriormente mencionado, involucra el producto escalar entre las observaciones y no a las observaciones en si (Hastie, 2017). De esta forma, se puede re expresar a la función de regresión mediante la siguiente expresión:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} \langle x^{*},x_{i}\rangle$ (Kunh-Johnson, 2013) 

\end{center}

De esta forma, se tiene que para evaluar $f(x)$ es necesario el cálculo del producto escalar entre la nueva observación ($x^{*}$) y cada una de las observaciones pertenecientes al conjunto de entrenamiento. A su vez, se cuenta con $n$ parámetros $\alpha_{i}$ con $i=1,\dots,n$, donde cada uno corresponde a una observación de entrenamiento.

Sin embargo, en SVR se tiene la propiedad de que solo un subconjunto de los datos tiene un rol activo en la predicción de una nueva observación. Esto en la medida de que los parámetros $\alpha_{i}$ asociados a las observaciones de entrenamiento las cuales se encuentran a $\pm\ \epsilon$ de la recta de regresión (es decir se encuentran dentro del intervalo de longitud $2 \epsilon$ alrededor de la recta de regresión) son iguales a $0$. (Kunh-Johnson, 2013)

A las observaciones las cuales determinan a la recta de regresión se les denomina support vectors. Además, en la medida de que el predictor se encuentra sujeto al producto escalar entre la nueva observacion y las observaciones de entrenamiento (en particular solo aquellas que sean support vectors), se puede generalizar con el fin de captar relaciones no lineales entre las variables. 

Para ello se utiliza una función denominada \textit{kernel} la cual permite agrandar el espacio original de las variables, con el fin de obtener relaciones lineales en un nuevo espacio de mayor dimensión (James, 2013). Esta función es una generalización del producto escalar y se denota de la siguiente forma:


\begin{center}

$K(x_{i},x_{j})$ (James, 2013)

\end{center}

De esta forma el predictor queda expresado como:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} K(x^{*},x_{i})$ (Kunh-Johnson, 2013)

\end{center}

A la hora de aplicar SVR existen diferentes kernels lo cuales se podrían utilizar. Uno de lo más utilizados en la bibliografía se denomina \textit{radial kernel}. Este es de la forma:

\begin{center}

$K(x^{*},x_{i})=exp \left(-\gamma \displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}  \right)$, $\gamma>0$ (James, 2013)

\end{center}

En donde si la observación $x^{*}$ se encuentra lejos de la observación $x_{i}$ en terminos de distancia euclidia, entonces se tiene que $\displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}$ es una cantidad grande y por consecuente $K(x^{*},x_{i})$ es pequeño. Por lo tanto, $x_{i}$ no va a tener un rol activo a la hora de predecir el valor de $x^{*}$.

Esto significa que el radial kernel posee un comportamiento local, en el sentido de que  las observaciones de entrenamiento cercanas tienen un mayor efecto en la predicción del valor de una nueva observación.

Por otro lado, se tiene que $\gamma$ es un parámetro de escala, el cual afecta la varianza en la estimación. Al igual que $C$, dicho parámetro generalmente se estima mediante cross-validation.

Luego, se destacan dos aspectos de los modelos SVR. En primer lugar, en el caso de que la relación entre las variables sea realmente lineal (problemas de regresión), se recomienda usar un linear kernel (producto escalar) sobre un radial kernel. (Kunh-Johnson, 2013)
 
A su vez, en la medida de la ecuación de regresión ($f(x)$) se expresa a través del producto escalar entre las observaciones, se recomienda estandarizar las mismas con el fin de tener una misma unidad de medida. (Kunh-Johnson, 2013)


\section{Cross validation e Hyperparameter tuning \label{sec:cv}}

A la hora de evaluar la performance de los diferentes modelos planteados, se realizó un procedimiento de \textit{cross-validation}, particularmente \textit{k-folds}.
El algoritmo consiste en dividir la muestra en $k$ submuestras de igual tamaño. Luego $k-1$ submuestras se usan como datos de entrenamiento y la muestra restante $k$ se usa para testear los datos.
A continuación, se procede a ajustar los datos de esa muestra con el modelo construido con las $k-1$ muestras. Donde el proceso se repite $k$ veces, con cada una de las $k$ muestras. De tal forma que cada $k$ muestras es utilizada una sola vez como datos de testeo. 
De esta forma, todas las observaciones se usan tanto para train como para test. A su vez, cada observación se usa para test una sola vez y para train $k-1$ veces. Los errores obtenidos en cada etapa se promedian para producir una sola estimación (error medio obtenido de los $k$ análisis realizados).
Con el fin de medir el error de predicción del modelo en los modelos planteados anteriormente se consideró la siguiente medida:

\begin{center}

$RMSE= \displaystyle \frac{1}{k} \sum_{k=1}^{K} RMSE_{k}$

\end{center}

Donde $RMSE_{k}$ es la raiz del error cuadratico medio en la k-ésima muestra.

\begin{center}

$RMSE_k= \displaystyle \sqrt{ \frac{1}{n_k} \sum_{j = 1}^{n_k} (y_j - \hat{y}_j)^{2}}$

\end{center}

dónde $n_k$ es la cantidad de observaciones en la k-ésima muestra. 

Con el fin de obtener el modelo con la mejor performance predictiva, se realizó un proceso de \textit{hyperparameter tuning}. El mismo consiste en obtener la mejor combinación de hiperparámetros posibles mediante una metodología de cross-validation.

Un hyperparámetro es una valor necesario para ajustar un modelo el cual no se determina a partir 
de los datos sino que, por el contrario, es necesario que sea especificado previamente a la realización del ajuste. Dependiendo del algoritmo con el que se trabaje, el rol de los mismos puede variar. Por ejemplo, como fue mencionado para el caso de Random Forest se tiene 2 hyperparámetros de gran importancia: la cantidad de variables que son seleccionadas para ajustar cada arbol y la cantidad de árboles. 

\section{Interpretabilidad \label{sec:cv}}

Una vez implementados los diferentes algoritmos de aprendizaje automático y realizado el proceso de hiperparameter tunning, se procedió a obtener una medida de interpretabilidad de los mismos. Para ello, se trabajó con métodos globales modelo - agnosticos de interpretación, aplicados a aquel modelo con mejor performance predictiva y haciendo hincapié en el análisis gráfico.

Estos métodos de interpretación para modelos de caja negra, consiste en describir el compartamiento promedio del modelo. Donde, los mismos generalmente se expresan mediante un valor esperado, basado en la distribución de los datos. (Molnar, 2021)

A pesar de que en la literatura existen diferentes métodos para llevar a cabo el análsis, se trabajó mediante dos aproximaciones. Como primera aproximación, se realizó el análisis mediante los gráficos denominados \texit{Partial Dependece Plot} (PDP). Luego, con el fin de realizar un análisis con mayor profundidad y en la medida de que la metodología anterior presenta ciertas limitantes, se trabajó mediante el estudio de \texit{Accumulated local effect plots} (ALE).

En las siguientes subsecciones se detallan los principales aspectos teóricos de ambas metodologías, al igual que sus ventajas y limitantes.

\subsection{Partial Dependence Plot (PDP) \label{subsec:pdp}}

Los gráficos denominados partial dependence plot (PDP) permiten observar el efecto marginal que una o dos variables tienen sobre la predicción de la variable de respuesta obtenida a través de un algoritmo de machine learning (Molnar, 2021). Estos gráficos pueden detectar cuando la relación entre la variable de respuesta y la variable predictora de interés es líneal, monótona, o bien cuando se trata de una forma funcional más compleja. 

La función \textit{partial dependence function} para el caso de regresión se define como:

$$\begin{align*}\hat{f}_{x_S,PDP}(x_S)&=E_{X_C}\left[\hat{f}(x_S,X_C)\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C)d{}x_C\end{align*}$$


En dónde $X_s$ son las variables para las cuales se quiere conocer el efecto sobre la predicción, mientras que $X_C$ corresponde al resto de las variables utilizadas en el algoritmo de machine learning $\hat{f}$.

Ahora bien, la estimación para la función anterior se obtiene mediante la métodología Monte Carlo promediando sobre la muestra de entrenamiento:

$$\hat{f}_{x_S, PDP}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$

en dónde $X_C^{(i)}$ son los valores en la base de datos para las variables en las cuáles no estamos interesados, y $n$ el número de observaciones.

Es importante mencionar que PDP es un método que permite determinar de manera global la relación entre una o dos variables predictoras sobre la variable de respuesta. Asimismo, interesa destacar que uno de los supuestos de PDP es que las variables en $X_C$ y $X_S$ no están correlacionadas (Molnar, 2021).

De manera similar se obtiene una estimación PDP para el caso de variables categóricas en dónde, para cada categoría, se realiza la estimación PDP forzando a que todas las observaciones tomen el valor correspondiente a dicha categoría.

Una de las ventajas principales de PDP es que es un método intuitivo y en el caso de variables incorrelacionadas, tiene una interpretación clara. El gráfico PDP permite observar cómo cambia la predicción promedio cuando la h-ésima variable predictora cambia. No obstante éste resultado no es tan claro en el caso de correlación entre las variables con las que se construye el gráfico, ya que se construyen conbinaciones de las variables que son irreales o con probabilidad muy baja. Esto ocurrre ya que la función PDP en un punto en particular se obtiene forzando a que todos las observaciones tomen dicho valor en particular, lo cual puede no tener sentido en algunos casos. 

Una solución a éste problema es a través de la utilización de los denominados Accumulated Local Effect plots (ALE plots) que trabajan con distribuciones condicionales en lugar de con distribuciones marginales como es el caso de PDP. 

\subsection{Accumulated Local Effects (ALE) Plot \label{subsec:aleplot}}

Como fue mencionado en la sección anterior, una forma de evitar el cálculo de una predicción promedio en base a casos irreales o de baja probabilidad, es trabajar con distribuciones condicionales en lugar de distribuciones marginales. Es el caso de los conocidos M-Plots (Marginal Plots). 

La estimación de la predicción promedio para el caso de M-Plots se realiza de la siguiente manera:

$$\begin{align*}\hat{f}_{x_S,M}(x_S)&=E_{X_C|X_S}\left[\hat{f}(X_S,X_C)|X_S=x_s\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C|x_S)d{}x_C\end{align*}$$


M-plots logra superar el problema de los casos irreales o de baja probabilidad en la medida que se promedian predicciones solamente para observaciones que tienen valores cercanos al valor particular de la variable para el cual se está midiendo el efecto.

No obstante, no resuelve el problema en el caso de variables correlacionadas ya que igualmente se está estimando el efecto combinado de las variables correlacionadas.

ALE plots resuelve el problema promediando al igual que M-Plot sobre distribuciones marginales pero calculando diferencias en la predicción en lugar de promedios. 

La forma matemática es como sigue:

$$\begin{align*}\hat{f}_{x_S,ALE}(x_S)=&\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\left[\hat{f}^S(X_s,X_c)|X_S=z_S\right]dz_S-\text{constant}\\=&\int_{z_{0,1}}^{x_S}\int_{x_C}\hat{f}^S(z_s,x_c)\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\text{constant}\end{align*}$$

Dónde $\hat{f}^S(z_s,x_c)$ representa el cambio en la predicción lo cuál se define como la derivada parcial:

$$\hat{f}^S(x_s,x_c)=\frac{\partial\hat{f}(x_S,x_C)}{\partial{}x_S}$$

Esto permite aislar el efecto de la variable de interés sobre las predicciones.

Luego, se resta uns constante lo cual centra el gráfico de tal manera que el efecto sobre los datos sea cero. Mas adelante se hará énfasis en éste punto.

Ahora bien, a pesar de que no todos los modelos trabajan con gradientes, esto no es un problema dado que la metodología de computación de ALE plots utiliza intervalos en lugar de gradientes.

\subsubsection{Estimación \label{subsubsec:est}}

ALE plots, al igual que PDP y M-plots, puede ser estimado para una o dos variables de interés. En éste apartado se presenta el caso de una sola variable.

Para estimar los efectos locales de la variable $z_j$, se divide el recorrido de la misma en diferentes intervalos, y se calcula la diferencia en las predicciones en cada uno de dichos intervalos. Este procedimiento por lo tanto, funciona para el caso de modelos que no tienen derivadas parciales.

El primer paso es estimar el efecto como sigue:

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{j}^{(i)}\in{}N_j(k)}\left[f(z_{k,j},x^{(i)}_{\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]$$

Dónde $N_j(k)$ corresponde al k-ésimo intervalo de la variable de interés $z_j$.

Luego, el efecto calculado anteriormente se centra de manera que el efecto medio tome valor 0:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x^{(i)}_{j})$$

Esto se realiza para poder tener una medida del efecto de la variable de interés en un determinado punto en comparación a la predicción promedio.

En lo que respecta a los intervalos, los mismos son definidos utilizando los cuantiles de la distribución de la variable de interés, lo cual asegura que cada intervalo va a contener el mismo número de observaciones. La desventaja principal de utilizar los cuantiles para definir los intervalos es que los mismos pueden tener longitud muy diferente, haciendo que el gráfico sea muy irregular.



\subsection{Tratamiento de valores faltantes \label{subsec:nas}}

La base de datos construida contiene variables con diferentes grado de datos faltantes. 

Éste problema fue abordado siguiendo dos estrategias de imputación. En primera instancia se entrenan diferentes modelos imputando solamente a las variables numéricas que tienen proporción de valores faltantes inferior a 0.15, y se procede en ésta primera instancia a realizar imputación por la media, lo cual es equivalente a asumir que el proceso generador de datos de éstos valores es un proceso aleatorio. Es decir, que los datos faltantes son generados al azar.

Posteriormente, se realizó un proceso de imputación de valores faltantes mediante un análisis supervisado. Para ello se trabajó de la siguiente manera:

1. Se ajustó un modelo tomando como variable de salida cada variable con datos faltantes.
2. En cada uno de ellos, se consideró como variables de entrada todas aquellas que originalmente no presentan datos faltantes, pero excluyendo la variable precio.
3. En cada variable, se sustituyó cada dato faltante por su predicción utilizando el modelo ajustado.

En particular, el algoritmo utilizado para ajustar los modelos fue Random Forest, mediante el paquete missRanger. En lo que respecta a hyperparameter tuning, se destaca que debido al tiempo computacional que conlleva fueron utilizados en todos los casos los valores por defecto.

Como fue mencionado anteriormente el criterio genérico para seleccionar las variables a imputar fue según la proporción de valores faltantes (proporción inferior a 0.15). Sin embargo, ésta metodología de imputación, a diferencia de la anterior, permite realizar el proceso para variables de tipo cualitativas. De esta forma, se incluyó en el análisis la variables item condition. No obstante, si bien existen otras variables que podrían ser incluídas, se decidió dejarlas fuera del análisis en la medida que no se consideran relevantes para el mismo (principalmente por no estar balancedas).

\chapter{Análisis exploratorio de  datos \label{cap:EDA}}

En esta sección se presentan las principales características de los datos con los cuales se trabajó. La base de datos se conforma por un total de \Sexpr{paste(dim(aptos)[1])} observaciones y \Sexpr{paste(dim(aptos)[2])} variables de las cuales, una vez realizado el proceso de tratamiento de datos faltantes se cuenta con un total de 24 variables en el caso de imputación por la media, y 25 en el caso de imputación por Random Forest. En la tabla bla del  \ref{nasA} bla se detalla la proporción de observaciones con datos faltantes por variables.|

En primer lugar se procedió a analizar el comportamiento de la variable de salida (precio). En forma de resumen se presenta a continuación la tabla con las principales medidas de resumen: 

<<>>=
sum.table <- aptos %>% summarise(Min = round(min(price),0),
                    Q1=round(quantile(price,.25),0),
                    Mediana = round(median(price),0),
                    Media = round(mean(price),0),
                    Q3 = round(quantile(price,.75),0),
                    Max = round(max(price),0),
                    Desvio = round(sd(price),0)
                    )

library(knitr)
options(knitr.table.format = "latex")
@


<<results=tex>>=
kable(sum.table, caption = 'Medidas de resumen de la variable de salida price', booktabs = T, format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@

Luego, se presenta gráficamente la distribución de la variable mediante un histograma.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
aptos %>% ggplot(aes(x=price)) + 
       geom_histogram(fill = 'navyblue', alpha = 0.9, bins = 40) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_text(face = 'bold', size = 12),
            axis.text.y = element_text(size = 12)) +
      labs(x = 'Precio de oferta') +
      scale_fill_manual(values = c('navyblue')) +
      geom_vline(xintercept = mean(aptos$price, na.rm = TRUE), 
                 colour = 'firebrick') +
      annotate("text", x = mean(aptos$price, na.rm = TRUE) + 72000, y = 5500, label = paste0("Media = USD ",round(mean(aptos$price, na.rm = TRUE))), color = 'firebrick') +
      scale_x_continuous(label = comma)
@
\captionof{figure}{Histograma del precio de oferta (price) de los apartamentos a la venta en Montevideo. El precio promedio es USD \Sexpr{paste(round(mean(aptos$price, na.rm = TRUE)))}}
\end{figure}

En lo que respecta a las variables de entrada, en primer lugar se presenta análisis de las variables cualítativas que se consideraron más relevantes. A continuación se presentan los gráficos de barras con el fin de observar la distribución de los niveles en los datos.

<<>>=
niveles <- aptos %>% group_by(bedrooms) %>% summarise(n = n())
colores <- data.frame(colores= c("gold1","darkviolet","green4", "dodgerblue2", "firebrick"))
niveles <- niveles %>% arrange(n)
niveles$id <- 1:nrow(niveles)
colores$id <- 1:nrow(niveles)
niveles <- left_join(niveles,colores,by="id")

niveles <- niveles %>% arrange(bedrooms)

p1 <- aptos %>% ggplot() +
  geom_bar(aes(x = bedrooms, y = (..count..)/sum(..count..), fill = bedrooms)) +
  theme(axis.ticks.x = element_blank(),
        legend.position = 'none',
        axis.title.y = element_blank(),
        axis.title.x = element_text(face = 'bold', size = 10),
        axis.text.x = element_text(face = 'bold')) + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(x = as.factor(bedrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
  labs(x = 'Cantidad de dormitorios') +
  scale_fill_manual(values = niveles$colores)

p2 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(full_bathrooms)), y = (..count..)/sum(..count..), fill = full_bathrooms)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(full_bathrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Cantidad de baños completos') +
      scale_fill_manual(values = c('aquamarine3','pink3'))

p3 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(zona_avditalia)), y = (..count..)/sum(..count..), fill = zona_avditalia)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(zona_avditalia),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Zona respecto a avenida italia') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))

p4 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(has_swimming_pool)), y = (..count..)/sum(..count..), fill = has_swimming_pool)) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(has_swimming_pool),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'El edificio tiene piscina') +
      scale_fill_manual(values = c('mediumpurple2', 'salmon3'))

@

\begin{figure}[h!]
\centering
<<fig = TRUE, fig.pos = 'h'>>=
grid.arrange(p1, p2, p3, p4, ncol = 2)
@
\captionof{figure}{Gráficos de barras para diferentes variables cualitativas. En panel superior se encuentran a la izquierda la variable \text{bedrooms} y a la derecha la variable \text{full\_bathrooms}. Por otro lado, en el panel inferior a la izquierda se encuentra la variable \text{zona\_avditalia} y a la derecha \text{has\_swimming\_pool}}
\end{figure}

Con el fin de observar la distribución de algunas de las variables graficadas anteriormente a la vez que evaluar si las variables podrían ser candidatas a variables explicativas del modelo, se presentan a continuación algunos gráficos de violín y gráficos de caja.

<<>>=
p5 <- aptos %>% 
      ggplot(aes(x=as.character(zona_avditalia), y=price,
                 fill=as.character(zona_avditalia))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('Precio de publicación') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('orangered2', 'springgreen4'))

p6 <- aptos %>%
      filter(!is.na(full_bathrooms)) %>%
      ggplot(aes(x=as.character(full_bathrooms), y=price, 
                 fill=as.character(full_bathrooms))) + 
      geom_violin() +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_blank(),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      ylab('') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black') +
      scale_y_continuous(labels = comma) +
       scale_fill_manual(values = c('yellow4', 'palevioletred'))
@


\begin{figure}[h!]
\centering
<<fig = TRUE, width = 6, height=3>>=
grid.arrange(p5, p6, ncol =3)
@
\captionof{figure}{Gráficos de violín y gráficos de caja. A la izquierda se encuentra el gráfico de price según \text{zona\_avditalia} mientras que a la derecha se encuentra el gráfico de price según \text{full\_bathrooms}. Es posible observar que los gráficos sugieren la existencia de una diferencia en media de price según \text{zona\_avditalia}, teniendo una media superior los apartamentos que se encuentran al Sur de avenida italia. A su vez, los apartamentos con dos o más baños completos tienen una media superior respecto a los apartamentos con 1 solo baño completo.}
\end{figure}

Ahora bien, con el objetivo de evaluar posibles interacciones sobre las variables anteriormente graficadas, se presenta el gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}.


\begin{figure}[h!]
\centering
<<fig = TRUE,width = 6, height=3>>=
aptos %>% ggplot(aes(y=price, group = full_bathrooms, fill = full_bathrooms)) + 
       geom_boxplot() +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_blank()) +
      labs(x = 'Precio de oferta') +
      scale_fill_manual(values = c('orangered2', 'springgreen4')) +
      facet_grid(full_bathrooms~zona_avditalia) +
      scale_y_continuous(label = comma)
@
\captionof{figure}{Gráfico de caja de price según \text{zona\_avditalia} y \text{full\_bathrooms}. El gráfico sugiere que las diferencias en media observardas se mantienen cuando condicionamos en \text{full\_bathrooms}. En particular, entre los apartamentos con 2 o más baños completos. se observa una media superior para los apartamentos ubicados al sur respecto a los apartamentos ubicados al Norte. En lo que respecta a los apartamentos con un sólo baño completo, el gráfico sugiere que los apartamentos al Sur poseen una media superior respecto a los ubicados al Norte, si bien la diferencia es menos en éste último caso.}
\end{figure}

En lo que respecta a la variables \text{dist\_shop},se realizó la siguiente discretización para poder tener una aproximacón de su efecto sobre la variable price:

- Menos de 1 km: para volores inferiores a 1000
- Entre 1 km y 5 km para valores en el intervalo [1000, 5000)
- Mayor a 5 km para valores mayores a 5000

a continuación se presentá el gráfico de cajas del precio de oferta según \text{dist\_shop} por \text{zona\_avditalia} utilizando la discretización mencionada.

\begin{figure}[h!]
\centering
<<fig = TRUE, width = 6, height=3>>=
aptos %>% 
      mutate(dist_shop = factor(case_when(dist_shop < 1000 ~ 'Menos de 1 km',
      dist_shop < 5000 ~ 'Entre 1 km y 5 km',
      TRUE ~ 'Más de 5 km'))) %>%
      ggplot(aes(y=price,x=dist_shop,fill=zona_avditalia)) + 
      geom_boxplot() +      
      theme(axis.ticks.x = element_blank(),
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text()) +
      labs(y = 'Precio de oferta', x = 'Distancia al shopping más cercano') +
      scale_fill_manual(values = c('orangered2', 'springgreen4'))  +
      scale_y_continuous(label = comma)      
@
\captionof{figure}{Gráfico de caja de \text{dist\_shop} segun \text{zona\_avditali}a. En todos los casos los apartementos ubicados al Sur tienen una media superior. Asimismo, el gráfico sugiere que entre los apartamentos ubicados al Norte, los apartamentos ubicados a menos de 1 km de un shopping tienen una media superior.}
\end{figure}


\begin{figure}[h!]
\centering
<<fig = TRUE>>=
aptos_cor <- aptos %>% select(price, dist_shop, dist_rambla, covered_area,
                              total_area, no_covered_area) 

corr<-round(cor(aptos_cor, use='pairwise.complete.obs') , 2)

ggcorrplot(corr, method = c("square"),type=c("upper"), 
           ggtheme = ggplot2::theme_gray,lab=TRUE)
@
\captionof{figure}{Mapa de correlación. Como es de esperar, precio y distancia a la ramble éste de montevideo estan correlacionadas negativamente.}
\end{figure}

Por último, se presentan algunos gráficos que permiten observar la relación entre el precio de los apartamentos, el área cubierta, y la variable zona_avditalia.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
ggpairs(aptos, c(3,14), mapping = ggplot2::aes(color = zona_avditalia, alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.5)))
@
\captionof{figure}{Gráficos de price según \text{covered\_area}. En el panel superior izquierdo se encuentra la densidad de price segú \text{convered\_area}. En el panel superior derecho se encuentra la correlación total entre price y \text{covered\_area} y la correlación por grupos según \text{zona\_avditalia}. En el panel inferior izquierdo se encuentra el gráfico de dispersión de price según \text{covered\_area} por \text{zona\_avditalia}, mientras que en el panel inferior derecho se encuentra la densidad de \text{covered\_area} según \text{zona\_avditalia}.}
\end{figure}

\chapter{Resultados \label{cap:Resultados}}

En ésta sección se presentan los principales resultados obtenidos en el presente trabajo.
Como se menciona en la sección marco metodológico, en primer lugar se presentan los resultados del modelo lineal, seguido por las siguientes técnicas de aprendizaje estadístico: random forest, boosting, y support vector regression. Para estas últimas, a su vez se presentan los resultados obtenidos en el proceso de hyperparameter tuning.

Por último, en forma de resumen, se presenta un análisis comparativo de los diferentes modelos en función de su performance predictiva. A partir del mismo se selecciona el mejor modelo y a éste se le realiza un análisis gráfico de interpretabilidad.

Se destaca que todas las etapas anteriores se aplicaron considerando las dos técnicas de imputación mencionadas en la sección marco metodológico.

\section{Modelo lineal \label{sec:ml}}

A continuación se presentan los principales resultados del modelo de regresión lineal, con la variable price como variable explicada.

<<>>=
lm <- lm(price ~ ., data = train)
RMSE_lm <- sqrt(mean((test$price - predict(lm,test))^2))

lm_mr <- lm(price ~ ., data = train_mr)
RMSE_lm_mr <- sqrt(mean((test_mr$price - predict(lm,test_mr))^2))

lmt <- rbind(broom::glance(lm), broom::glance(lm))
lmt <- lmt %>% select(-"sigma" , - "AIC", - "BIC",
                  - "deviance", -"df.residual", -"nobs", -"df", -"logLik")

met <- tribble(
  ~Imput, ~RMSE,
  "Media", round(RMSE_lm), 
  "Miss Ranger",  round(RMSE_lm_mr)
)

lmt <- cbind(met, lmt)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(lmt, caption = c("Principales resultados de los modelos lineales ajustados según el método de imputación utilizados"), escape = F, booktabs = T, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "hold_position") %>% row_spec(0,bold=TRUE)
@


No se observa que el método de imputación genere diferencias sustanciales en término de RMSE. Por lo tanto se realiza el diagnóstico sobre los residuos del modelo solamente para el caso de imputación por la media.

\begin{figure}[h!]
\centering
<<fig = TRUE, width=6, height=3>>=
lm1 <- ggplot(as_tibble(lm$residuals) %>% 
             mutate(id = seq(1, length(lm$residuals), 1))) + 
      geom_point(aes(x = id, y = value), color = 'red', alpha = 1/5) +
      theme(axis.title.y = element_text(face = 'bold', size = 14),
            axis.title.x = element_text(),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(x = 'Id.', y = TeX("$\\hat{\\epsilon}$"))

mediana_reslm <- quantile(lm$residuals, probs = 0.5)
media_reslm <- mean(lm$residuals)

lm2 <- ggplot(as_tibble(lm$residuals)) + 
      geom_density(aes(value), color = 'red') +
      theme(axis.title = element_text(face = 'bold', size = 14),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(y = TeX("$\\hat{f}_{\\epsilon}$"), x = TeX("$\\hat{\\epsilon}$"))

grid.arrange(lm1, lm2, ncol = 2)
@
\captionof{figure}{Residuos del modelo líneal. En el panel izquiero se encuentra el gráfico de dispersión mientras que en el pane izquierdo se encuentra la densidad estimada.}
\end{figure}

Como fue mencionado en la sección \ref{mlMT} en primer lugar se realizó la prueba de normalidad de Lilliefors (1976) sobre los residuos del modelo. El p-valor de la prueba para los residuos del modelo lineal es menor a 0.5 con lo cual se rechaza la hipótesis nula de normalidad.

Por otro lado, en lo que respecta a la prueba de hetoroscedasticidad de Breuch-Pagan también se rechaza la hipótes nula de homoscedasticidad.

Estos resultados se mantienen incambiados según se utilice imputación de valores faltantes por la media o por Random Forest.

En el anexo \ref{mlA} se encuentran las tablas con los resultados de las pruebas aplicadas.

\section{Árbol de regresión \label{sec:arbol}}

A continuación se presentan los resultados del árbol de regresión ajustado sobre la variable price utilizando las metodologías de imputación mencionadas en la sección marco metodológico. 

En primer lugar se presenta, para cada modelo ajustado, el gráfico luego de realizar el proceso de poda. Respecto a éste punto, en el \ref{arbolA} se presenta la tabla con los diferentes valores del parámetro de costo-complejidad, su error mediante un proceso de cross-validation asociado, al igual que el tamaño del árbol que dicho valor del parámetro implica y otras métricas de interés.

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
##### Arbol de regresion

# summary(arbol)

# Proceso de poda

#broom::tidy(arbol$cptable)

# Grafico de la evolucion del error

cp_error <- data.frame(arbol$cptable)

# Obtengamos el cp

cp_opt <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"CP"]

npart <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"nsplit"]

#cp_error %>% ggplot(aes(x=CP,y=xerror))+geom_point(color="red")+geom_line()

# Otra forma

# Grafico

rpart.plot(arbol.prune,roundint = T,digits = -3)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable price mediante la base de datos construida en base a información obtenida de Mercado Libre,  una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt}. El método de imputación sobre valores faltantes es en éste caso imputación por la media. El árbol se conforma por \Sexpr{npart+1} nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart} particiones.}
\end{figure}

En función a lo observado en la figura, se tiene que la primera variable en realizar una partición binaria es la variable \text{full\_bathrooms}. Las variables \text{dist\_rambla}, \text{total\_area}, \text{bedrooms} e \text{ingresomedio\_ech} son utilizadas en las siguientes particiones.

El gráfico de árbol realizando imputación de valores faltantes por random forest se encuentra en el anezo debido a que no presenta diferencias sustanciales con el árbol realizando imputación por la media presentado e la figura anterior.

Una vez observada las principales características de los modelos, se procedió a obtener cierta medida de la performance predictiva de los mismos. Para ello, se calculó la raíz error cuadrático medio (RMSE). 

A continuación se presenta la tabla con los resultados para las metodologías de imputación utilizadas.

<<>>=
# RMSE test
RMSE_arbol <- sqrt(mean((test$price-predict(arbol.prune,test))^2))
RMSE_arbol_mr <- sqrt(mean((test_mr$price-predict(arbol.prune.mr,test_mr))^2))

rmse_arbol <- tribble(
  ~Imput, ~RMSE,
  "Media",   round(RMSE_arbol),
  "Miss Ranger", round(RMSE_arbol_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(rmse_arbol, caption = 'Error cuadrático medio de los modelos ajustados por Árbol de regresión', booktabs = T, col.names = c("Método de imputación","RMSE"), format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


El modelo tiene un error cuadrático medio de \Sexpr{round(RMSE_arbol)}.

\section{Random Forest \label{sec:randomforest}}

En éste sección se presentan los resultados del modelo de Random Forest ajustado. Como fue mencionado anteriormente, en primer lugar se realizó el ajuste sobre los datos dónde las variables cuantitativas con proporción de datos faltantes inferior a 0.1 fueron estimadas por la media. Posteriormente, se ajustó el modelo utilizando un procesi de imputación mediante Random Forest. 

A continuación se presenta la tabla con los errores cuadráticos medios de los modelos mencionados. No se observa una diferencia entre los valores lo cual puede deberse a que las variables imputadas fueron seleccionadas con un bajo porcentaje de valores faltantes.

<<>>=
# RMSE test
RMSE_rf <- sqrt(mean((test$price-predictions(predict(rf_train,test)))^2))
RMSE_rf_mr <- sqrt(mean((test_mr$price-predictions(predict(rf_train_mr,test_mr)))^2))

rmse_rf <- tribble(
  ~Imput, ~ECM,
  "Media",   round(RMSE_rf),
  "Miss Ranger", round(RMSE_rf_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(rmse_rf, caption = 'Error cuadrático medio de los modelos ajustados por Random Forest', booktabs = T, col.names = c("Método de imputación","RMSE"), format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


\begin{figure}[h!]
\centering
<<fig=TRUE, width=6, height=3>>=
############ RF 

# Importancia de las variables

importancia_rf <- data.frame(rf_train$variable.importance)

colnames(importancia_rf) <- c("importance")

importancia_rf$variables <- row.names(importancia_rf)

#### Veamos imputación por missranger

# Importancia de las variables

importancia_rf_mr <- data.frame(rf_train_mr$variable.importance)

colnames(importancia_rf_mr) <- c("importance")

importancia_rf_mr$variables <- row.names(importancia_rf_mr)

p1 <- importancia_rf %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="",x="") +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_text(face = 'bold'),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 12),
            axis.title.y = element_blank()) +
      scale_fill_manual(values = c('navy', 'orangered3', 'darkgreen', 'slateblue3', 'brown3', 'gold3', 'springgreen3', 'pink3', 'royalblue4', 'firebrick'))

p2 <- importancia_rf_mr %>% arrange(desc(importance)) %>% slice(1:10) %>% ggplot(aes(y=reorder(variables,importance),x=importance,fill=variables))+
  geom_col()+theme(legend.position="none")+labs(y="",x="")+
      theme(axis.text.x = element_blank(),
            axis.text.y = element_text(face = 'bold'),
            axis.ticks.x = element_blank(),
            axis.title.x = element_text(size = 12),
            axis.title.y = element_blank()) +
      scale_fill_manual(values = c('navy', 'orangered3', 'darkgreen', 'slateblue3', 'brown3', 'gold3', 'springgreen3', 'pink3', 'royalblue4', 'firebrick'))

grid.arrange(p1, p2, ncol = 2)
@
\captionof{figure}{Importancia de las variables en los modelos de random forest ajustados. Se observa que \text{total\_area} es la variable con mayor importancia en ambos modelos. A su vez, todas las variables geoespacialesconstruidas aparecen entre las más importantes de ambos modelos.}
\end{figure}

\vspace{3cm}

\section{Boosting \label{sec:boosting}}

<<>>=
RMSE_boosting <- RMSE(kernlab::predict(boosting_train, test), test$price)
RMSE_boosting_mr <- RMSE(kernlab::predict(boosting_train, test_mr), test_mr$price)

rmse_boost <- tribble(
  ~Imput, ~ECM,
  "Media",   round(RMSE_boosting),
  "Miss Ranger", round(RMSE_boosting_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(rmse_boost, caption = 'Error cuadrático medio de los modelos ajustados por Boosting', booktabs = T, col.names = c("Método de imputación","RMSE"), format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


\section{Support vector regression \label{sec:svr}}


<<>>=
RMSE_svr <- RMSE(kernlab::predict(SVR_train, test), test$price)
RMSE_svr_mr <- RMSE(kernlab::predict(SVR_train_mr, test_mr), test_mr$price)


rmse_svr <- tribble(
  ~Imput, ~ECM,
  "Media",   round(RMSE_svr),
  "Miss Ranger", round(RMSE_svr_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(rmse_svr, caption = 'Error cuadrático medio de los modelos ajustados por SVR', booktabs = T, col.names = c("Método de imputación","ECM"), format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@


\section{Interpretabilidad \label{sec:cv}}


\section{Selección del mejor modelo e interpretabilidad \label{sec:cv}}

A continuación se presenta la tabla con los errores cuadráticos medios de los modelos implementados.


%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAFíA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apa}
\bibliography{TFGbiblo}

%%%%%%%%%%%%%%%%%%%%%%%%%%% ANEXO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{appendix}

\chapter{Anexo} 

\section{Variables \label{varsA}} 

<<>>=
vars <- readxl::read_excel(here('mercado_libre/Api', 'criterios_de_limpieza.xlsx'),
                                sheet = 'anexo_vars') %>% arrange(Fuente)
n <- vars %>% group_by(Fuente) %>% summarise(conteo = n())


aux <- as_tibble(matrix(rep(" ",4), ncol = 4, nrow = 1))
colnames(aux) <- colnames(vars)
vars <- bind_rows(aux, vars)

library(knitr)
options(knitr.table.format = "latex")
@


\section{Fórmula de Haversine para el cálculo de distancias \label{harvesineA}}

La fórmula de Haversine tiene la siguiente expresión:

$$d = 2 \, r \, \text{sen}^{-1} \bigg(\sqrt{ \text{sen}^{2} \, \frac{\phi_2 - \phi_1}{2} + \text{cos} (\phi_1) \, \text{cos}(\phi_2) \, \text{sen}^{2} \,  \frac{\psi_2 - \psi_1}{2}} \bigg)$$ (PONER AUTOR, Y FECHA) 

dónde d es la distancia entre dos puntos de longitud y latitud $(\psi_1, \phi_1)$ y $(\psi_2, \phi_2)$ respectivamente, y r el radio de la tierra.

\section{Proporción de valores faltantes por variable \label{nasA}} 

<<results=tex>>=
kable(vars %>% select(-Fuente), caption = 'Descripción de las variables utilizadas', booktabs = T, longtable = TRUE, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = c("hold_position"), font_size = 10) %>% row_spec(0,bold=TRUE) %>%
      pack_rows("Fuente: API ML", 2, as.numeric(n[1,2]))  %>% 
      pack_rows("Fuente: Elaboración propia", as.numeric(n[1,2] + 1), nrow(vars)) 
@

<<>>=
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),3)) %>% data.frame() %>% 
   rename(prop_na=".") %>% arrange(desc(prop_na))
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(p_na %>% filter(p_na > 0), caption = 'Proporción de valores faltantes por variable', booktabs = T, col.names = c("Proporción por variable"), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@

\section{Modelo lineal \label{mlA}} 

A continuación se presentan los resultados de las pruebas de normalidad y de homocedasticidad mencionadas en la sección \ref{ml}. Esto considerando ambos métodos de imputación de valores faltantes.

<<>>=
bp <- lmtest::bptest(lm) # se rechaza h0 de varianza constante 

bp_mr <- lmtest::bptest(lm_mr) # se rechaza h0 de varianza constante 

bp_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media",   "< 2.2e-16", "Se rechaza H0",
  "Miss Ranger", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(bp_t, caption = 'Resultados de la prueba de Breush-Pagam para el análisis de homoscedasticidad de los residuos', booktabs = T, col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@


<<>>=
lt <- lillie.test(residuals(lm)) #se rechaza h0 de normalidad

lt_mr <- lillie.test(residuals(lm_mr)) #se rechaza h0 de normalidad

lt_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media", "< 2.2e-16", "Se rechaza H0",
  "Miss Ranger", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(lt_t, caption = 'Resultados de la prueba de Llliefors para el análisis de normalidad de los residuos', booktabs = T, col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ".")) %>% kable_styling() %>% row_spec(0,bold=TRUE)
@
\section{Arbol de regressión \label{arbolA}} 

\begin{figure}[h!]
\centering
<<fig = TRUE>>=
##### Arbol de regresion

# Grafico de la evolucion del error

cp_error_mr <- data.frame(arbol_mr$cptable)

# Obtengamos el cp

cp_opt_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"CP"]

npart_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"nsplit"]

# Grafico

rpart.plot(arbol.prune.mr,roundint = T,digits = -3)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable price mediante la base de datos construida en base a información obtenida de Mercado Libre,  una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt_mr}. El método de imputación sobre valores faltantes es en éste caso imputación por Random Forest. El árbol se conforma por \Sexpr{npart_mr+1} nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart_mr} particiones.}
\end{figure}


A continuación, a modo de resumen se presenta la tabla de los modelos mencionados en la sección \ref{arbol}. La misma contiene diferentes valores del parámetro de costo-complejidad, su error mediante un proceso de cross-validation asociado, y el tamaño del árbol que dicho valor del parámetro implica.

Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro
de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation.

<<>>=

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(cp_error %>% select(-rel.error, -xstd), caption = 'Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation.   La metodología de imputación de valores  faltantes utilizada en éste caso es imputación por la media.', booktabs = T, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@

<<>>=

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(cp_error_mr %>% select(-rel.error, -xstd), caption = 'Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de cross-validation. La metodología de imputación de valores  faltantes utilizada en éste caso es imputación por Random Forest.', booktabs = T, format.args = list(big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position") %>% row_spec(0,bold=TRUE)
@




\section{siete \label{ane:ind}}


\section{ocho \label{ane:var}}


\end{appendix}


\end{document}
