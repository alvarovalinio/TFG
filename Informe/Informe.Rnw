\documentclass[12pt,twoside,spanish,a4paper,openany]{book}
\usepackage{geometry}\geometry{top=3cm,bottom=3cm,left=3cm,right=3cm}
\setlength{\parskip}{0em}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{tocbibind}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{boxedminipage}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage[spanish,es-tabla]{babel}
%\usepackage{parskip}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage[backend=bibtex]{biblatex}
\bibliography{bibliography.bib}
%\usepackage{fancyvrb}

\captionsetup[table]{skip=0pt} 

\fancyhf{} 
\fancyhead[LE]{\leftmark} 
\fancyhead[RO]{\nouppercase{\rightmark}} 
%\fancyfoot[LE,RO]{\thepage} 
\rfoot{\thepage} 
\pagestyle{fancy} 

\topmargin 2mm
\oddsidemargin 2mm
\evensidemargin 2mm

\makeindex
\setcounter{secnumdepth}{3}

\linespread{1.6}

\begin{document}
\SweaveOpts{concordance=TRUE, echo = FALSE}

<<echo=FALSE>>=
options(scipen = 999)  
@

<<>>=
Sys.setenv(TEXINPUTS=getwd(),
               BIBINPUTS=getwd(),
               BSTINPUTS=getwd())
@

\pagenumbering{roman}


%%%%%%%%%%%%%%%%%%%%%%%%%% CARATULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\begin{center}

\includegraphics{logo_informe.jpg}

\vspace{1.5 cm}

UNIVERSIDAD DE LA REPÚBLICA

Facultad de Ciencias Económicas y de Administración

Licenciatura en Estadística

Trabajo final de grado

\vspace{2.5cm}

\textbf{\large Precio de oferta de apartamentos en Montevideo:} \\
\textbf{\large Una aproximación desde la ciencia de datos.}

\vspace{1.5 cm}

Lucía Coudet y
Alvaro Valiño


\end{center}


\vspace{2cm}

\noindent Tutores:\\
\noindent Natalia da Silva\\


\vspace{1cm}

\begin{center}

\noindent Montevideo, Diciembre 2021.

\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Agredecimientos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

\vspace{5cm}

\begin{center}
\textit{\latinword{Agredecimientos}}
\end{center}

Agredecer en primera instancia a nuestra tutora Natalia da Silva por su dedicación y compromiso en cada instancia de éste proceso. A la Facultad de Ciencias Económicas y de Administración de la UdelaR que ha acompañado nuestra formación universitaria y profesional a lo largo de muchos años. A nuestras familias, amigos y compañeros que fueron un pilar fundamental para poder estar hoy finalizando esta etapa de nuestras vidas. A quienes han sido nuestros docentes a lo largo de la carrera por la formación y conocimientos brindados.
\end{abstract}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESUMEN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

\vspace{5cm}

\begin{center}
\textbf{Abstract}
\end{center}
El siguiente trabajo consiste en la implementación de técnicas de aprendizaje estadístico con el fin de realizar predicciones sobre el precio de oferta de los apartamentos a la venta en Montevideo, Uruguay. Para ello, se trabajó con una base de datos de elaboración propia a partir de los apartamentos que se ofertan a través de la plataforma \textit{Mercado Libre}. La misma contiene información para el periodo comprendido entre el mes de Mayo al mes de Setiembre del año 2021. Esto último en la  medida que no es posible la obtención de datos históricos. En cuanto a las técnicas de aprendizaje estadístico, se trabajó con algoritmos de \textit{Árboles de Decisión} y métodos agregados de los mismos como ser \textit{Random Forest} y \textit{Boosting}, al igual que con métodos de \textit{regresión robusta} (\textit{Support Vector Regression}). Asimismo, se implementó un ajuste mediante un \textit{Modelo de Regresión Lineal Múltiple} el cual es conocido en la literatura económica como \textit{Modelo de Precios Hedónicos} y usualmente aplicado en problemas de este índole. Éste úlimo con el fin de comparar su capacidad predictiva en relación a las técnicas anteriores. Con respecto a la performance predictiva de los algoritmos, se observó que el ajuste por \textit{Boosting} presentó una performance predictiva superior, esto último luego de haber ajustado los respectivos \textit{parámetros de ajuste}. En cuanto al análisis de interpretabilidad de los modelos, se trabajó con métodos \textit{modelo-agnósticos}, haciendo hincapié en las metodologías \textit{importancia de las variables permutadas} y \textit{gráficos de dependencia  parcial}. De esta forma, se observó que la distancia entre el apartamento y la rambla este de Montevideo es la variable más importante en el ajuste, y a su vez esta presenta una relación inversa en cuanto a la predicción del precio de oferta del mismo. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{page}{1} 
 
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<librerias>>=
library(knitr) # citado
library(tidyverse) #citado ggplot2 en lugar de tidyverse
library(sf) #citado
library(scales) # citado
library(here) #citado
library(gridExtra)
library(data.table)
library(magrittr)
library(ggcorrplot) #citado
library(corrplot)
library(RColorBrewer)
library(caret) #citado
library(doParallel) #citado
library(rpart) #citado
library(rpart.plot) #citado
library(rattle) #citado
library(ranger) #citado
library(missRanger) #citado
library(kableExtra)
library(xtable)
library(RColorBrewer)
library(lmtest) #citado
library(nortest) #citado
library(glmnet) #citado
library(GGally) #citado
library(latex2exp)
library(gbm) #citado
library(kernlab) #citado
library(vip) #citado 
library(iml) #citado
library(viridis) #citado
@


<<>>=
options(scipen = 999)

source(here("Funciones","funcion_imput_media.R"))

#### DATOS

aptos_yearmonth <- list.files(path = here("Datos/Limpios"), 
                              pattern = "*.csv", full.names = T)

yearmonth <- c('aptos_202106','aptos_202107',"aptos_202108", "aptos_202109", "aptos_202110" )

aptos <- sapply(aptos_yearmonth, FUN=function(yearmonth){
      read_csv(file=yearmonth)}, simplify=FALSE) %>% bind_rows()


aptos <- aptos %>% group_by(id) %>% 
      arrange(desc(fecha_bajada)) %>%
      slice(1) %>% ungroup()

aptos <- aptos %>% mutate_if(is.character, as.factor)

# Filtramos por el criterio en price - eliminamos obs. con price superior al percentil 95%

aptos_todos <- aptos

aptos <- aptos %>% filter(price <= quantile(aptos$price,.95))

# Perdemos esta cantidad de registros

# nrow(aptos_todos) - nrow(aptos)

# vemos prop de NA
p_na <- sapply(aptos, function(x) round(sum(is.na(x))/length(x),4)) %>% data.frame() %>% 
      rename(prop_na=".") %>% arrange(desc(prop_na))

#### Definimos variables Sin na imputamos por la media

aptos_sin_na <- imput_media(aptos,p=.1)

aptos_mr <- read_csv(here("Datos/Limpios/aptos_mr","aptos_mr.csv")) 
aptos_mr <- aptos_mr %>% mutate_if(is.character, as.factor)

############ train - test 

set.seed(1234)
ids <- sample(nrow(aptos_sin_na), 0.8*nrow(aptos_sin_na))

train <- aptos_sin_na[ids,]
test <- aptos_sin_na[-ids,]

train_mr <- aptos_mr[ids,]
test_mr <- aptos_mr[-ids,]

######### estandarizacion (SVR)

aptos_sin_na_std <- aptos_sin_na

# Estandarizamos variables continuas

aptos_sin_na_std$covered_area <- scale(aptos_sin_na_std$covered_area)

aptos_sin_na_std$total_area <- scale(aptos_sin_na_std$total_area)

aptos_sin_na_std$no_covered_area <- scale(aptos_sin_na_std$no_covered_area)

aptos_sin_na_std$ingresomedio_ech <- scale(aptos_sin_na_std$ingresomedio_ech)

aptos_sin_na_std$dist_shop <- scale(aptos_sin_na_std$dist_shop)

aptos_sin_na_std$dist_rambla <- scale(aptos_sin_na_std$dist_rambla)


aptos_mr_std <- aptos_mr

# Estandarizamos variables continuas

aptos_mr_std$covered_area <- scale(aptos_mr_std$covered_area)

aptos_mr_std$total_area <- scale(aptos_mr_std$total_area)

aptos_mr_std$no_covered_area <- scale(aptos_mr_std$no_covered_area)

aptos_mr_std$ingresomedio_ech <- scale(aptos_mr_std$ingresomedio_ech)

aptos_mr_std$dist_shop <- scale(aptos_mr_std$dist_shop)

aptos_mr_std$dist_rambla <- scale(aptos_mr_std$dist_rambla)

############ train - test 

set.seed(1234)
ids <- sample(nrow(aptos_sin_na_std), 0.8*nrow(aptos_sin_na_std))

train_std <- aptos_sin_na_std[ids,]
test_std <- aptos_sin_na_std[-ids,]

train_mr_std <- aptos_mr_std[ids,]
test_mr_std <- aptos_mr_std[-ids,]

############## Cargamos los modelos

#### Arbol

load(here("Modelos/Resultados/ARBOL","arbol_train.RDS")) #levanta objeto con nombre arbol

load(here("Modelos/Resultados/ARBOL","arbol_prune_train.RDS")) #arbol.p

load(here("Modelos/Resultados/ARBOL","arbol_train_mr.RDS")) #levanta objeto con nombre arbol
load(here("Modelos/Resultados/ARBOL","arbol_prune_train_mr.RDS")) #arbol.p

cp_error <- data.frame(arbol$cptable)

#Obtengamos el cp

cp_opt <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"CP"]
npart <- arbol$cptable[which.min(arbol$cptable[,"xerror"]),"nsplit"]

# Arbol para dist rambla

load(here("Modelos/Resultados/ARBOL","arbol_prune_lat_lon.RDS")) 


#### RF

load(here("Modelos/Resultados/RF","RF_train.RDS")) # rf_TRAIN

load(here("Modelos/Resultados/RF","RF_train_mr.RDS")) # rf_train_mr


#### Boosting

load(here("Modelos/Resultados/BOOSTING","boosting_train.RDS")) 

load(here("Modelos/Resultados/BOOSTING","boosting_train_mr.RDS")) 

#### SVR

load(here("Modelos/Resultados/SVR","SVR_train.RDS"))

load(here("Modelos/Resultados/SVR","SVR_train_mr.RDS"))

#### Caret

#### LM

load(here("Modelos/Resultados/LM/Caret","LM_caret.RDS")) 

load(here("Modelos/Resultados/LM/Caret","LM_caret_mr.RDS")) 

#### RF

load(here("Modelos/Resultados/RF/Caret","RF_caret.RDS")) 

load(here("Modelos/Resultados/RF/Caret","RF_caret_mr.RDS")) 

load(here("Modelos/Resultados/RF/Caret","RF_caret_tunning.RDS")) 

load(here("Modelos/Resultados/RF/Caret","RF_caret_tunning_mr.RDS")) 

#### Boosting

load(here("Modelos/Resultados/BOOSTING/Caret","Boosting_caret.RDS")) 

load(here("Modelos/Resultados/BOOSTING/Caret","Boosting_caret_mr.RDS")) 

load(here("Modelos/Resultados/BOOSTING/Caret","Boosting_caret_tunning.RDS")) 

load(here("Modelos/Resultados/BOOSTING/Caret","Boosting_caret_tunning_mr.RDS")) 

#### SVR

load(here("Modelos/Resultados/SVR/Caret","SVR_caret.RDS"))

load(here("Modelos/Resultados/SVR/Caret","SVR_caret_mr.RDS"))

load(here("Modelos/Resultados/SVR/Caret","SVR_caret_tunning.RDS"))

load(here("Modelos/Resultados/SVR/Caret","SVR_caret_tunning_mr.RDS"))

# cARGAMOS IMPORTANCIA VARIABLES

load(here("Modelos/Resultados/RF","importancia_rf.RDS")) 

load(here("Modelos/Resultados/RF","importancia_rf_mr.RDS")) 

load(here("Modelos/Resultados/SVR","importancia_SVR.RDS"))

load(here("Modelos/Resultados/SVR","importancia_SVR_mr.RDS"))

load(here("Modelos/Resultados/BOOSTING","importancia_boosting.RDS")) 

load(here("Modelos/Resultados/BOOSTING","importancia_boosting_mr.RDS")) 

# INTERPRETACIÓN

load(here("Modelos/Resultados/BEST","importancia_best.RDS")) 

load(here("Modelos/Resultados/BEST","pdp_dist_rambla.RData"))

load(here("Modelos/Resultados/BEST","pdp_dist_shop.RData"))

load(here("Modelos/Resultados/BEST","pdp_total_area.RData"))

load(here("Modelos/Resultados/BEST","pdp_full_bathrooms.RData"))

load(here("Modelos/Resultados/BEST","pdp_dr_fb.RData"))

load(here("Modelos/Resultados/BEST","pdp_ta_ech.RData"))

pdp_dist_rambla <- pdp_dist_rambla_d
pdp_dist_shop <- pdp_dist_shop_d
pdp_total_area <- pdp_total_area_d
pdp_full_bathrooms <- pdp_full_bathrooms_d
pdp_dr_fb <- pdp_dr_fb_d
pdp_ta_ech <- pdp_ta_ech_d

rm(pdp_dist_rambla_d, pdp_dist_shop_d, pdp_total_area_d, pdp_full_bathrooms_d, pdp_dr_fb_d, pdp_ta_ech_d)

@


<<mapas>>=
# Vectoria INE
mapa_barrio <- st_read(here("Fuentes_externas/Mapas", "INE_barrios"), quiet = TRUE)
mapa_barrio <- st_transform(mapa_barrio, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Geometría centros comerciales
mall <- st_read(here("Fuentes_externas/Mapas","centros_comerciales"), quiet = TRUE)
mall <- st_transform(mall, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
mall <- mall %>% select(Name, geometry)

# Geometría zona_avditalia
avd_italia <- st_read(here("Fuentes_externas/Mapas","zona_avditalia"), quiet = TRUE)
avd_italia <- st_transform(avd_italia, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
avd_italia <- avd_italia %>% select(Name, geometry)

# Geometría Rambla Este - MVD
rambla <- st_read(here("Fuentes_externas/Mapas", "rambla_Este"), quiet = TRUE)
rambla <- st_transform(rambla, '+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')
rambla <- rambla %>% select(Name, geometry)
@


<<>>=
# Centroide barrios

#Devuleve geometría con el centroide de cada barrios
centroide_barrios <- st_centroid(mapa_barrio)

# Extrae coordenadas (longitud y latitud) de geometría del centroide
centroide_barrios <- centroide_barrios %>%
  mutate(lon_barrio = st_coordinates(centroide_barrios$geometry)[,1],
         lat_barrio = st_coordinates(centroide_barrios$geometry)[,2])

# Pasa latitud y longitud del centroide a objeto sf
centroide_barrios_sf <- centroide_barrios %>% 
  st_as_sf(coords = c("lat_barrio","lon_barrio"), crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Transforma coordenadas a formato long lat
centroide_barrios_sf_t <- st_transform(centroide_barrios_sf,crs='+proj=longlat +zone=21 +south +datum=WGS84 +units=m +no_defs')

# Auxiliar
centroide_barrios <- centroide_barrios %>% 
  mutate(aux_lon = NA,
         zona_avditalia = NA)

# Extrae latitud y longitud de la geometría zona_avditalia
puntos_avditalia <- st_coordinates(avd_italia)

puntos_avditalia <- as_tibble(puntos_avditalia) %>% select(-Z, -L1) %>%
  rename('lon_avditalia' = 'X',
         'lat_avditalia' = 'Y')

# Geometría avd italia se conforma en total de 60 puntos
# For loop  - asigna zona a cada barrio 

for (i in 1:nrow(centroide_barrios)) {
  centroide_barrios$aux_lon[i] <- which.min(abs(centroide_barrios$lon_barrio[i] - 
                                                  puntos_avditalia$lon_avditalia))
  centroide_barrios$zona_avditalia[i] <- ifelse(
    puntos_avditalia$lat_avditalia[centroide_barrios$aux_lon[i]] < 
      centroide_barrios$lat_barrio[i], 'norte', 'sur')
}

centroide_barrios <- centroide_barrios %>% 
  data.frame() %>% 
  select(NOMBBARR, zona_avditalia)

mapa_barrio <- mapa_barrio %>% left_join(centroide_barrios, by = 'NOMBBARR')
@


%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCCION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{chapter}{0}
\setcounter{section}{0}
\setcounter{table}{0}

\chapter{Introducción \label{cap:Intro}}

El presente trabajo tiene como principal objetivo el estudio e implementación de diferentes técnicas de aprendizaje estadístico mediante las cuáles realizar predicciones de una variable de interés. En particular, se trabajó con el precio de oferta en dólares estadounidenses de los apartamentos a la venta en el departamento de Montevideo, Uruguay, disponibles en \textit{Mercado Libre}  (precio de oferta en dólares) para el periodo Mayo a Setiembre del año 2021.

Para ello, se plantearon 3 objetivos específicos, siendo estos: 1) obtención y procesamiento de los datos para transformarlos en información, con énfasis en la automatización y reproducibilidad de los mismos; 2) implementación de técnicas de aprendizaje estadístico que permitan una mayor flexibilidad en la estimación con respecto a las técnicas clásicas de estimación y 3) una primera aproximación a un análisis de interpretabilidad enfocándose en los \textit{métodos globales modelo-agnósticos}.

En lo que respecta al primer objetivo, los datos de oferta de los apartamentos fueron obtenidos a través de la interfaz de programación de aplicaciones (API) de \textit{Mercado Libre}. Esto implicó la creación de un programa que permite una descarga automatizada de la información disponible. Asimismo, se trabajó con fuentes adicionales de información externas. En particular con la \textit{Encuesta Contínua de Hogares} (ECH) (\url{https://www.ine.gub.uy/web/guest/encuesta-continua-de-hogares1})y datos obtenidos de la plataforma \textit{Google My Maps} (\url{https://www.google.com/intl/es/maps/about/mymaps/}). De esta manera, fueron construídas variables adicionales que se consideraron de interés para el problema planteado.

En cuanto al segundo objetivo, las técnicas de aprendizaje estadístico implentadas se separan en dos grupos. En primer lugar, un conjunto de métodos de agregación basados en \textit{Árboles de Decisión}, en particular \textit{Random Forest} y \textit{Boosting} (James, 2013) (\cite{james2013introduction}). En segundo lugar, se trabajó con la técnica \textit{Support Vector Regression} la cual pertenece al grupo de los modelos de regresión robusta. (Vapnik, 1999) (\cite{vapnik1999nature})

En lo que respecta a las técnicas clásicas de estimación, se realizó la aplicación de un \textit{Modelo de Regresión Lineal Múltiple} (Carmona, 2005) (\cite{carmona2005modelos}). Éste último en la medida que es una técnica usualmente aplicada en problemas de éste índole y conocido en la literatura económica como \textit{Modelo de Precios Hedónicos}. 

Una vez implementados los modelos mencionados en el punto anterior, con el fin de determinar el modelo con mejor capacidad predictiva, se llevó a cabo un análisis comparativo en función de diferentes métricas.

Por último, en la medida que los modelos implementados (con excepción del \textit{Modelo de Regresión Lineal Múltiple}) se denominan como modelos de caja negra (Molnar, 2020) (\cite{molnar2020interpretable}) se llevó a cabo un análisis de interpretabilidad aplicado al mejor modelo en términos de performance predictiva. Esto último fue abordado principalmente mediante la realización de un análisis gráfico. 

Se destaca que las técnicas utilizadas implican un alto costo computacional. De esta forma, con el fin de obtener un mayor poder de cómputo se utilizó un enfoque desde la programación en paralelo. A su vez, todas las etapas fueron realizadas teniendo en cuenta la reproducibilidad de las mismas.

Los resultados obtenidos fueron a través del lenguaje y entorno de programación para análisis estadístico y gráfico, \textit{R} (\cite{statsR}), enfocándose en la optimización de todos los procesos principalmente mediante la programación en paralelo. 

El trabajo se conforma de 8 capítulos. A continuación se presenta en el Capítulo \ref{cap:datos} el detalle de la obtención y depuración de los datos utilizados. Luego, en el Capítulo  \ref{cap:Antec} se mencionan los principales antecedentes consultados. En lo que respecta al Capítulo \ref{cap:MT}, se detalla la metodología utilizada y el sustento teórico de la misma. Éste último seguido por el Capítulo \ref{cap:reproduc} donde se presentan los aspectos que conciernen a la reproducibilidad del trabajo. Posteriormente, en los capítulos \ref{cap:EDA} y \ref{cap:Resultados} se presenta el análisis exploratorio de los datos y los principales resultados obtenidos respectivamente. Por su parte, en el Capítulo \ref{cap:conclu} se presentan las principales conclusiones abordadas y principales líneas de trabajo para futuras investigaciones. 

\chapter{Datos \label{cap:datos}}

Como fue mencionado en el Capítulo \ref{cap:Intro}, el principal objetivo del trabajo es el estudio e implementación de diferentes técnicas de aprendizaje estadístico mediante las cuáles realizar predicciones de una variable de interés. En particular se trabajó con el precio de oferta en dólares estadounidenses de los apartamentos a la venta en Montevideo, Uruguay, para el periodo Mayo a Setiembre del año 2021.

A continuación se presenta en la Sección \ref{sec:obtencion} la descripción del proceso de obtención de los datos. Luego, en la Sección \ref{sec:procesamiento} se detallan los principales criterios de depuración utilizados. Posteriormente, en la Sección \ref{sec:fuentesext} se describen las principales fuentes externas de información utilizadas. Por último en la Sección \ref{sec:varselab} se presentan las variables construídas en base a elaboración propia.

\section{Obtención \label{sec:obtencion}}

La base de datos utilizada fue obtenida consultando la API de \textit{Mercado Libre}. Donde con el fin de interactuar con la misma, es necesario seguir los siguientes pasos: 1) El usuario se registra en la web \url{https://developers.mercadolibre.com.uy/}; 2) se crea una aplicación con la cual se obtiene un identificador y una contraseña para interactuar con la API y 3) se obtiene una clave (\textit{token}) para poder realizar las consultas. 

En particular, para obtener la información de las publicaciones de apartamentos se realiza una consulta a la API especificando en la \textit{url} la categoría \textit{MLU1474}. Adicionalmente, para considerar solamente los apartamentos en el departamento de Montevideo, se especifica en la \textit{url} el identificador de cada uno de los barrios de Montevideo. En el Anexo \ref{barriosA} se presenta en la Tabla \ref{tab11} los barrios de Montevideo y su identificador. 

Luego, con el fin de obtener un mayor número de variables explicativas, se accedió a los atributos específicos de cada publicación. Para ello, es necesario ingresar en la \textit{url} cada identificador de cada publicación. 

De esta manera fue posible obtener la información de todos los apartamentos a la venta en Montevideo disponible en la API de \textit{Mercado Libre} y considerada de interés de manera automatizada. 

Se destacan dos aspectos relevantes en cuanto al funcionamiento de la API de \textit{Mercado Libre}. 

En primer lugar, es posible obtener la información de las publicaciones vigentes a la fecha de consulta. Por lo tanto, se realizaron descargas mensuales abarcando el periodo comprendido entre los meses de Mayo a Setiembre del año 2021, ambos inclusive. Dado que las publicaciones mantienen los identificadores a través de los meses, para el caso de las publicaciones que se mantienen vigentes más de un mes, se optó por considerar los datos correspondientes al último mes de vigencia. 

En segundo lugar, la API permite extraer hasta un límite determinado de publicaciones por consulta (10.000 consultas a la fecha de realización del trabajo). Debido a que las consultas fueron realizadas de forma iterativa por barrio, fue posible sobrellevar esta limitante. 

\section{Procesamiento y criterios de depuración \label{sec:procesamiento}}

El proceso de depuración de los datos fue realizado diferenciando según la naturaleza de las variables.
 
En primer lugar, en lo que respecta a las variables cuantitativas el proceso de depuración se centró en el reconocimiento de valores erróneos, por ejemplo valores que repitan una secuencia de números.

Para ello se construyó una función auxiliar que es capaz de detectar cuándo un valor tiene 3 o más números iguales repetidos. En ese caso se considera que el dato es erróneo y se lo asigna como valor faltante, con excepción de la variable precio de oferta en dólares donde la observación completa es eliminada.

Asimismo se decidió no considerar en el análisis las observaciones cuyo precio de oferta es inferior al valor del percentil 75\% entre las observaciones con precio inferior a USD 40.000 ya que se consideran datos erróneos o de muy baja frecuencia. De manera similar y debido a la elevada presencia de valores atípicos, se eliminan todas las observaciones cuyos valores de la variable precio de oferta en dólares superan el percentil 95\% de la misma. De esta manera, no se consideraron en el análisis aproximadamente un 8 \% de las observaciones.

A su vez, en lo que respecta a las variables área total y área cubierta se decidió asignar como valor faltante a todos los valores superiores a 1000 metros cuadrados, ya que se considera que podrían ser datos erróneos. 

En segundo lugar, respecto a las variables cualitativas, existe un conjunto de variables asociadas a la publicación del inmueble que toman valor Si, No, o faltante. Debido a que en todos los casos se trata de campos no obligatorios para el usuario que realiza la publicación, no es posible diferenciar fehacientemente entre los valores faltantes y el valor No. Por lo tanto se trabajó bajo el supuesto de que los valores faltantes corresponden a el valor No y de esta forma se realizó la recodificación correspondiente.

En tercer lugar, en cuanto a las variables latitud y longitud que permiten georreferenciar a los apartmentos, se realizó un proceso de depuración atendiendo a la factibilidad del dato. Para ello se consideraron los valores de ambas variables correspondientes a coordenadas geográficas dentro de Montevideo. De esta manera, valores de latitud inferiores a -35 y superiores a -34.7, al igual que valores de longitud inferiores a -56.5 y superiores a -56 se consideraron datos erróneos ya que Montevideo se encuentra entre dichos valores de latitud y longitud. Los mismos fueron recodificados imputando los valores correspondientes al centroide del barrio donde se ubica el apartamento.

A su vez, se detectó otro tipo de error en las georreferencias siendo este el caso de aquellas georreferencias que no se ubican dentro del polígono del barrio al cual pertenece el apartamento. Para estos casos, se supone que el dato correcto es el nombre del barrio y no la georreferencia específica, y se imputa el valor de latitud y longitud correspondiente al centroide del barrio.

Dada la complejidad que implica la detección de estas georreferencias erróneas, mediante un procedimiento de \textit{trade-off} entre costo  y complejidad, se procedió de la siguiente forma: 1) se separó el mapa de Montevideo en dos regiones utilizando el corte de la calle Avenida Italia en continuación con la calle Avenida 18 de Julio; 2) se computó a qué región resultante de 1) pertenece el apartamento utilizando los valores de latitud y longitud; 3) se procedió de forma análoga a 2) considerando el centroide del barrio al cual pertenece el apartamento, 4) se evalua si 2) y 3) coinciden, en caso que difieran se le imputa al apartamento los valores de latitud y longitud correspondientes al centroide del barrio.  

En cuarto lugar, se decidió considerar en el análisis las variables con porcentaje de valores faltantes inferior al 15 \%. En la Tabla \ref{tabna} de la Sección del Anexo \ref{varsA} se presenta la proporción de valores faltantes para cada una de ellas.

Por último, en lo que respecta al tipo de cambio, todos los valores de la variable precio de oferta expresados en moneda nacional fueron convertidos a dólares estadounidenses utilizando el tipo de cambio a la fecha de obtención de los datos. Sobre éste punto, se destaca que la obtención del valor del tipo de cambio se realiza de manera automatizada realizando un procedimiento de \textit{Web Scrapping} sobre la página web oficial del \textit{Instituto Nacional de Estadistica} (INE) (\url{https://www.ine.gub.uy/}).

\section{Fuentes externas de información\label{sec:fuentesext}}

En la medida que la base de datos construida contiene información sobre la latitud y longitud donde está ubicado cada apartamento, se consideró de interés la elaboración de variables geoespaciales.

La herramienta utilizada para construir mapas fue \textit{Google My Maps}, el cual es un servicio puesto en marcha por \textit{Google} en abril del 2007, que permite a los usuarios crear mapas personalizados para uso propio o para compartir. Los usuarios pueden añadir puntos, líneas y formas sobre \textit{Google Maps}. (\url{https://sites.google.com/mrpiercey.com/resources/geo/my-maps}).

Una vez que se construyen los mapas desde dicha plataforma, se exportan los archivos generados. Luego, éstos son transformados a archivos ESRI Shapefile utilizando \textit{QGIS} el cual es un  \textit{Sistema de Información Geográfica} (SIG).

El formato ESRI (Environmental Systems Research Institute, Inc.) Shapefile (SHP) es un formato vectorial de almacenamiento digital donde se guarda la localización de los elementos geográficos y los atributos asociados a ellos. Es un formato multiarchivo, es decir está generado por varios ficheros informáticos. El número mínimo requerido es de tres y tienen las extensiones siguientes:
      
\begin{itemize}       
\item  shp es el archivo que almacena las entidades geométricas de los objetos.
\item  shx es el archivo que almacena el índice de las entidades geométricas.
\item  dbf es la base de datos, en formato dBASE, donde se almacena la información de los atributos de los objetos.
\end{itemize}  

(ESRI, 1998) (\cite{environmental1998esri})

En lo que respecta a la geometría del departamento de Montevideo la misma fue obtenida a partir de los archivos Shapefile disponibles en la página web del \textit{Instituto Nacional de Estadistica} (INE) en la siguiente dirección: \url{https://www.ine.gub.uy/}.

Por otra parte, se utilizó también información de la encuesta contínua de hogares 2020 para la construcción de variables que consideren el nivel de ingreso por barrio.

\section{Variables construidas \label{sec:varselab}}

Como se menciona en la Sección \ref{sec:fuentesext}, tener la georreferencia específica en la base de datos motivó la elaboración de variables geoespaciales. En particular, las variables construídas fueron: ubicación respecto a la calle Avenida Italia en continuación con la calle Avenida 18 de Julio (zona Avd. Italia), distancia a la rambla este de Montevideo (distancia a la rambla este), y distancia al centro comercial más cercano (distancia al centro comercial más cercano).

La metodología utilizada para el cálculo de las distancias fue a través de la fórmula de \textit{Haversine} la cual permite el cómputo de la distancia mínima entre dos puntos que se encuentran en un cuerpo esférico utilizando latitud y longitud. Para ello en particular se trabajó con la función \textit{distm} del paquete \textit{geosphere} (\cite{geos}). En el Anexo \ref{harvesineA} se especifican los detalles sobre el cálculo. 

En lo que respecta a la variable zona Avd. Italia, la misma toma valor norte o sur según el apartamento se encuentre al norte o al sur del corte de la calle Avenida Italia en continuación con la calle Avenida 18 de Julio. Para ello, se tomó el punto más cercano a la ubicación del apartamento en el mapa de líneas que compone la calle Avenida Italia en continuación con la calle Avenida 18 de Julio. En particular, se toma la diferencia mínima en términos de longitud entre el apartamento y los puntos que componen el mapa. Una vez obtenido el mínimo, se comparan las latitudes. En caso que la latitud del apartamento sea mayor que la del punto más cercano seleccionado, se asigna sur, y en otro caso se asigna norte.

Por otra parte, la variable distancia al centro comercial más cercano fue contruida calculando la distancia en metros entre el apartamento y el centro comercial más cercano. Para ello, fue necesaria la obtención de las coordenadas geográficas (latitud y longitud) de todos los centros comerciales ubicados en Montevideo (Montevideo Shopping Center, Punta Carretas Shopping, Tres Cruces Shopping, Nuevocentro Shopping y Portones Shopping).

Por último, la variable distancia a la rambla este fue construida utilizando la distancia en metros entre el apartamento y la rambla Este de Montevideo. Esta variable fue construida a través de los resultados observados mediante el ajuste de un \textit{Árbol de Regresión} de la variable precio de oferta en dólares en función de la latitud y la longitud que corresponde a la ubicación de cada apartamento. En el mismo se observó que los apartamentos con predicciones superiores toman valores de latitud entre -34.92 y -34.89, y valores de longitud mayores o iguales a -56.17. En la Sección del  Anexo \ref{arbollatlonA} se encuentra el gráfico del árbol ajustado.

Se destaca que los mapas utilizados para construir las variables zona Avd. Italia, distancia a la rambla este y distancia al centro comercial más cercano fueron obtenidos en base a elaboración propia a partir de \textit{Google My Maps}, según detallado en la Sección \ref{sec:fuentesext}. 
En la Figura \ref{map1} se presenta el mapa del departamento de Montevideo, Uruguay, con las variables construidas.

\newpage

\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
      geom_sf(aes(fill = zona_avditalia )) +
      geom_sf(data = mall) +
      geom_sf(data = avd_italia, color = 'blue', size = 0.5) +
      geom_sf(data = rambla, color = 'yellow2', size = 0.5) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      ggrepel::geom_label_repel(data = mall,aes(label = Name, geometry = geometry),
      stat = "sf_coordinates", min.segment.length = 0,
      colour = "black", segment.colour = "black",
      size = 3, alpha = 0.8) +
      xlab('Longitud') +
      ylab('Latitud') +
      scale_fill_manual(name = 'Zona Avd. \n Italia', values = c('orangered2', 'springgreen4'))
@
\captionof{figure}{Mapa de Montevideo, Uruguay con la georreferencia de los centros comerciales, calle Avenida Italia en continuación con la calle Avenida 18 de Julio y rambla este de Montevideo . La línea azul indica la calle Avenida Italia en continuación con la calle Avenida 18 de Julio mientras que la línea amarilla indica la rambla este de Montevideo.}
\label{map1}
\end{figure}

Por otra parte, se construyó la variable ingreo medio ECH utilizando la información de la encuesta contínua de hogares (ECH) del año 2020 disponible en \url{https://www.ine.gub.uy/}. En particular se utilizó la variable \textit{HT11}: Ingreso total del hogar con valor locativo sin servicio doméstico (en pesos uruguayos). Se calculó el ingreso medio por barrio de los hogares de Montevideo y luego se asignó a cada observación, el nivel de ingreso medio que le corresponda según el barrio donde se encuentre el apartamento.

\newpage
En la Figura \ref{map2} se presenta el mapa del ingreso promedio de los hogares por barrio de Montevideo.

<<>>=

load(here("Fuentes_Externas/ECH/HyP_2020_Terceros.RData"))

f <- f %>% 
  select(numero, nper, hogar, nombarrio, HT11, ht13, YHOG, YSVL, lp_06, pobre_06,
                  i228, i174, i259, i175, h155, h155_1, h156, h156_1, pesomen) %>%
  filter(hogar == 1)

# Considera pesos ECH

f <- f %>% 
  group_by(nombarrio) %>%
  summarise(media_ingbarr = sum(pesomen*HT11, na.rm = TRUE) / 
              sum(pesomen, na.rm = TRUE))

f <- f %>% rename('NOMBBARR' = 'nombarrio')

# Quitamos espacios en blanco al final de nombbarr
f$NOMBBARR <- trimws(f$NOMBBARR, which = "right", whitespace = "[ \t\r\n]")

f$NOMBBARR <- recode(as.factor(f$NOMBBARR), 
                     'Malvín' = 'Malvin',
                     'Malvín norte' = 'Malvin norte',
                     'Unión' = 'Union',
                     'Maroñas, Parque Guaraní' = 'Maroñas, Parque Guarani',
                     'Villa García, Manga Rur.' = 'Villa Garcia, Manga Rur.')

mapa_barrio <- mapa_barrio %>% left_join(f, by = 'NOMBBARR')

media_mvd <- mean(f$media_ingbarr)/1000
@


\begin{figure}
\centering
<<fig=TRUE>>=
ggplot(mapa_barrio)+
            geom_sf(aes(fill = media_ingbarr/1000 )) +
      theme(axis.text.x = element_text(angle = 45),
            text = element_text(),
            panel.grid.major = element_line(
                  color = '#cccccc',linetype = 'dashed',size = .3),
            panel.background = element_rect(fill = 'aliceblue'),
            axis.title = element_blank(),
            axis.text = element_text(size = 8),
            legend.position = 'bottom',
            legend.text = element_text(angle = 0, size = 7),
            legend.title = element_text(size = 9, face = 'bold', hjust = 0.5)) +
      xlab('Longitud') + ylab('Latitud') +
      scale_fill_gradient2(low = 'red', mid = 'white', high = 'blue', name = "Ingreso promedio \n por mil ECH",labels = comma,  midpoint = media_mvd) 
@
\captionof{figure}{Mapa del departamento de Montevideo, Uruguay, por nivel de ingreso promedio por mil obtenido a partir de la Encuesta Contínua de Hogares 2020. Los barrios en color rojo tienen un ingreso promedio inferior al ingreso promedio de Montevideo mientras que los barrios en color azul tienen un ingreso promedio superior al mismo.}
\label{map2}
\end{figure}

\chapter{Antecedentes \label{cap:Antec}}

Como fue mencionado en el Capítulo \ref{cap:Intro}, el presente trabajo  tiene como principal objetivo la implementación y estudio de diferentes técnicas de aprendizaje estadístico multivariadas aplicadas sobre el precio de oferta de los apartamento a la venta en Montevideo, Uruguay.

En éste sentido, existen diversos artículos y documentos de trabajo que tratan en particular sobre la implementación de modelos predictivos para el precio de los inmuebles en el mercado uruguayo.

En primer lugar, se tiene el trabajo de Ponce (2012) (\cite{ponce2012precio}) donde propone un modelo de precios de fundamentos para las viviendas el cual se basa en el hecho de que una vivienda puede ser considerada como un activo de inversión y como un activo que brinda servicios. De esta forma, el precio puede ser considerado como el resultado del mercado por los servicios de vivienda y como el resultado de equilibrio en un mercado de activos. Concluye, entre otros aspectos, que los precios de las viviendas fluctúan más que lo justificado por sus fundamentos lo cual implica periodos de subvaloración o sobrevaloración de los precios de las viviendas. (Ponce, 2012) (\cite{ponce2012precio})

Posteriormente, Ponce y Tubio (2013) (\cite{ponce2013precios}) realizan una aplicación de modelos hedónicos para el mercado uruguayo. Para ello utilizan una base de datos con información de inmuebles que se ofertan a través de la web. La misma contiene información de más de 500 inmobiliarias y más de 20 barrios de Montevideo, Uruguay. Se trata de una sistematización de metodologías existentes para la elaboración de índices de precios de inmuebles, en particular atendiendo a modelos que permitan evaluar el desvío de los precios corrientes con respecto a los fundamentos del mercado. (Ponce, 2013) (\cite{ponce2013precios})

Luego, Landaberry y Tubio (2015) (\cite{landaberry2015estimacion}) con el fin de monitorear el mercado de viviendas en Uruguay proponen una serie de índices de precios. En particular para el caso de Montevideo proponen índices desde un enfoque hedónico ya que, entre otras cosas, permiten estimar precios sombras para los atributos de las viviendas. (Landaberry, 2015) (\cite{landaberry2015estimacion})

En el año 2017 Goyeneche et al (\cite{goyeneche2017prediccion}) trabajaron en la construcción de un modelo predictivo del valor contado de un determinado inmueble, entendido como valor contado aquel que es asignado por el tasador. En línea con esto, recurrieron a la metodología de \textit{Stacking} con el fin de lograr predicciones más precisas. En particular trabajaron con información de inmuebles en Montevideo, Uruguay. La base de datos utilizada fue proporcionada por el Banco Hipotecario del Uruguay (BHU). (Goyeneche, 2017) (\cite{goyeneche2017prediccion})

Por último, en el año 2019 Picardo (\cite{picardo2019prediccion}) presenta modelos predictivos para el precio de las viviendas. En particular trabajó mediante la implementación de \textit{Modelos de Regresión Lineal Múltiple}, \textit{Árboles de Regresión}, y el algoritmo \textit{Random Forest} para inmuebles ubicados en Montevideo, Uruguay. La base de datos utilizada corresponde a elaboración propia obtenida a través de la web y registros administrativos de transacciones de la Dirección General de Registros (DGR). 

Se destaca que éste último se considera como principal antecedente, donde en el presente trabajo se tiene como principal línea de investigación profundizar en las técnicas de aprendizaje estadístico al igual que obtener una mayor medida de interpretabildad de las mismas.

En lo que respecta a las técnicas estadísticas utilizadas, se consultaron diferentes literaturas. En éste sentido, entre las bibliografías destacadas para los algoritmos de aprendizaje estadístico se encuentra el libro publicado por Gareth et al (\cite{james2013introduction}), al igual que el libro publicado por Hastie et al (\cite{hastie2001tibshirani}).

Por otra parte, en lo que respecta al detalle metodológico del modelo de regresión líneal, fue consultado principalmente el trabajo de Francesc Carmona (\cite{carmona2005modelos}). 

Por último, para el análisis de interpretabilidad de los modelos de aprendizaje estadístico fueron consultados los trabajos de Molnar (\cite{molnar2020interpretable}) y Greenwell et al (\cite{greenwell2020variable}).


\chapter{Marco teórico y metodología \label{cap:MT}}

En este Capítulo se presenta los aspectos metodológicos más relevantes para la realización del trabajo. El mismo se encuentra conformado por cuatro secciones.

En primer lugar, en la Sección \ref{sec:machinelearning} se detalla la metodología de los modelos ajustados. Luego en las Secciones \ref{sec:cv} y \ref{sec:interp}, se encuentra explicitada la metodología para evaluar la performance predicitva y realizar un análisis de interpretabilidad de los modelos ajustados respectivamente.

Por último, en la Sección \ref{sec:secfalt} se presenta la metodología llevada a cabo para realizar el tratamiento de datos faltantes en la base de datos.

\section{Análisis Supervisado y Aprendizaje Estadístico\label{sec:machinelearning}}

Con el fin de obtener predicciones del precio de oferta de los apartamentos, fueron implementadas diferentes técnicas de aprendizaje estadístico. Estas consisten en modelar y analizar conjuntos de datos, mediante el aprendizaje de ejemplos, con el fin de predecir y estimar resultados en forma automática.

En este contexto, se realizó un análisis supervisado, en la medida de que se cuenta con una variable de salida ($Y$) y variables de entrada ($X$). Por lo tanto, se tiene que los posibles modelos son de la forma:

\begin{center}

$Y=f(X)+\epsilon$

\end{center}

Siendo $f$ una función desconocida y $\epsilon$ un error aleatorio independiente de $X$ e $Y$ con media 0. Se denota a la matriz $X$ de dimensión $n \times p$ a la matriz de datos, donde se tiene $n$ observaciones de entrenamiento y $p$ variables. 

La i-ésima fila se corresponde a la i-ésima observación (perteneciente al conjunto de entrenamiento) siendo de la forma $x_{i}=(x_{i1},\dots,x_{ip})^{T}$, con $x_{i}\in\mathbb R^{p}$. Por otro lado, se denota una nueva observación (o pertenciente al conjunto de testeo) como $x^{*}=(x_{i1}^{*},\dots,x^{*}_{ip})^{T}$, donde esta es un vector p-dimensional (al igual que $x_{i}$). 

A la hora de estimar $f$, se realizó mediante métodos paramétricos, explicitados en la Sección \ref{subsec:mlMT}, y métodos no paramétricos detallados en las Secciones \ref{subsec:arbol}, \ref{subsec:rf}, \ref{subsec:boosting} y \ref{subsec:svr}. 

En el primer caso, se asumió la forma funcional de $f$ y se procedió a estimar sus respectivos parámetros. Por otro lado, en los métodos no paramétricos, no se asumió la forma funcional de $f$.

\subsection{Modelo de Regresión Lineal Múltiple \label{subsec:mlMT}}

El \textit{Modelo de Regresión Lineal Múltiple} donde para el problema de aplicación seleccionado se conoce en la literatura como \textit{Modelo de Regresión Lineal Múltiple de Precios Hedónicos} (o simplemente \textit{Modelo de Precios Hedónicos}) parte del supuesto de que los precios observados de los productos se pueden desglosar en una suma de cantidades específicas de determinadas características asociadas al bien. De esta manera se define un set implícito de precios, también conocidos como \textit{precios hedónicos}. (Rosen, 1974) (\cite{rosen1974hedonic})

Este tipo de modelos fueron introducidos por Giriliches en el año 1961 (\cite{griliches1961hedonic}) para el precio de los automóviles y luego desarrollado y profundizado por Rosen en el año 1974 (\cite{rosen1974hedonic}).

De esta manera, el precio del bien es regresado sobre las características del mismo, y utilizando técnicas clásicas de estimación se obtienen los anteriormente mencionados precios hedónicos.

En particular, este modelo puede aplicarse a los precios de los bienes inmuebles.
Donde, para estos últimos se destaca que entre las características asociadas al inmueble pueden considerarse características que son propias del mismo así como también características asociadas a la localización geográfica en donde se encuentra ubicado, entre otras. (De Bruyne, 2013) (\cite{de2013explaining}).

Formalmente, el modelo se especifica como:

$$Y = f(X) + \epsilon$$

donde 

$$f(X) = \mathbf{X'}\boldsymbol{\beta}$$

$$\mathbf{X'}\boldsymbol{\beta} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p$$

con $\epsilon$ igual al vector de errores del modelo y a su vez $\epsilon_i$ independientes e identicamente distribuidos $N(0, \sigma^2)$. (Carmona, 2005) (\cite{carmona2005modelos}).

Siendo $(X_1, \cdots, X_p)$ el vector de las $p$ características asociadas al bien y $(\beta_1, \cdots, \beta_p)$ el vector de los precios hedónicos. Es importante observar que el vector de precios hedónicos asociados a las características coincide con el vector de los parámetros de un modelo líneal clásico.

La estimación de los parámetros $\boldsymbol{\beta} = (\beta_1, \cdots, \beta_p)$ se realiza por el método de los mínimos cuadrados. Para ello, se intenta hallar los valores $\boldsymbol{\hat{\beta}} = (\hat{\beta_1}, \cdots, \hat{\beta_p})$ que minimicen la suma de cuadrados de los residuos $\boldsymbol{\epsilon'\epsilon} = \boldsymbol{(Y - X'\beta)'(Y - X'\beta)}$.

De esta manera, se obtiene que la estimación del vector  $\mathbf{\beta}$ que minimiza la suma de los cuadrados de los residuos es solución de la siguiente ecuación:

$$\mathbf{X'X\beta=X'Y} 

$$\hat{Y} = \hat{f}(X) =  \boldsymbol{X'\hat{\beta}}$$

En lo que respecta a la bondad de ajuste del modelo, se utilizaron el coeficiente de determinación ($R^2$) y coeficiente de determinación ajustado ($R^2_a$).

Se define al coeficiente de determinación ($R^2$) a través de la siguiente ecuación:

$$R^2 = 1 - \frac{SCR}{SCT}$$

donde:

\begin{itemize}
\item $SCR = \sum_{i=1}^n (y_i - \hat{y_i})^2$
\item $SCT = \sum_{i=1}^n (y_i - \bar{y})^2$
\item $n$ el número de observaciones.
\end{itemize}

Por otro lado, se define el coeficiente de determinación ajustado mediante la siguiente expresión:

$$R^2_a = 1 - \big(\frac{n-1}{n-p-1}\big)(1-R^2)$$

donde

\begin{itemize}
\item n es el número de observaciones
\item p es el número de variables explicativas del modelo
\item $R^2$ es el coeficiente de determinación.
\end{itemize}


En lo que respecta a la significación global del modelo, se utilizó la prueba \textit{F de Fischer} (Prueba F) en donde la hipotesis nula implica la no significación global del modelo:

$$H_0) \beta_j = 0 \, \forall \, j = 1, \cdots, p$$
$$H_1) \beta_j \neq 0 \, \text{para al menos un} \, j = 1, \cdots, p$$

El estadístico del contraste (F) y su ditribución bajo la hipótesis nula cierta tiene la siguiente forma:

$$F = \frac{(SCE - SCR)/p}{SCR/(n-p-1)} \sim F_{p, n-p-1}$$

donde

\begin{itemize}
\item $SCE$ es la suma de los cuadrados explicada
\item $SCR$ es la suma de los cuadrados de los residuos
\item $n$ es el número de observaciones
\item $p$ el número de variables incluídas en el modelo
\end{itemize}

La hipótesis nula de no significación global del modelo se rechaza cuando $F > F_{p, n-p-1; 1-\alpha}$, siendo $\alpha$ el nivel de significación del contraste. (Carmona, 2005) (\cite{carmona2005modelos})

Por otra parte, para la significación individual de las variables, se utilizó la prueba \textit{T de Student} (Prueba t) en donde la hipotesis nula implica la no significación de la variable $X_j$ para explicar a la variable de respuesta. De esta manera, el contraste se especifica de la siguiente forma:

$$H_0) \beta_j = 0$$
$$H_1) \beta_j \neq 0$$

El estadístico del contraste (t) y su ditribución bajo la hipótesis nula cierta tienen la siguiente forma:

$$t = \frac{\hat{\beta_j} - \beta_j}{\hat{\sigma_j}(\hat{\beta_j})} \sim t_{n-p-1}$$

donde
\begin{itemize}
\item $\hat{\beta_j}$ es el parámetro estimado asociado a la variable $X_j$
\item $\hat{\sigma_j}(\hat{\beta_j})$ es el desvío estimado del parámetro estimado asociado a la variable $X_j$
\end{itemize}

La hipótesis nula de no significación de la variable $X_j$ se rechaza cuando $|t| > t_{n-p-1; 1- \alpha}$. (Carmona, 2005) (\cite{carmona2005modelos})

Con el fin de testear los supuestos de normalidad y homocedasticidad de los residuos del modelo existen diferentes pruebas. 

En particular, en este trabajo se realizó la prueba de normalidad de \textit{Lilliefors} sobre los residuos del modelo. Esta prueba utiliza el estadístico \textit{Kolmogorov-Smirnov} para el caso en que la media y la varianza poblacional son desconocidos. (Lilliefors, 1967) (\cite{lilliefors1967kolmogorov})

El estadístico del contraste consiste en calcular la máxima diferencia absoluta entre la distribución empírica y la función de distribución acumulada hipotética:

$$D = \max_X |F^{*}(X) - S_N(X)|$$

donde $S_N(X)$ es la distribución acumulada en los datos mientras que $F^{*}(X)$ es la distribución acumulada de una variable aleatoria con distribución normal.

En el caso que $D$ exceda un determinado valor crítico entonces se rechaza la hipótesis nula de distribución normal. (Lilliefors, 1967) (\cite{lilliefors1967kolmogorov})

Para la aplicación de la prueba anterior se utilizó la función \textit{lillie.test} del paquete \textit{nortest} (\cite{nortest}).

Por otro lado, se aplicó la prueba de homocedasticidad de \textit{Breusch-Pagan} sobre los residuos del modelo, utilizando la función \textit{bptest} del paquete \textit{lmtest} (\cite{lmtest}). Esta prueba consiste en ajustar un modelo lineal para los residuos del modelo de regresión lineal mediante la siguiente expresión:

$$\dfrac{e^{2}}{\hat{\sigma}^{2}}= Z\alpha + \mu$$

donde:

\begin{itemize}
\item $e$ es el vector de la estimación de los residuos en el modelo original
\item $\hat{\sigma}$ la estimación del desvío de los residuos en el modelo original
\item $Z$ es las matriz de variables explicativas
\item $\alpha$ es el vector de parámetros asociados a las variables explicativas
\item $\mu$ es un vector aleatorio donde cada componente sigue una distribución normal, media cero y varianza constante
\end{itemize}

De esta forma, una vez ajustado el modelo de regresión sobre los residuos del modelo inicial, el contraste se especifica de la siguiente forma:

$$H_0) \sigma_{i}^{2} = \sigma^{2} \, \forall \, i = 1, \cdots, p$$
$$H_0) \sigma_{i}^{2} \neq \sigma^{2} \, \text{para al menos un} \, i = 1, \cdots, p$$

El estadístico del contraste y su distribución bajo la hipótesis nula cierta son de la siguiente manera:

$$BP=\dfrac{SCE}{2} \sim \chi^{2}_{p}$$

De esta manera, se rechaza la hipótesis nula de homocedasticidad en el caso en que mucha varianza sea explicada por las variables explicativas. Por defecto, se utilizan en el contraste todas las variables explicativas del modelo inicial. (Breusch, 1979) (\cite{breusch1979simple}) 

Se destaca que una de las ventajas más importantes de éste tipo de modelos es la fácil interpretación. No obstante suelen tener una mala performance predictiva en comparación a otros enfoques ya que puden presentar problemas de heterocedasticidad, multicolinealidad, y variables omitidas. (James, 2013) (\cite{james2013introduction})

Por otra parte, el modelo de regresión lineal múltiple puede ser generalizado para el caso no lineal, lo cual no ha sido implementado en el presente trabajo.

\subsection{Árboles de Regresión\label{subsec:arbol}}

Luego de relizar la implemetación del \textit{Modelo de Regresión Lineal Múltiple}, se procedió a modelar mediante un \textit{Árbol de Decisión}. En la medida de que se cuenta con una variable de salida continua, se construyó un \textit{Árbol de Regresión}.

A pesar de que en la literatura existen diversos enfoques para la construcción de estos modelos, se trabajó con el método \textit{CART} el cual fue propuesto por \textit{Breiman}, \textit{Friedman}, \textit{Olshen} y \textit{Stone} en 1984. (Breiman, 2017) (\cite{breiman2017classification})

Este método se caracteriza por la realización de particiones binarias recursivas del espacio de las variables de entrada. Mediante las mismas, se conforma una organización jerárquica en forma de árbol, donde en cada nodo interior se tiene una pregunta (dicotómica) sobre una variable de entrada y en cada nodo terminal (denominado "hoja") una decisión.

De esta forma, se procede a dividir el conjunto de los valores posibles de $X_{1}\dots,X_{p}$ (variables de entrada) en $J$ regiones disjuntas $R_{1},\dots,R_{J}$.(James, 2013) (\cite{james2013introduction})

Luego para cada observación que se encuentra en la región $R_{j}$ se realiza la misma predicción. Siendo esta, en el contexto de árboles de regresión, el promedio de la variable respuesta en dicha región.

En el momento de la construcción de las regiones ($R_{1},\dots,R_{J}$) se realiza de tal forma que en cada subconjunto resultante (denominados como "nodos hijos") en cada iteración implique una disminución en la impureza de estos.

Para ello, se construyen las regiones $R_{1}$, \dots, $R_{j}$ de forma tal que minimicen la suma de cuadrados de los residuos (SCR).  

\begin{center}

$\displaystyle \sum^{J}_{j=1} \sum_{i\  \in R_{j}} \left(y_{i} - \hat{y}_{R_{j}} \right) ^{2}$ 

\end{center}

Siendo $\hat{y}_{R_{j}}$ el promedio de la variable respuesta en la j-ésima región.

Para lograr este cometido se utiliza una separación recursiva binaria de la siguiente forma. Se selecciona la variable $X_{j}$ y el número $s$ dividiendo el espacio en dos regiones $R_{1}(j,s) = \lbrace{ X : X_{j} < s \rbrace}$ y $R_{2}(j,s) = \lbrace{ X: X_{j} \geq s \rbrace}$  de forma tal que se haga mínimo 

\begin{center}

$\displaystyle \sum_{i:x_{i} \in R_{1}(j,s)} \left(y - \hat{y}_{R_{1}}\right)^{2} + \sum_{i:x_{i} \in R_{2}(j,s)} \left(y - \hat{y}_{R_{2}}\right)^{2}$ 

\end{center}

Una vez encontrada la mejor partición se separan los datos en las regiones resultantes y se repite el proceso en cada región. Es decir, se busca nuevamente la mejor variable y el mejor punto de corte de forma que se incremente la disminución de la impureza en los nodos hijos.

El proceso continúa hasta que se satisfaga algún criterio de parada. Un criterio de para puede ser por ejemplo que los nodos terminales tengan cierto número de observaciones.

Luego de definido el criterio de construcción de las regiones y el criterio de parada, se procede a realizar un proceso de poda en el árbol obtenido basado en un criterio de \textit{costo-complejidad}. Esto en la medida de que si se deja crecer el árbol de forma indefinida se obtiene un modelo con un alto grado de sobre ajuste (\textit{overfitting}). Por su contraparte, un árbol muy "pequeño", posiblemente no logre capturar la estructura del conjunto de datos. (Hastie, 2001) (\cite{hastie2001tibshirani}).

El proceso de poda realizado, consiste en dejar crecer el árbol hasta que los nodos terminales tengan cierto número de observaciones (dicho árbol se denota como $T_{0}$). Luego se elige aquel subárbol el cual posee un menor error de predicción en el conjunto de testeo. En la medida de que un procedimiento de \textit{validación cruzada} aplicado en cada posible subárbol es muy costoso en términos de "tiempo computacional", surge como alternativa el método basado en un criterio de \textit{costo-complejidad}.

En dicho método se define a $T_{\alpha}$ como un subárbol obtenido al podar a $T_{0}$. De esta forma, para cada $\alpha$ se busca $T_{\alpha}$ que minimice la siguiente expresión:

\begin{center}

$C_{\alpha}(T)=\displaystyle \sum^{|T|}_{m=1} N_{m} Q_{m}(T) + \alpha|T|$

\end{center}

Donde se tiene que $|T|$ es igual número de nodos terminales del árbol $T$, mientras que $N_{m}$ es el número de observaciones en la región $R_{m}$. Por otro lado, la expresión $Q_{m}(T)$ consiste la medida de impureza.

En cuanto al parámetro $\alpha$, el mismo consiste en un parámetro de penalización aplicado a la complejidad (tamaño) del árbol. Donde valores altos de este, penalizan a árboles de gran tamaño. De esta forma, controla el compromiso entre la complejidad y la bondad de ajuste del modelo. Dicho parámetro se estima mediante \textit{validación cruzada}.

\subsection{Bagging y Random Forest\label{subsec:rf}}


A pesar de que los \textit{Árboles de Regresión} poseen un alto grado de interpretabilidad y sencillez en su implementación, estos poseen la gran limitante de ser inestables. Esto en el sentido de que pequeñas variaciones en el conjunto de entrenamiento y testeo generan grandes cambios en las estimaciones. (Hastie, 2001) (\cite{hastie2001tibshirani})

Por lo tanto, se emplearon diferentes métodos alternativos buscando estabilidad en las predicciones.

En primer lugar, se aplicó el método \textit{Random Forest} desarrollado por \textit{Breiman} en 1994. Este método consiste en construir un estimador combinando distintas versiones de estimadores.

En este contexto, estas nuevas versiones se construyen generando nuevos conjuntos de entrenamiento, mediante la técnica de remuestreo \textit{bootstrap}. Esta técnica consiste en la generación de varias muestras con reemplazo, del conjunto de datos de entrenamiento, donde a cada observación se le asigna el mismo peso ($\frac{1}{n}$, siendo $n$ el número de observaciones). Al número de muestras \textit{bootstrap} se le suele denotar $B$.

A la hora de utilizar este método en problemas de regresión, se procede a tomar varias muestras \textit{bootstrap}, donde a partir de cada una de ellas se construye un \textit{Árbol de Regresión}. Luego, se le asigna a la observación el promedio de las respuestas de los \textit{Árboles de Regresión} construidos en cada muestra. 

Se destaca que a cada árbol ajustado no se le realiza un proceso de poda. Por lo que estos mismos presentan una gran varianza, pero bajo sesgo. Sin embargo, al predecir mediante un promedio de los $B$ árboles, se logra una reducción considerable en la varianza del estimador y de esta forma se mejora la precisión de la predicción. (Hastie, 2001) (\cite{hastie2001tibshirani})

A su vez, el algoritmo a la hora de construir los diferentes estimadores (árboles), no considera en cada partición el total de variables, sino un subconjunto de estas elegido de forma aleatoria. Como primera aproximación se procedió a utilizar la parte entera de $\sqrt{p}$, siendo $p$ el número de variables. En etapas posteriores del análisis, se modificó el valor del mismo con el fin de obtener un mayor poder predictivo.

Este último punto es lo que diferencia al algoritmo con su versión más simple denominada \textit{Bagging} (también desarrollada por \textit{Breiman}). En este último se considera en cada división el total de las variables, por lo que resulta ser un caso particular del método \textit{Random Forest}.

Por lo tanto, a modo de resumen el algoritmo utilizado para realizar el ajuste mediante el método \textit{Random Forest} se expresa de la siguiente forma:

\begin{itemize}

\item{1) Sea $B$ el número de muestras bootstrap, entonces para $b = 1,\dots,B$}

  \subitem{(a) Se obtiene una muestra bootstap $Z^{b}$ de tamaño $n$ del conjunto de entrenamiento}
  
  \subitem{$(b)$ Se ajusta un árbol a los datos obtenidos en $Z^{b}$, de forma recursiva mediante la repetición de los siguientes pasos en cada nodo terminal hasta que esten conformados por un número mínimo de observaciones:}
  
  \subsubitem{$(b_{1})$ De forma aleatoria se selecciona $m$ variables de las posibles $p$ variables}
   
   \subsubitem{$(b_{2})$ De las $m$ variables obtenidas, se selecciona la mejor variable y el mejor punto de corte}
   
   \subsubitem{$(b_{3})$ Se realiza una partición en dicho nodo generado dos nodos hijos.}

\item{2) Se obtiene un conjunto de árboles $\{ T_{b} \}_{1}^{B}$}

  
  \end{itemize}
  

De esta forma, en este método se tiene que el estimador toma la siguiente forma:

\begin{center}

$\hat{f}(X) = \dfrac{1}{B} \displaystyle \sum_{b=1}^{B} T(X,\Theta_{b})$

\end{center}

Donde $\Theta_{b}$ caracteriza el b-ésimo árbol aleatorio en terminos de la muestra \textit{bootstrap} utilizada, las variables de partición, los puntos de corte en cada nodo y los valores de predicción en cada nodo terminal. 

Se optó por trabajar con el método \textit{Random Forest} ya que se destaca sobre el método \textit{Bagging} principalmente cuando se tiene que una variable es muy influyente. Esto se debe a que si en cada partición se consideran todas las variables a la hora de construir los diferentes $B$ árboles, en la medida de que se tiene una variable muy influyente, posiblemente dichos árboles no difieran mucho entre sí. Esta limitante no se presenta en \textit{Random Forest} en el sentido de que selecciona de forma aleatoria un subconjunto de los predictores en cada iteración. (Hastie, 2001) (\cite{hastie2001tibshirani})

Una característica relevante del método \textit{Random Forest} (al igual que en \textit{Bagging}), es que cada observación posee cierta probabilidad de ser seleccionada en cada remuestra realizada. De esta forma, se cuenta con un conjunto de observaciones las cuales no son utilizadas para construir el estimador.

Este conjunto de observaciones se denomina como \textit{out of bag observations} (\textit{OOB}). Por lo tanto, en cada iteración se procede a predecir dichas observaciones, mediante el estimador obtenido. Repitiendo este procedimiento para las $n$ observaciones, se calcula el \textit{error OOB}. Dicha medida se procedió a utilizar como una primera aproximación en cuanto a la performance predictiva del modelo.

A pesar de que el método anteriormente mencionado logra solucionar el problema de la inestabilidad por parte de los \textit{Árboles de decisión}, este método se caracteriza por presentar una baja interpretabilidad. 
\subsection{Boosting\label{subsec:boosting}}

Como segunda alternativa para obtener un modelo de predicción estable, se trabajó con el método de agregación basado en \textit{Árboles de Decisión}, \textit{Boosting}. Este método, al igual que los presentados en la Sección \ref{subsec:rf}, consiste en la combinación de la salida de varios estimadores con el fin de producir un estimador más preciso.

Sin embargo, el mismo difiere con los métodos anteriores en la forma de realizar este proceso. Mientras que en \textit{Random Forest} (\textit{Bagging}) se procede a construir varios árboles mediante diferentes conjuntos de entrenamiento y combinando la predicción de cada uno, en \textit{Boosting} se trabaja mediante un enfoque secuencial. (James, 2013) (\cite{james2013introduction})

En este método, se construye una sucesión de estimadores, los cuales surgen de forma iterativa usando una modificación del conjunto de datos realizada a partir de la performance del estimador en el paso anterior. De esta forma, en cada iteración, se toma como variable de salida los residuos del modelo anterior y no a la variable de respuesta original ($Y$). 

Esto último, con el fin de realizar un proceso de actualización de los residuos del modelo y por lo tanto mejorando la predicción del estimador en áreas donde el mismo no realiza un buen ajuste. (Hastie, 2001) (\cite{hastie2001tibshirani})

Generalmente, en \textit{Boosting} cada árbol suele estar conformado por pocas particiones, por lo que el procedimiento de aprendizaje suele ser "lento". (James, 2013) (\cite{james2013introduction})

De forma resumida, el álgoritmo consiste en aplicar los siguientes pasos de forma iterativa:

1. Se establece $\hat{f}(X) = 0$ y $r = Y$ en el conjunto de entrenamiento.

2. Para cada $v = 1, 2, \cdots, V$ se repite:

a. Se ajusta un árbol $\hat{f}^v(X)$ con $d$ particiones (es decir, $d+1$ nodos terminales) en los datos de entrenamiento (X,r).

b. Se actualiza $\hat{f}$ agregando el nuevo árbol en una versión reducida:

$$\hat{f}(X) \leftarrow \hat{f}(X) + \lambda \hat{f}^v(X)$$

c. Se actualizan los residuos 

$$r \leftarrow r - \lambda \hat{f}^v(X) $$

3. Se genera el modelo

$$\hat{f}(X) =  \displaystyle \sum_{v = 1}^V \lambda \hat{f}^v(X)$$

Donde $V$ denota el número de árboles utilizados en el algortimo. Se destaca que, a diferencia de \textit{Random Forest}, \textit{Boosting} puede generar un sobreajuste a los datos en caso que $V$ sea grande, a pesar de que éste sobreajuste ocurra de manera lenta. (James, 2013) (\cite{james2013introduction})

Por otro lado, el parámetro $\lambda$ controla la tasa a la que aprende el algoritmo. Donde $\lambda$ suele ser un número positivo pequeño, usualmente 0.01 o 0.001. A su vez, se tiene que generalemente, cuanto menor el valor de $\lambda$, mayor el número de arboles ($V$) necesarios. (Hastie, 2001) (\cite{hastie2001tibshirani})

Por último, la letra $d$ denota el número de particiones en cada árbol, dicho parámetro controla la complejidad de cada estimador. Se observa que en el caso $d=1$ implica que cada árbol tenga solamente una partición y de esta forma se cuenta con un modelo aditivo (cada término involucra una sola variable). 

A su vez, el parámetro $d$ puede ser intepretado también como el parámetro que controla el orden de interacción entre los modelos, ya que las $d$ particiones pueden involucrar a los sumo $d$ variables. (James, 2013) (\cite{james2013introduction})

Dichos parámetros ($V$, $\lambda$ y $d$) se estiman mediante una metodología de \textit{validación cruzada}.

\subsection{Support Vector Regression (SVR)\label{subsec:svr}}

Los modelos denominados \textit{Support Vector Regression} (SVR), surgen como una generalización aplicada a problemas de regresión de los modelos \textit{Support Vector Machine} (SVM). (Kunh, 2013) (\cite{kuhn2013applied}) 

Por lo tanto, al ser una generalización de los SVM (para problemas de regresión), poseen características muy similares, principalmente la robustez en cuanto a observaciones atípicas. De esta forma, se tiene que los SVR pertenecen al grupo denominado \textit{robust regression}, donde en estos métodos se busca minimizar el efecto de observaciones atípicas en la ecuación de regresión. (Kunh, 2013) (\cite{kuhn2013applied})

Estos métodos surgen como alternativa a los modelos de regresión lineal, ya que estos ultimos a la hora de estimar los parámetros buscan minimizar la suma de cuadrados residuales (SCR). Lo cual conlleva que una observación que no sigue la tendencia del resto, puede ser muy influyente. (Kunh, 2013) (\cite{kuhn2013applied})

A pesar de que existen varios enfoques para llevar acabo SVR en este trabajo se centró en el denominado $\epsilon$-\textit{insensitive regression} (Kunh, 2013) (\cite{kuhn2013applied}). En este contexto, a la hora de obtener las estimaciones de los parámetros del modelo, se define una nueva función de pérdida denominada $\epsilon$-\textit{insensitive loss function} (Vapnik, 1999) (\cite{vapnik1999nature}), siendo de la forma:

\begin{center}

$L(Y,f(X,\beta))=L(|Y-f(X,\beta)|_{\epsilon})$ 

\end{center}


\begin{center}

$|Y-f(X,\beta)|_{\epsilon} = \begin{cases} 0, & \mbox{si } |Y-f(X,\beta)| \leq \epsilon \mbox{} \\ |Y-f(X,\beta)|-\epsilon, & \mbox{en otro caso } \mbox{} \end{cases}$

\end{center}

En función a la ecuación anterior, se tiene que la perdida es igual a 0 si la discrepancia entre los predicho y lo observado es menor a $\epsilon$, siendo $\epsilon$ un umbral establecido en forma previa. Por lo tanto se tiene que tanto las observaciones atípicas, como las observaciones que poseen un buen ajuste (residuos pequeños), no tienen efecto en la ecuación de regresión. 

En este contexto, para estimar los parámetros del modelo, SVR utiliza la función de perdida anteriormente definida, pero a su vez considerando un parámetro de penalización. En dicho método se busca obtener los coeficientes que minimizan la siguiente expresión:

\begin{center}

$C \displaystyle \sum_{i=1}^{n}L(|y_i-f(x_i,\beta)|_{\epsilon}) + \displaystyle \sum_{j=1}^{P} \beta^{2}_{j}$


\end{center}

Donde el parámetro $C$ es un parámetro de penalización, el cual generalmente se estima mediante \textit{validación cruzada}. En este contexto el parámetro $C$ cumple un rol de indicar la complejidad del modelo. Conforme aumenta el valor de éste el modelo obtiene mayor flexibilidad, en la medida que el efecto de los errores es aumentado. Por otro lado, al disminuir este parámetro el modelo se vuelve más rígido y con menor posibilidad de sobre ajustar a las observaciones.

Luego, se tiene que la solución al problema de minimización anteriormente mencionado, involucra el producto escalar entre las observaciones y no a las observaciones en si (Hastie, 2001) (\cite{hastie2001tibshirani}). De esta forma, se puede re expresar a la función de regresión mediante la siguiente expresión:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} \langle x^{*},x_{i}\rangle$ 

\end{center}

De esta forma, se tiene que para evaluar $f(x^{*})$ es necesario el cálculo del producto escalar entre la nueva observación ($x^{*}$) y cada una de las observaciones pertenecientes al conjunto de entrenamiento. A su vez, se cuenta con $n$ parámetros $\alpha_{i}$ con $i=1,\dots,n$, donde cada uno corresponde a una observación de entrenamiento.

Sin embargo, en SVR se tiene la propiedad de que solo un subconjunto de los datos tiene un rol activo en la predicción de una nueva observación. Esto en la medida de que los parámetros $\alpha_{i}$ asociados a las observaciones de entrenamiento las cuales se encuentran a $\pm\ \epsilon$ de la recta de regresión (es decir se encuentran dentro del intervalo de longitud $2 \epsilon$ alrededor de la recta de regresión) son iguales a $0$. (Kunh, 2013) (\cite{kuhn2013applied})

A las observaciones las cuales determinan a la recta de regresión se les denomina \textit{support vectors}. Además, en la medida de que el predictor se encuentra sujeto al producto escalar entre la nueva observación y las observaciones de entrenamiento (en particular solo aquellas que sean \textit{support vectors}), se puede generalizar con el fin de captar relaciones no lineales entre las variables. 

Para ello se utiliza una función denominada \textit{kernel} la cual permite agrandar el espacio original de las variables, con el fin de obtener relaciones lineales en un nuevo espacio de mayor dimensión (James, 2013) (\cite{james2013introduction}). Esta función es una generalización del producto escalar y se denota de la siguiente forma:


\begin{center}

$K(x_{i},x_{j})$

\end{center}

De esta forma el predictor queda expresado como:

\begin{center}

$f(x^{*})=\beta_{0}+\displaystyle \sum_{i=1}^{n} \alpha_{i} K(x^{*},x_{i})$ 

\end{center}

A la hora de aplicar SVR existen diferentes \textit{kernels} lo cuales se podrían utilizar. Uno de lo más utilizados en la bibliografía se denomina \textit{radial kernel}. Este es de la forma:

\begin{center}

$K(x^{*},x_{i})=exp \left(-\gamma \displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}  \right)$, $\gamma>0$ 

\end{center}

En donde si la observación $x^{*}$ se encuentra lejos de la observación $x_{i}$ en terminos de distancia euclidia, entonces se tiene que $\displaystyle \sum_{j=1}^{p} (x^{*}_{j}-x_{ij})^{2}$ es una cantidad grande y por consecuente $K(x^{*},x_{i})$ es pequeño. Por lo tanto, $x_{i}$ no va a tener un rol activo a la hora de predecir el valor de $x^{*}$.

Esto significa que el \textit{radial kernel} posee un comportamiento local, en el sentido de que las observaciones de entrenamiento cercanas tienen un mayor efecto en la predicción del valor de una nueva observación.

Por otro lado, se tiene que $\gamma$ es un parámetro de escala, el cual afecta la varianza en la estimación. Al igual que $C$, dicho parámetro generalmente se estima mediante \textit{validación cruzada}.

Luego, se destacan dos aspectos de los modelos SVR. En primer lugar, en el caso de que la relación entre las variables sea realmente lineal (problemas de regresión), se recomienda usar un \textit{linear kernel} (producto escalar) sobre un \textit{radial kernel}. (Kunh, 2013) (\cite{kuhn2013applied})
 
A su vez, en la medida de la ecuación de regresión ($f(X)$) se expresa a través del producto escalar entre las observaciones, se recomienda estandarizar las mismas con el fin de tener una misma unidad de medida. (Kunh, 2013) (\cite{kuhn2013applied})


\section{Validación cruzada y parámetros de ajuste \label{sec:cv}}

A la hora de evaluar la performance de los diferentes modelos planteados, se realizó un procedimiento de \textit{validación cruzada}, particularmente \textit{k-folds}.

El algoritmo consiste en dividir la muestra en $k$ submuestras de igual tamaño. Luego $k-1$ submuestras se usan como datos de entrenamiento y la muestra restante $k$ se usa para testear los datos.

A continuación, se procede a ajustar los datos de esa muestra con el modelo construido con las $k-1$ muestras. Donde el proceso se repite $k$ veces, con cada una de las $k$ muestras. De tal forma que cada una de las $k$ muestras es utilizada una sola vez como datos de testeo. (James, 2013) (\cite{james2013introduction})


De esta forma, todas las observaciones se usan tanto para entrenar (\textit{train}) como para testear (\textit{test}). A su vez, cada observación se usa para \textit{test} una sola vez y para \textit{train} $k-1$ veces. Los errores obtenidos en cada etapa se promedian para producir una sola estimación (error medio obtenido de los $k$ análisis realizados).

Con el fin de medir el error de predicción del modelo en los modelos planteados anteriormente se consideró la siguiente medida:

\begin{center}

$RMSE= \displaystyle \frac{1}{k} \sum_{i=1}^{k} RMSE_{i}$ 

\end{center}

Donde $RMSE_{i}$ es la raiz del error cuadratico medio en la i-ésima muestra.

\begin{center}

$RMSE_i= \displaystyle \sqrt{ \frac{1}{n_i} \sum_{j = 1}^{n_i} (y_j - \hat{y}_j)^{2}}$

\end{center}

Siendo $n_i$ es la cantidad de observaciones en la i-ésima muestra. 

Asimismo, como medida de la performance predictiva se utilizó también el error absoluto medio:

\begin{center}

$MAE= \displaystyle \frac{1}{k} \sum_{i=1}^{k} MAE_{i}$

\end{center}

Donde $MAE_{i}$ es el error absoluto medio en la i-ésima muestra.

\begin{center}

$MAE_i= \displaystyle \frac{1}{n_i} \sum_{j = 1}^{n_i} (|y_j - \hat{y}_j|)$

\end{center}

Nuevamente, siendo $n_i$ la cantidad de observaciones en la i-ésima muestra. 

Por otro lado, con el fin de obtener el modelo con la mejor performance predictiva, se realizó un proceso de selección de los \textit{parámetros de ajuste}. El mismo consiste en obtener la mejor combinación de parámetros de ajuste posibles mediante una metodología de \textit{validación cruzada}.

En este contexto, se define a un \textit{parámetro de ajuste} como un valor necesario para ajustar un modelo el cual no se determina a partir de los datos sino que, por el contrario, es necesario que sea especificado previamente a la realización del ajuste. Dependiendo del algoritmo con el que se trabaje, el rol de los mismos puede variar. 

\section{Interpretabilidad \label{sec:interp}}

Una vez implementados los diferentes algoritmos de aprendizaje estadístico y ajustados los \textit{parámetros de ajuste}, se procedió a explorar métodos para interpretar los resultados de los modelos de aprendizaje estadístico utilizados.

Para ello, se trabajó con \textit{métodos globales modelo-agnósticos} de interpretación, aplicados a los modelos de aprendizaje estadístico y principalmente a aquel modelo con mejor performance predictiva, haciendo hincapié en el análisis gráfico.

Estos métodos de interpretación para modelos de caja negra consisten en describir el compartamiento promedio del modelo. Donde, los mismos generalmente se expresan mediante un valor esperado, basado en la distribución de los datos. (Molnar, 2020) (\cite{molnar2020interpretable})

A pesar de que en la literatura existen diferentes métodos para llevar a cabo el análsis, se trabajó mediante dos aproximaciones. 

Como primera aproximación, se implementó un análisis sobre la importancia de cada predictor, mediante la metodología de \textit{importancia de las variables permutadas}. Luego, se realizó el análisis mediante los gráficos denominados \textit{Partial Dependece Plot} (PDP). 

En las siguientes subsecciones se detallan los principales aspectos teóricos de las metodologías, al igual que sus ventajas y limitantes.

\subsection{Importancia de las variables permutadas \label{subsec:import}}

La metodología de \textit{importancia de las variables permutadas} consiste en obtener una medida de importancia de determinada variable de entrada, calculando el aumento en el error de predicción obtenido al permutar los valores en dicha variable.

De esta forma, se tiene que si la variable es relevante, al permutar los valores de la misma de forma aleatoria en el conjunto de entranamiento, se genera un aumento en el error de predicción (esto debido a que al permutar los valores de la variable se deshace cualquier relación entre la variable y la variable de respuesta) (Greenwell, 2020) (\cite{greenwell2020variable})

A la hora de llevar acabo esta metodología, en primer lugar se computa una métrica de la performance predictiva del modelo obtenida sin alterar el conjunto de entrenamiento. Luego, se computa dicha métrica permutando para cierta variable de interés, sus valores en el conjunto de entrenamiento. Esto con el fin de observar la diferencia entre el valor original de la métrica y el nuevo valor obtenido.

A modo de resumen, si denotamos $X_{1},\dots,X_{j}$ como las variables de interés, entonces el algortimo consiste en:

1) Para cada variable de interés $X_i$ con $i = 1,\dots,j$:

a) Se permuta los valores de la variable $X_{i}$ en el conjunto de entranamiento

b) Se calcula la performance predicitva mediante una métrica predefinida en a). 

c) Se calcula la diferencia entre la medida de error original y la obtenida en b)

2) Se ordena a cada variable de forma decreciente en función de c)
 
Por ende, se tiene que la variable es relevante si al permutar sus valores, aumenta el error de predicción del modelo. Por su contraparte, una variable no es relevante, si al permutar sus valores el error del modelo permanece incambiado. (Greenwell, 2020) (\cite{greenwell2020variable})

Se destaca que si se cuenta con variables altamente correlacionadas, este método posee una gran limitante. En la medida de que al permutar los valores de una sola variable, pueden generarse observaciones las cuales no son factibles. (Greenwell, 2020) (\cite{greenwell2020variable})


\subsection{Gráficos de dependencia parcial (PDP) \label{subsec:pdp}}

Los \textit{gráficos de dependencia parcial} (PDP) permiten observar el efecto marginal que una o dos variables tienen sobre la predicción de la variable de respuesta obtenida a través de un algoritmo de aprendizaje estadístico. (Molnar, 2020) (\cite{molnar2020interpretable}) 

Estos gráficos pueden detectar cuando la relación entre la variable de respuesta y la variable predictora de interés es líneal, monótona, o bien cuando se trata de una forma funcional más compleja. 

La \textit{función de dependencia parcial} para el caso de regresión se define como:

$$\begin{align*}\hat{f}_{X_S,PDP}(X_S)&=E_{X_C}\left[\hat{f}(X_S,X_C)\right]\\&=\int_{X_C}\hat{f}(X_S,X_C)\mathbb{P}(X_C)d{}X_C\end{align*}$$


En donde $X_s$ son las variables para las cuales se quiere conocer el efecto sobre la predicción, mientras que $X_C$ corresponde al resto de las variables utilizadas en el algoritmo de aprendizaje estadístico $\hat{f}$.

Ahora bien, la estimación para la función anterior se obtiene mediante la métodología \textit{Monte Carlo} promediando sobre la muestra de entrenamiento:

$$\hat{f}_{X_S, PDP}(X_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$

En donde $x_C^{(i)}$ son los valores en la base de datos para las variables que no son de interés y $n$ el número de observaciones.

Se destaca que PDP es un método que permite determinar de manera global la relación entre una o dos variables predictoras sobre la variable de respuesta. Asimismo, interesa destacar que uno de los supuestos de PDP es que las variables en $X_C$ y $X_S$ no están correlacionadas. (Molnar, 2020) (\cite{molnar2020interpretable})

De manera similar se obtiene una estimación PDP para el caso de variables categóricas en donde, para cada categoría, se realiza la estimación PDP forzando a que todas las observaciones tomen el valor correspondiente a dicha categoría. (Molnar, 2020) (\cite{molnar2020interpretable})

Una de las ventajas principales de PDP es que es un método intuitivo y en el caso de variables incorrelacionadas, tiene una interpretación clara. El gráfico PDP permite observar cómo cambia la predicción promedio cuando la h-ésima variable predictora cambia. No obstante éste resultado no es tan claro en el caso de correlación entre las variables con las que se construye el gráfico, ya que se construyen conbinaciones de las variables que son irreales o con probabilidad muy baja. Esto ocurrre ya que la función PDP en un punto en particular se obtiene forzando a que todos las observaciones tomen dicho valor en particular, lo cual puede no tener sentido en algunos casos. (Molnar, 2020) (\cite{molnar2020interpretable})

Por último, se destaca que, para el caso de las variables de naturaleza contínua, debido al alto costo computacional que conlleva la realización de esta metodología, a la hora de obtener los resultados se procedió a trabajar con una grilla de tamaño 20 sobre el recorrido de cada variable utilizando puntos de corte equidistantes.

\section{Tratamiento de datos faltantes \label{sec:secfalt}}

La base de datos construida contiene variables con diferentes grado de datos faltantes. 

Éste problema fue abordado siguiendo dos estrategias de imputación. En primera instancia se entrenan diferentes modelos imputando solamente a las variables numéricas que tienen proporción de valores faltantes inferior a 0.15, y se procede en esta primera instancia a realizar imputación por la media, lo cual es equivalente a asumir que el proceso generador de datos de éstos valores es un proceso aleatorio. Es decir, que los datos faltantes son generados al azar.

Posteriormente, se realizó un proceso de imputación de valores faltantes mediante un análisis supervisado. Para ello se trabajó de la siguiente manera: 1) Se ajustó un modelo tomando como variable de salida cada variable con datos faltantes, 2) en cada uno de ellos, se consideró como variables de entrada todas aquellas que originalmente no presentan datos faltantes, pero excluyendo la variable precio y 3) en cada variable, se sustituyó cada dato faltante por su predicción utilizando el modelo ajustado.

En particular, el algoritmo utilizado para ajustar los modelos fue \textit{Random Forest}, mediante el paquete \textit{missRanger} (\cite{missR}). En lo que respecta a los \textit{parámetros de ajuste}, se destaca que debido al tiempo computacional que conlleva fueron utilizados en todos los casos los valores por defecto.

Como fue mencionado anteriormente el criterio genérico para seleccionar las variables a imputar fue según la proporción de valores faltantes (proporción inferior a 0.15 para las variables de tipo numéricas). Sin embargo, esta metodología de imputación, a diferencia de la anterior, permite realizar el proceso para variables de tipo cualitativas. De esta forma, se incluyó en el análisis la variable condición que indica si el apartamento es nuevo o usado. No obstante, si bien existen otras variables que podrían ser incluídas, se decidió dejarlas fuera del análisis en la medida que no se consideran relevantes para el mismo (principalmente por presentar grupos no balancedos).

En la Tabla \ref{tabna} de la Sección del Anexo \ref{varsA} se detalla la proporción de valores faltantes en las variables utilizadas a la hora de ajustar los modelos. Por otra parte, en la Tabla \ref{tab10b} de la Sección del Anexo \ref{varsA} se presenta el listado de variables con el o los métodos de imputación de valores faltantes implementado.

\chapter{Reproducibilidad \label{cap:reproduc}}

En éste Capítulo se detalla brevemente las características tenidas en cuenta para lograr algunos aspectos de la reproducibilidad de los resultados obtenidos al igual que modificaciones de los mismos.

En primer lugar, en lo que respecta a los datos utilizados, se destaca que los mismos se encuentran disponibles en \url{https://drive.google.com/drive/folders/1uTIr9JVc5SZS2bOVG4faexfW0q4m9sVL?usp=sharing}. Asimismo, se trabajó con un repositorio remoto público disponible en \url{https://github.com/alvarovalinio/TFG}, donde se encuentra disponible el código necesario para la obtención de nuevos datos siguiendo los pasos detallados en la Sección \ref{sec:obtencion}. 

A su vez, en dicho repositorio se encuentra disponible en diferentes scripts del lenguaje de programación \textit{R} (\cite{statsR}) el código necesario para realizar la implementación de todos los modelos estadísticos utilizados. 

Por otra lado, como se detalla en el Capítulo \ref{cap:MT} varias de las técnicas implementadas involucran un proceso de simulación de valores aleatorios. De esta forma se consideró apropiado trabajar con una única semilla.

Luego, según se menciona en el Capítulo \ref{cap:Intro}, con el fin de obtener un mayor poder de cómputo en los algoritmos implementados, se utilizó un enfoque desde la programación en paralelo. Se destaca que que éste enfoque presenta ciertas limitantes en cuanto a la reproducibilidad de los resultados obtenidos. En éste sentido, los resultados presentados en la Sección \ref{sec:hypt} fueron obtenidos utilizando un total de dos núcleos y estableciendo una semilla en cada uno de ellos. Con el fin de llevar a cabo este cometido se trabajó con el paquete de R \textit{doParallel} (\cite{dopar}). 

Por último, el informe se realizó utilizando el sistema \textit{Sweave} que permite generar reportes dinámicos incorporando código de \textit{R} con documentos de Latex. El mismo se encuentra también disponible en el repositorio remoto público \url{https://github.com/alvarovalinio/TFG}. 

\chapter{Análisis exploratorio de  datos \label{cap:EDA}}

En este Capítulo se presenta los principales resultados obtenidos a la hora de realizar el análisis exploratorio de los datos, luego de realizar el proceso de depuración de los mismos detallado en la Sección \ref{sec:procesamiento} . De esta forma, la base de datos está conformada por un total de \Sexpr{paste(dim(aptos)[1])} observaciones y \Sexpr{paste(dim(aptos)[2])} variables. 

Por otra parte, una vez realizado el proceso de tratamiento de datos faltantes especificado en la Sección \ref{sec:secfalt} se mantiene un total de \Sexpr{paste(dim(aptos_sin_na)[2])} variables en el caso de imputación de valores faltantes por la media, y \Sexpr{paste(dim(aptos_mr)[2])} variables en el caso de imputación de valores faltantes por \textit{Random Forest}. 

En la Tabla \ref{tab10} disponible en la Sección del Anexo \ref{varsA} se detalla el listado de variables utilizadas para la implementación de los diferentes modelos en etapas posteriores del análisis. 

Una vez obtenida la base de datos luego de realizar los procesos mencionados anteriormente, se procedió a analizar el comportamiento de la variable de respuesta precio de oferta en dólares. A continuación se presenta en la Tabla \ref{tab1} las principales medidas de resumen: 

<<>>=
sum.table <- aptos %>% summarise(Min = round(min(price),0),
                    Q1=round(quantile(price,.25),0),
                    Mediana = round(median(price),0),
                    Media = round(mean(price),0),
                    Q3 = round(quantile(price,.75),0),
                    Max = round(max(price),0),
                    Desvio = round(sd(price),0),
                    CV = round(Desvio/Media,2))

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(sum.table, booktabs = F, align = "c", format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Medidas de resumen de la variable precio de oferta en dólares. En la medida que la media es superior a la mediana se tiene una asimetría hacia la derecha. Por otro lado, se observa un desvío de 76.788 dólares estadounidenses y un coeficiente de variación de 0.47.}
\label{tab1}
\end{table}

En la Figura \ref{fig1} se presenta gráficamente la distribución de la variable mediante un gráfico de histograma.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
aptos %>% ggplot(aes(x=price)) + 
       geom_histogram(fill = '#3D56B2', alpha = 0.9, bins = 40) +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_text(face = 'bold', size = 12),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_text(face = 'bold', size = 12),
            axis.text.y = element_text(size = 12)) +
      labs(x = 'Precio de oferta en dólares', y = 'Frecuencia') +
      scale_fill_manual(values = c('#3D56B2')) +
      geom_vline(xintercept = mean(aptos$price, na.rm = TRUE), 
                 colour = 'red', linetype = 'dashed') +
      annotate("text", x = mean(aptos$price, na.rm = TRUE) + 72000, y = 5500, label = paste0("Media = USD ",round(mean(aptos$price, na.rm = TRUE))), color = 'red') +
      scale_x_continuous(label = comma)
@
\captionof{figure}{Histograma del precio de oferta en dólares de los apartamentos a la venta en Montevideo, Uruguay. El precio promedio es USD \Sexpr{paste(round(mean(aptos$price, na.rm = TRUE)))}. Como fue comentado en la Tabla \ref{tab1} se observa una asimetría a la derecha en la distribución.}
\label{fig1}
\end{figure}

En lo que respecta a las variables de entrada, en primer lugar se realizó un análisis exploratorio en cuanto a las variables de naturaleza cualitativa. A continuación se presenta en la Figura \ref{fig2} la distribución de los niveles en los datos para un conjunto de variables mediante gráficos de barras.

<<>>=
# niveles <- aptos %>% group_by(bedrooms) %>% summarise(n = n())
# colores <- data.frame(colores= c("gold1","darkviolet","green4", "dodgerblue2", "firebrick"))
# niveles <- niveles %>% arrange(n)
# niveles$id <- 1:nrow(niveles)
# colores$id <- 1:nrow(niveles)
# niveles <- left_join(niveles,colores,by="id")
# 
# niveles <- niveles %>% arrange(bedrooms)

p1 <- aptos %>% ggplot() +
  geom_bar(aes(x = bedrooms, y = (..count..)/sum(..count..)), fill = '#3D56B2') +
  theme(axis.ticks.x = element_blank(),
        legend.position = 'none',
        axis.title.y = element_text(face = 'bold', size = 10),
        axis.title.x = element_text(face = 'bold', size = 10),
        axis.text.x = element_text(face = 'bold')) + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(x = as.factor(bedrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
  labs(x = 'Cantidad de dormitorios', y = 'Porcentaje') 

p2 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(full_bathrooms)), y = (..count..)/sum(..count..)), fill = '#3D56B2') +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(full_bathrooms),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Cantidad de baños completos') 

p3 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(zona_avditalia)), y = (..count..)/sum(..count..)), fill = '#3D56B2') +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_text(face = 'bold', size = 10),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(zona_avditalia),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'Zona Avd. Italia', y = 'Porcentaje') 

p4 <-aptos %>% ggplot() +
      geom_bar(aes(x = fct_infreq(as.factor(has_swimming_pool)), y = (..count..)/sum(..count..)),  fill = '#3D56B2') +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold', size = 10),
            axis.text.x = element_text(face = 'bold')) + 
      scale_y_continuous(labels = scales::percent) +
      geom_text(aes(x = as.factor(has_swimming_pool),label = scales::percent(round((..count..)/sum(..count..), 2)), 
                    y = (..count..)/sum(..count..)), stat = "count", vjust = -0.5, size = 2) + 
      labs(x = 'El edificio tiene piscina') 
@

\begin{figure}[H]
\centering
<<fig = TRUE, fig.pos = 'h'>>=
grid.arrange(p1, p2, p3, p4, ncol = 2)
@
\captionof{figure}{Gráfico de barras para diferentes variables cualitativas. En el panel superior izquierdo se encuentra graficada la variable cantidad de dormitorios mientras que en el panel superior derecho la variable cantidad de baños completos. Por otro lado, en el panel inferior izquierdo se encuentra la variable zona Avd. Italia mientras que en el panel inferior derecho la variable el edificio tiene piscina.}
\label{fig2}
\end{figure}

Según se observa en la Figura \ref{fig2} la variable zona Avd. Italia posee niveles balanceados en contraposición a las variables el edificio tiene piscina y cantidad de baños completos. Por su parte, la variable  cantidad de dormitorios presenta mayor proporción de observaciones hacia los niveles centrales (uno y dos dormitorios).

Con el fin de observar la covariación entre la variable precio de oferta en dólares y las variables presentadas en la Figura \ref{fig2}, se presentan en la Figura \ref{fig3} los gráficos de caja y gráficos de violín correspondientes.

<<>>=
p5 <- aptos %>% 
      ggplot(aes(x=as.character(zona_avditalia), y=price)) + 
      geom_violin(fill = '#3D56B2') +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text(face = 'bold', size = 10),
            axis.title.x = element_text(face = 'bold', size = 10),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      labs(y='Precio de oferta en dólares', x='Zona Avd. Italia' ) +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black', fill = '#3D56B2') +
      scale_y_continuous(labels = comma) 

p6 <- aptos %>%
      filter(!is.na(full_bathrooms)) %>%
      ggplot(aes(x=as.character(full_bathrooms), y=price)) + 
      geom_violin(fill = '#3D56B2') +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_text(face = 'bold', size = 10),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      labs(y='', x='Cantidad de baños completos' ) +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black', fill = '#3D56B2') +
      scale_y_continuous(labels = comma) 

p7 <- aptos %>%
      filter(!is.na(bedrooms)) %>%
      ggplot(aes(x=as.character(bedrooms), y=price)) + 
      geom_violin(fill = '#3D56B2') +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text(face = 'bold', size = 10),
            axis.title.x = element_text(face = 'bold', size = 10),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      labs(y='Precio de oferta en dólares', x='Cantidad de dormitorios') +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black', fill = '#3D56B2') +
      scale_y_continuous(labels = comma) 

p8 <- aptos %>%
      filter(!is.na(has_swimming_pool)) %>%
      ggplot(aes(x=as.character(has_swimming_pool), y=price)) + 
      geom_violin(fill = '#3D56B2') +
      theme(text = element_text( family = 'sans'),
            axis.title.y = element_text( size = 10),
            axis.title.x = element_text(face = 'bold', size = 10),
            plot.title = element_blank(),
            legend.title = element_blank(),
            axis.text.x = element_text(face = 'bold', size = 10),
            axis.text.y = element_text(face = 'bold', size = 10),
            legend.position = 'none') +
      scale_x_discrete() +
      labs(y='', x='El edificio tiene piscina' ) +
      geom_boxplot(width=0.1, colour='black', outlier.color = 'black', fill = '#3D56B2') +
      scale_y_continuous(labels = comma) 
@


\begin{figure}[H]
\centering
<<fig = TRUE>>=
grid.arrange(p7, p6, p5, p8, ncol =2)
@
\captionof{figure}{Gráfico de violín y gráfico de caja del precio de oferta en dólares según diferentes variables cualitativas. En el panel superior izquierdo se encuentra graficada la variable cantidad de dormitorios mientras que en el panel superior derecho la variable cantidad de baños completos. Por otro lado, en el panel inferior izquierdo se encuentra la variable zona Avd. Italia mientras que en el panel inferior derecho la variable el edificio tiene piscina.}
\label{fig3}
\end{figure}

Como se observa en la Figura \ref{fig3}, tanto la mediana como la dispersión del precio de oferta en dólares se incrementa conforme aumenta la cantidad de dormitorios del apartamento. Dicho comportamiento también se observa para la cantidad de baños completos.

Por otra parte, los apartamentos ubicados al sur de la calle Avenida Italia en continuación con la calle Avenida 18 de Julio presentan una mediana y dispersión superior del precio de oferta en dólares, respecto a los apartamentos ubicados al norte.

En lo que respecta a los apartamentos con piscina en el edificio, de manera similar presentan una mediana y dispersión superior del precio de oferta en dólares, respecto a los apartamentos que no tienen piscina en el edificio.

Por último, se procedió a realizar un análisis gráfico bivariado en cuanto a la variable precio de oferta en dólares. De esta forma, en la Figura \ref{fig4}, se presenta la distribución de la misma teniendo en cuenta los efectos de las variables zona Avd. Italia y cantidad de baños completos. Mientras que en la Figura \ref{fig5} de forma análoga, se observa los efectos de las variables distancia a la rambla este y cantidad de baños completos.

Se destaca que en lo que respecta a la variable distancia a la rambla este, se realizó la siguiente discretización para poder tener una aproximacón de su efecto sobre la variable precio de oferta en dólares:

- Inferior al primer cuantil: valores inferiores a \Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.25)))} (metros).

- Entre primer y segundo cuantil: para valores en el intervalo [\Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.25)))}, \Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.5)))}) (metros).

- Entre segundo y tercer cuantil: para valores en el intervalo [\Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.5)))}, \Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.75)))}) (metros).

- Igual o superior al tercer cuantil: valores iguales o superiores a \Sexpr{paste(round(quantile(aptos$dist_rambla, prob = 0.75)))} (metros).

\newpage

\begin{figure}[H]
\centering
<<fig = TRUE,width = 6, height=3>>=
aptos %>% ggplot(aes(y=price, group = full_bathrooms, fill = full_bathrooms)) + 
       geom_boxplot() +
      theme(axis.ticks.x = element_blank(),
            legend.position = 'none',
            axis.title.y = element_text(face = 'bold', size = 12),
            axis.title.x = element_text(face = 'bold', size = 12),
            axis.text.x = element_blank()) +
      labs(y = 'Precio de oferta en dólares') +
      scale_fill_manual(values = c('orangered2', 'springgreen4')) +
      facet_grid(full_bathrooms~zona_avditalia) +
      scale_y_continuous(label = comma)
@
\captionof{figure}{Gráfico de caja del precio de oferta en dólares según zona Avd. Italia y cantidad de baños completos. El gráfico sugiere que las diferencias en mediana observardas en la Figura \ref{fig3} se mantienen cuando condicionamos en cantidad de baños completos.}
\label{fig4}
\end{figure}

En particular, la Figura \ref{fig4} muestra que entre los apartamentos con dos o más baños completos, se observa una mediana superior para los apartamentos ubicados al sur respecto a los apartamentos ubicados al norte. En lo que respecta a los apartamentos con un sólo baño completo, se observa que los apartamentos al sur poseen una mediana superior respecto a los ubicados al norte, si bien la diferencia es menor en éste último caso.

En primer lugar, se observa en la Figura \ref{fig5} que el precio de oferta en dólares de los apartamentos se ve diferenciado según la cantidad de baños completos. Esto último en la medida que, condicionando por todos los niveles de la variable distancia a la rambla este discretizada, la mediana de los apartamentos con dos o más baños completos es superior a aquellos con un solo baño completo. 

Por otro lado, entre los apartamentos con dos o más baños completos, el precio se ve diferenciado si se considera la discretización de la variable distancia a la rambla este en función de los cuantiles de la misma. Esto último debido a que la mediana del precio de los apartamentos de los dos primeros niveles es mayor que la de los últimos.

\newpage

\begin{figure}[H]
\centering
<<fig = TRUE>>=
aptos %>% 
      mutate(dist_rambla = factor(case_when(dist_rambla < quantile(dist_rambla, prob = 0.25) ~ '1er cuantil',
                                            dist_rambla < quantile(dist_rambla, prob = 0.5) ~ 'Entre el 1er y  2do cuantil',
                                            dist_rambla < quantile(dist_rambla, prob = 0.75) ~ 'Entre el 2do y 3er cuantil',
                                          TRUE ~ 'Mayor al tercer cuantil'))) %>%
      ggplot(aes(y=price,x=dist_rambla,fill=full_bathrooms)) + 
      geom_boxplot() +      
      theme(axis.ticks.x = element_blank(),
            axis.title.y = element_text(face = 'bold'),
            axis.title.x = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45, size = 12),
            legend.title = element_text(face = 'bold'),
            legend.position = 'right') +
      labs(y = 'Precio de oferta en dólares', x = 'Distancia a la rambla este') +
      scale_fill_manual('Cantidad de baños \n completos', values = c('darkgreen', 'darkmagenta'))  +
      scale_y_continuous(label = comma) +
      scale_x_discrete(labels = c('1er cuantil','Entre el 1er y \n 2do cuantil','Entre el 2do y \n 3er cuantil','Mayor al tercer \n cuantil')) 
@
\captionof{figure}{Gráfico de caja del precio de oferta en dólares según distancia a la rambla este y cantidad de baños completos utilizando una discretización de la variable distancia a la rambla este mediante los cuantiles.}
\label{fig5}
\end{figure}

Por otro lado, en lo que respecta al análisis exploratorio para las variables cuantitativas, se procedió a analizar la correlación lineal entre las mismas utilizando el \textit{coeficiente de correlación lineal de Pearson}. Para ello, en la Figura \ref{figcor} se presenta a modo de resumen la matriz de correlación de manera gráfica.

En primer lugar, en cuanto a la variable precio de oferta en dólares se observa en la Figura \ref{figcor} que las variables área total, área cubierta y distancia a la rambla este se encuentran linealmente correlacionadas de forma moderada. Donde para las dos primeras esta relación es de carácter positivo mientras que para la última es de carácter negativo.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
aptos_cor <- aptos %>% select(price, dist_shop, dist_rambla, covered_area,
                              total_area, no_covered_area) 

corr<-round(cor(aptos_cor, use='pairwise.complete.obs') , 2)

ggcorrplot(corr, method = c("square"),type=c("upper"), 
           ggtheme = ggplot2::theme_gray,lab=TRUE) +
      ggplot2::scale_x_discrete(labels = c('Precio de oferta \n en dólares','Distancia al \n centro  comercial \n más cercano','Distancia a la \n rambla Este','Área \n total','Área \n cubierta','Área \n no cubierta')) +
      ggplot2::scale_y_discrete(labels = c('Distancia al \n centro  comercial \n más cercano', 'Distancia a la \n rambla Este',  'Área \n cubierta', 'Área \n total', 'Área \n no cubierta')) +
      ggplot2::theme(axis.text = element_text(vjust = 0.5)) +
      theme(axis.text = element_text(face = 'bold', size = 6),
            legend.title = element_text(face = 'bold')) 
@
\captionof{figure}{Gráfico de matriz de correlación de las variables precio de oferta en dólares, distancia al centro comercial más cercano, distancia a la rambla este, área total, área cubierta, y área no cubierta.} 
\label{figcor}
\end{figure}

Por otro lado, se tiene que tanto distancia al centro comercial más cercano como distancia a la rambla este, no se encuentran linealmente correlacionadas con área total, al igual que el área cubierta y área no cubierta. Por último, se observan correlaciones lineales positivas las cuales son producto de la construcción misma de las variables. Por ejemplo, la correlación lineal positiva entre área total y área cubierta.

\chapter{Resultados \label{cap:Resultados}}

En esta Sección se presentan los principales resultados obtenidos luego de realizar la implementación de las técnicas detalladas en el Capítulo \ref{cap:MT}.

En primer lugar, en la Sección \ref{sec:resml} se tienen los resultados obtenidos luego de relizar un ajuste mediante un \textit{Modelo de Regresión Lineal Múltiple}. Luego, en la secciones \ref{sec:resarbol}, \ref{sec:resRF}, \ref{sec:resboosting} y  \ref{sec:ressvr} se exponen los resultados de la implementación de las técnicas de aprendizaje estadístico explicitadas en el Capítulo \ref{cap:MT}.

Con el fin de tener una primera aproximación de la performance predictiva de los modelos, se trabajó con una metodología de muestra de entrenamiento y muestra de testeo. Para ello, se consideró el 80\% de los datos como muestra de entrenamiento y el restante 20\% como muestra de testeo. 

A continuación, en forma de resumen, se muestra en la Sección \ref{sec:hypt} un análisis comparativo de los diferentes modelos en función de su performance predictiva mediante la metodología de \textit{k-folds} y el proceso de selección de los \textit{parámetros de ajuste} especificada en la Sección \ref{sec:cv}. En particular, se trabajó con \textit{5-folds}.

Por último, se presenta en la Sección \ref{sec:resinterp} el análisis gráfico de interpretabilidad según detallado en la Sección \ref{sec:interp} aplicado al mejor modelo en términos de performance predictiva resultado de la etapa anterior.

Se destaca que todas las etapas anteriores se aplicaron considerando las dos técnicas de imputación mencionadas en la Sección \ref{sec:secfalt}. 

\section{Modelo de Regresión Lineal Múltiple \label{sec:resml}}

En esta sección se presentan los principales resultados obtenidos luego de realizar un ajuste mediante un \textit{Modelo de Regresión Lineal Múltiple}. 

En lo que respecta a las estimaciones de los coeficientes del modelo, en la Sección del Anexo \ref{mlA} se expone en las Tablas \ref{taba} y \ref{tabb} el resumen del ajuste para cada método de imputación de valores faltantes utilizado, con los coeficientes estimados, el error estándar de cada uno de ellos, el estadístico t de student y su p-valor asociado.

De esta forma, en función a lo explicitado en las Tablas \ref{taba} y \ref{tabb}, se tiene que, dado las demás variables constantes, el precio del apartamento disminuye a medida que aumenta la distancia a la rambla este. Esto en la medida de que la estimación del parámetro asociado a dicha variable se encuentra precedido por un signo negativo.

Por otro lado, se tiene que aquellos apartamentos con dos o más baños completos, dado las demás variables constantes, se caracterizan por presentar un precio mayor a aquellos con un solo baño completo. Esto debido a que la estimación del parámetro asociado al nivel de referencia (2 o más) de la variable cantidad de baños completos, se encuentra precedido de un signo positivo.

Por otra parte, en la Tabla \ref{tab2} se presentan los resultados globales del \textit{Modelo de Regresión Lineal Múltiple}.

Se destaca que el ajuste se realizó utilizando la función \textit{lm} del paquete \textit{stats} (\cite{statsR}).

<<>>=
lm <- lm(price ~ ., data = train)
RMSE_lm <- sqrt(mean((test$price - predict(lm,test))^2))

lm_mr <- lm(price ~ ., data = train_mr)
RMSE_lm_mr <- sqrt(mean((test_mr$price - predict(lm_mr,test_mr))^2))

lmt <- rbind(broom::glance(lm), broom::glance(lm_mr))
lmt <- lmt %>% select(-"sigma" , - "AIC", - "BIC",
                  - "deviance", -"df.residual", -"nobs", -"df", -"logLik", -"p.value")

lmt <- lmt %>% mutate(p.value = "< 2.2e-16",
                      r.squared = round(r.squared, 2),
                      adj.r.squared = round(adj.r.squared, 2),
                      statistic = round(statistic,0))

MAE_lm <- mean(abs(test$price-predict(lm,test)))
MAE_lm_mr <- mean(abs(test_mr$price-predict(lm_mr,test_mr)))


met <- tribble(
  ~Imput, ~RMSE, ~MAE,
  "Media", round(RMSE_lm), round(MAE_lm),
  "Random Forest", round(RMSE_lm_mr), round(MAE_lm_mr)
)

lmt <- cbind(met, lmt)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(lmt, escape = F, booktabs = F, align = "c", format.args = list(big.mark = ","), col.names = c('Imputación', 'RMSE', 'MAE', "$R^2$", "$R^2_a$","Estadístico F", "P-Valor" )) %>% kable_styling(latex_options = "HOLD_position", font_size = 10) %>% row_spec(0,bold=TRUE)
@
\caption{Principales resultados de los modelos lineales ajustados según el método de imputación utilizado, considerando muestra de entrenamiento y muestra de testeo.}
\label{tab2}
\end{table}

Según se observa en la Tabla \ref{tab2} los modelos lineales ajustados tienen un error de predicción de aproximadamente 43,000 y 30,000 dólares si se utiliza como medida de error a la raíz cuadrada del error cuadrático medio y error absoluto medio respectivamente. Asimismo, los modelos explican aproximadamente un 70 \% de la varianza en los datos y son globalmente significativos.

Por último, en lo que respecta al análisis de los residuos, en primer lugar se realiza un análisis gráfico de los mismos. En la Figura \ref{figerrorml} se presentan los errores en función de los valores ajustados y la distribución de los mismos considerando ambos métodos de imputación.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
lm1 <- ggplot(as_tibble(lm$residuals)) +
      geom_point(aes(x = lm$fitted.values, y = value), color = '#3D56B2', alpha = 1/7) +
      theme(aspect.ratio = 1,
            axis.title = element_text(face = 'bold',size = 12),
            axis.text.x = element_text(angle = 45, size = 8),
            axis.text.y = element_text(size = 8),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(x = 'Valores predichos', y = 'Errores') +
      scale_x_continuous(labels = comma)

mediana_reslm <- quantile(lm$residuals, probs = 0.5)
media_reslm <- mean(lm$residuals)

lm2 <- ggplot(as_tibble(lm$residuals)) + 
      geom_histogram(aes(value), fill = '#3D56B2') +
      theme(axis.title = element_text(face = 'bold',size = 12),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5),
            axis.text.x = element_text(angle = 45, size = 8),
            axis.text.y = element_text(size = 8)) +
      labs(y = 'Frecuencia', x = 'Errores') 

lm_mr3 <- ggplot(as_tibble(lm_mr$residuals)) +  
      geom_point(aes(x = lm_mr$fitted.values, y = value), color = '#3D56B2', alpha = 1/5) +
      theme(aspect.ratio = 1,
            axis.title = element_text(face = 'bold',size = 12),
            axis.text.x = element_text(angle = 45, size = 8),
            axis.text.y = element_text(size = 8),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(x = 'Valores predichos', y = 'Errores') +
      scale_x_continuous(labels = comma)

mediana_reslm_mr <- quantile(lm_mr$residuals, probs = 0.5)
media_reslm_mr <- mean(lm_mr$residuals)

lm_mr4 <- ggplot(as_tibble(lm_mr$residuals)) + 
      geom_histogram(aes(value), fill = '#3D56B2') +
      theme(axis.title = element_text(face = 'bold',size = 12),
           axis.text.x = element_text(angle = 45, size = 8),
            axis.text.y = element_text(size = 8),
            plot.title = element_text(face = 'bold', size = 14, hjust = 0.5)) +
      labs(y = 'Frecuencia', x = 'Errores')

#TeX("$\\hat{f}_{\\epsilon}$")

grid.arrange(lm1, lm2, lm_mr3, lm_mr4, ncol = 2)
@
\captionof{figure}{Gráficos de los residuos de los \textit{Modelos de Regresión Lineal Múltiple} ajustados. En el panel superior se encuentran los resultados para el caso de imputación de valores faltantes por la media, mientras que en el panel inferior se encuentran los resultados para el caso de imputación de valores faltantes por \textit{Random Forest}. A la izquierda se encuentra el gráfico de dispersión entre los errores del modelo y los valores predichos. A la derecha se encuentra el histograma de los mismos.}
\label{figerrorml}
\end{figure}

El gráfico de dispersión de la Figura \ref{figerrorml} sugiere heterocedasticidad en los residuos de los modelos ajustados. Por otra parte, el histograma de los residuos muestra que los mismos se encuentran en torno al valor 0.

Una vez realizado el análisis gráfico se procedió a realizar las pruebas sobre los supuestos de los residuos del modelo mencionadas en la Sección \ref{subsec:mlMT}. Considerando ambos métodos de imputación y un nivel de significación del 0.05, se procedió a rechazar tanto el supuesto de normalidad como el supuesto de homocedasticidad de los residuos en la medida que se rechazan las hipótesis nulas de ambas pruebas. Esto último teniendo en cuenta que en ambos casos el p-valor asociado a la prueba es inferior al nivel de significación considerado. 

En las Tablas \ref{tab13} y \ref{tab14} de la Sección del Anexo \ref{mlA} se encuentran los resultados de las pruebas aplicadas.

\section{Árbol de regresión \label{sec:resarbol}}

A continuación se presentan los resultados del ajuste mediante un \textit{Árbol de regresión}
considerando ambas metodologías de imputación de valores faltantes. 

En primer lugar se presenta, para cada modelo ajustado, el gráfico luego de realizar el proceso de
poda mencionado en la Sección \ref{subsec:arbol}. Respecto a éste punto, en la Sección del Anexo \ref{arbolA} se
presenta en las Tablas \ref{tab17} y \ref{tab18} los diferentes valores del parámetro de costo-complejidad, su error mediante un proceso de \textit{validación cruzada} asociado, al igual que el
tamaño del árbol que dicho valor del parámetro implica y otras métricas de interés.

Se destaca que el ajuste se realizó utilizando la función \textit{rpart} del paquete \textit{rpart}
(\cite{rpart}).

Según se observa en la Figura \ref{figarb} el árbol se conforma por \Sexpr{npart+1} nodos terminales
(hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se
encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart} particiones.

Asimismo, se tiene que la primera variable en realizar una partición binaria es la variable cantidad
de baños completos. Las variables distancia a la rambla este, área total, cantidad de dormitorios e
ingreso medio ECH son utilizadas en las siguientes particiones.

De esta forma, se tiene que el precio de un apartamento con más de un baño completo, que se encuentra a menos de 2,039 metros de distancia a la rambla este y con un área total mayor o igual a 107 metros cuadrados, es predicho por el modelo como 351,524 dólares.

Por su contraparte, un apartamento con un solo baño completo, con un área total menor a 55.2 metros cuadrados y que se encuentra a una distancia mayor o igual de 1,462 metros a la rambla este, el modelo realiza una predicción sobre el precio del mismo de 107,403 dólares.

Por otra parte, el gráfico correspondiente al árbol realizado mediante imputación de valores faltantes por \textit{Random Forest} es presentado en la Figura \ref{figarbol} de la Sección del Anexo \ref{arbolA} debido a que no presenta diferencias sustanciales con el gráfico del árbol obtenido realizando imputación por la media.

\newpage

\begin{figure}[H]
\centering
<<fig = TRUE>>=
rpart.plot(arbol.prune,roundint = T,digits = -3)
@
\captionof{figure}{\textit{Árbol de regresión} obtenido al ajustar la variable precio de oferta en
dólares una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt}. El método de
imputación de valores faltantes utilizado en este caso es imputación por la media.}
\label{figarb}
\end{figure}

\newpage

Una vez observada las principales características de los modelos, se procedió a obtener cierta medida de la performance predictiva en función de las métricas definidas en la Sección \ref{sec:cv}. Las mismas se presentan a continuación en la Tabla \ref{tab3}. 

<<>>=
# RMSE test
RMSE_arbol <- sqrt(mean((test$price-predict(arbol.prune,test))^2))
RMSE_arbol_mr <- sqrt(mean((test_mr$price-predict(arbol.prune.mr,test_mr))^2))
 
r2_arbol <- sum((predict(arbol.prune,test)-mean(test$price))^2) /
                        sum((test$price - mean(test$price))^2)

r2_arbol_mr <- sum((predict(arbol.prune.mr,test_mr)-mean(test_mr$price))^2) /
                        sum((test_mr$price - mean(test_mr$price))^2)

mae_arbol <- mean(abs((test$price-predict(arbol.prune,test))))
mae_arbol_mr <- mean(abs((test_mr$price-predict(arbol.prune.mr,test_mr))))

res_arbol <- tribble(
  ~Imput, ~RMSE, ~R2, ~MAE, 
  "Media",   round(RMSE_arbol), round(r2_arbol, 2), round(mae_arbol),
  "Random Forest", round(RMSE_arbol_mr), round(r2_arbol_mr, 2), round(mae_arbol_mr)
)
library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(res_arbol, booktabs = F, align = "c", col.names = c('Imputación', 'RMSE', "$R^2$", "MAE"), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 12) %>% row_spec(0,bold=TRUE)
@
\caption{Principales medidas de resumen de los árboles de regresión ajustados según metodología de imputación utilizada, considerando muestra de entrenamiento y muestra de testeo. Se presentan la raíz cuadrada del error cuadrático medio (RMSE), el coeficiente de determinación ($R^2$), y el error absoluto medio (MAE).}
\label{tab3}
\end{table}

Como se observa en la Tabla \ref{tab3}, ambos modelos tienen un error promedio de predicción de aproximadamente 43,000 y 30,000 dólares si se utiliza como medida de error a la raíz cuadrada del error cuadrático medio y el error absoluto medio respectivamente. Asimismo, los modelos explican aproximadamente un 70 \%  de la varianza en los datos.

\section{Random Forest \label{sec:resRF}}

En esta sección se presentan los resultados del ajuste por \textit{Random Forest} considerando ambas metodologías de imputación. 

Se destaca que el ajuste se realizó utilizando la función \textit{ranger} del paquete \textit{ranger} (\cite{ranger}) y tomando los valores por defecto de los parámetros de ajuste detallados en la Sección \ref{subsec:rf} para ambos métodos de imputación. 

De esta manera, en primer lugar se presenta en la Tabla \ref{hyprf} los valores utilizados. 

<<>>=

hyp_rf <- tribble(
  ~Hyperparametro, ~Valor,
  "Número de árboles",  as.character(round(rf_train$num.trees)), 
  "Cantidad de variables",  as.character(rf_train$mtry)
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(hyp_rf, booktabs = F, align = "c", col.names = c('parámetro de ajuste ', 'Valor'), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 12) %>% row_spec(0,bold=TRUE)
@
\caption{Valores por defecto de los \textit{parámetros de ajuste} utilizados en los modelos ajustados por \textit{Random Forest}. Se detalla el número de árboles y la cantidad de variables seleccionadas de manera aleatoria en cada partición.}
\label{hyprf}
\end{table}

Luego se presenta graficamente en la Figura \ref{rf_import2} un análisis de la importancia permutada de las variables siguiendo la metodología detallada en la Sección \ref{subsec:import}. 

\begin{figure}[H]
\centering
<<fig = TRUE, width=6, height=3>>=
############ RF 

p1 <- importancia_rf$data %>% 
      ggplot(aes(y=reorder(Variable,Importance), x=Importance)) +
      geom_col(fill='#3D56B2') +
      theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')) +
      labs(x="Importancia") +
      scale_y_discrete(labels = c('Tiene terraza','Zona Avd. Italia','Área no cubierta',
                                  'Distancia al centro \n comercial más cercano',
                                  'Cantidad de dormitorios',
                                  'Ingreso medio ECH','Cantidad de baños \n completos',
                                  'Distancia a la rambla este','Área cubierta', 'Área total'))


p2 <- importancia_rf_mr$data %>% 
      ggplot(aes(y=reorder(Variable,Importance), x=Importance)) +
      geom_col(fill='#3D56B2') +
      theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')) +
      labs(x="Importancia") +
      scale_y_discrete(labels = c('Condición','Zona Avd. Italia','Área no cubierta',
                                  'Distancia al centro \n comercial más cercano',
                                  'Cantidad de dormitorios',
                                  'Ingreso medio ECH','Cantidad de baños \n completos',
                                  'Distancia a la rambla este','Área total', 'Área cubierta'))



# Importancia de las variables
grid.arrange(p1, p2, ncol = 2)
@
\captionof{figure}{Gráfico de la importancia permutada de las 10 primeras variables en los modelos ajustados por \textit{Random Forest}. Se observa que las variables área cubierta, área total y distancia a la rambla este son las 3 variables más importantes en ambos modelos.}
\label{rf_import2}
\end{figure}

Según se observa en la Figura \ref{rf_import2} todas las variables geoespaciales construidas se encuentran entre las 10 variables más importantes para ambos casos. Por otro lado se destaca que, a la hora de realizar imputación de valores faltantes por \textit{Random Forest}, la variable condición se encuentra entre las 10 variables más importantes.

Luego se procedió a obtener cierta medida de la performance predictiva en función de las métricas definidas en la Sección \ref{sec:cv}. Las mismas se presentan a continuación en la Tabla \ref{tab4}.

<<>>=
# RMSE test
predictions_rf <- predictions(predict(rf_train,test))
predictions_rf_mr  <- predictions(predict(rf_train_mr,test_mr))

RMSE_rf <- sqrt(mean((test$price-predictions_rf)^2))
RMSE_rf_mr <- sqrt(mean((test_mr$price - predictions_rf_mr)^2))

r2_rf <- sum((predictions_rf-mean(test$price))^2) /
      sum((test$price - mean(test$price))^2)

r2_rf_mr <- sum((predictions_rf_mr-mean(test_mr$price))^2) /
      sum((test_mr$price - mean(test_mr$price))^2)

mae_rf <- mean(abs((test$price-predictions_rf)))
mae_rf_mr <- mean(abs((test_mr$price-predictions_rf_mr)))

r2_rf_oob <- rf_train$r.squared
r2_rf_mr_oob <- rf_train_mr$r.squared

RMSE_rf_oob <- round(sqrt(rf_train$prediction.error))
RMSE_rf_mr_oob <- round(sqrt(rf_train_mr$prediction.error))

res_rf <- tribble(
      ~Imput, ~RMSE, ~R2, ~MAE, ~RMSE_oob, ~R2_oob, 
      "Media",   round(RMSE_rf), round(r2_rf,2), round(mae_rf), round(RMSE_rf_oob), round(r2_rf_oob,2),
      "Random Forest", round(RMSE_rf_mr), round(r2_rf_mr,2), round(mae_rf_mr), round(RMSE_rf_mr_oob), round(r2_rf_mr_oob,2)
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(res_rf, escape = F, booktabs = F, align = "c", col.names = c('Imputación', 'RMSE', "$R^2$", "MAE", 'RMSE OOB', "$R^2$ OOB"), format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Principales medidas de resumen de los modelos ajustados por \textit{Random Forest} según metodología de imputación de valores faltantes utilizada, considerando muestra de entrenamiento y muestra de testeo. Se presentan la raíz cuadrada del error cuadrático medio (RMSE), el coeficiente de determinación ($R^2$), y el error absoluto medio (MAE), al igual que el error \textit{out of bag} (RMSE OOB) y el coeficiente de determinación \textit{out of bag} ($R^2$ OOB).}
\label{tab4}
\end{table}

Según se observa en la tabla \ref{tab4} ambos modelos tienen un error promedio de predicción de aproximadamente 26,000 y 17,000 dólares si se utiliza como medida de error a la raíz cuadrada del error cuadrático medio  y al error absoluto medio respectivamente. Asimismo, los modelos explican aproximadamente un 82 \% de la varianza en los datos. Por su parte, si se consideran las métricas de RMSE y $R^2$ \textit{out of bag}, se tiene un error medio de predicción de aproximadamente 27,000 dólares y una varianza explicada aproximadamente del 90 \%.

\section{Boosting \label{sec:resboosting}}

En esta sección se presentan los resultados del ajuste por \textit{Boosting} considerando ambas metodologías de imputación de valores faltantes. 

Se destaca que el ajuste se realizó utilizando la función \textit{gbm} del paquete \textit{gbm} (\cite{gbm}) y tomando los valores por defecto de los \textit{parámetros de ajuste} detallados en la Sección \ref{subsec:boosting} para ambos métodos de imputación. De esta manera, en primer lugar se presenta en la Tabla \ref{hypb} los valores utilizados. 

<<>>=

hyp_boost <- tribble(
  ~Hyperparametro, ~Valor,
  "Número de árboles",  as.character(round(boosting_train$n.trees)), 
  "Tasa de aprendizaje", as.character(boosting_train$shrinkage),
  "Número de particiones en cada árbol",as.character(round(boosting_train$interaction.depth))
)



library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(hyp_boost, booktabs = F, col.names = c('parámetro de ajuste ', 'Valor'), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 12) %>% row_spec(0,bold=TRUE)
@
\caption{Valores por defecto utilizados de los \textit{parámetros de ajuste} en los modelos ajustados por \textit{Boosting}. Se detalla el número de árboles, la tasa de aprendizaje y el número de particiones en cada árbol.}
\label{hypb}
\end{table}

Luego, se presenta graficamente en la Figura \ref{boost_import} un análisis de la importancia permutada de las variables siguiendo la metodología detallada en la Sección \ref{subsec:boosting}. 

\begin{figure}[H]
\centering
<<fig = TRUE, width=6, height=3>>=

p3 <- importancia_boosting$data %>% 
      ggplot(aes(y=reorder(Variable,Importance),x=Importance)) +
      geom_col(fill='#3D56B2') +
                  theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')) +
      labs(y="Variable",x="Importancia") +
      scale_y_discrete(labels = c('Área no cubierta', 'Tiene seguridad', 'Tiene terraza', 
                                  'Tiene piscina', 'Cantidad de dormitorios', 'Área cubierta', 
                                  'Distancia a la rambla este', 
                                  'Ingreso medio ECH','Cantidad de baños \n completos',
                                  'Área total'))

p4 <- importancia_boosting_mr$data %>%
      ggplot(aes(y=reorder(Variable,Importance),x=Importance)) +
      geom_col(fill='#3D56B2') +
                        theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')) +
      labs(y="Variable",x="Importancia") +
      scale_y_discrete(labels = c('Zona Avd. Italia', 'Condición', 'Tiene terraza', 
                                  'Tiene piscina', 'Cantidad de dormitorios', 'Área cubierta', 
                                  'Distancia a la rambla este', 
                                  'Ingreso medio ECH','Cantidad de baños \n completos',
                                  'Área total'))
     

grid.arrange(p3, p4, ncol = 2)

#importancia_boosting <- vip::vip(boosting_train,plot=F)
#importancia_boosting_mr <- vip::vip(boosting_train_mr,plot=F)
@
\captionof{figure}{Gráfico de la importancia permutada de las 10 primeras variables en los modelos ajustados por \textit{Boosting}. Se observa que las variables área total, cantidad de baños completos e ingreso medio ECH son las 3 variables más importantes en ambos modelos.}
\label{boost_import}
\end{figure}

En lo que respecta a las variables geoespaciales construidas, se observa en la Figura \ref{boost_import} que la variable distancia a la rambla este se encuentra entre las 10 variables más importantes para ambos casos. Por otro lado se destaca que a la hora de realizar la imputación por \textit{Random Forest}, la variable condición se encuentra entre las 10 variables más importantes.

Luego, se procedió a obtener cierta medida de la performance predictiva en función de las métricas definidas en la Sección \ref{sec:cv}. Las mismas se presentan a continuación en la Tabla \ref{tab5}.

<<>>=
predictions_boost <- gbm::predict.gbm(boosting_train, test)
predictions_boost_mr <- gbm::predict.gbm(boosting_train_mr, test_mr)

RMSE_boost <- RMSE(predictions_boost, test$price)
RMSE_boost_mr <- RMSE(predictions_boost_mr , test_mr$price)


r2_boost <- sum((predictions_boost-mean(test$price))^2) /
      sum((test$price - mean(test$price))^2)

r2_boost_mr <- sum((predictions_boost_mr-mean(test_mr$price))^2) /
      sum((test_mr$price - mean(test_mr$price))^2)

mae_boost <- mean(abs((test$price-predictions_boost)))
mae_boost_mr <- mean(abs((test_mr$price-predictions_boost_mr)))

res_boost <- tribble(
      ~Imput, ~RMSE, ~R2, ~MAE, 
      "Media",   round(RMSE_boost),  round(r2_boost,2), round(mae_boost),
      "Random Forest", round(RMSE_boost_mr),  round(r2_boost_mr,2), round(mae_boost_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(res_boost, booktabs = F, align = "c", col.names = c('Imputación', 'RMSE', "$R^2$", "MAE"), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Principales medidas de resumen de los modelos ajustados por \textit{Boosting} según metodología de imputación utilizada, considerando muestra de entrenamiento y muestra de testeo. Se presentan la raíz cuadrada del error cuadrático medio (RMSE), el coeficiente de determinación ($R^2$), y el error absoluto medio (MAE).}
\label{tab5}
\end{table}

Según se observa en la Tabla \ref{tab5} ambos modelos tienen un error promedio de predicción de aproximadamente 40,000 y 28,000 dólares si se utiliza como medida de error a la raíz cuadrada del error cuadrático medio y el error absoluto medio respectivamente. Asimismo, los modelos explican aproximadamente un 60 \% de la varianza en los datos.

\section{Support vector regression \label{sec:ressvr}}

En esta sección se presentan los resultados del ajuste por \textit{Support Vector Regression} considerando ambas metodologías de imputación de valores faltantes. 

Se destaca que el ajuste se realizó utilizando la función \textit{ksvm} del paquete \textit{kernlab} (\cite{kernlab}) y tomando los valores por defecto de los \textit{parámetros de ajuste} detallados en la Sección \ref{subsec:svr} para ambos métodos de imputación.

De esta manera, en primer lugar se presenta en la Tabla \ref{hypsvr} los valores de los \textit{parámetros de ajuste} utilizados. 

<<>>=
hyp_svr <- tribble(
  ~Hyperparametro, ~Valor,
  "Parámetro de complejidad",'1', 
  "Umbral", '0.1',
  "Parámetro de escala", '0.03085'
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(hyp_svr, booktabs = F, align = "c", col.names = c('parámetro de ajuste ', 'Valor'), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 12) %>% row_spec(0,bold=TRUE)
@
\caption{Valores por defecto utilizados de los \textit{parámetros de ajuste} en los modelos ajustados por \textit{Support Vector Regression}. Se detalla el parámetro de complejidad, el umbral y el parámetro de escala.}
\label{hypsvr}
\end{table}

Por otro lado en la Figura \ref{svrimport} se presenta gráficamente un análisis de la importancia permutada de las variables siguiendo la metodología detallada en la Sección \ref{subsec:import}. 

\begin{figure}[H]
\centering
<<fig = TRUE, width=6, height=3>>=

p3 <- importancia_SVR$data %>% 
      ggplot(aes(y=reorder(Variable,Importance),x=Importance)) +
  geom_col(fill='#3D56B2') +
            theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')) +
      labs(y="Variable",x="Importancia") +
      scale_y_discrete(labels = c('Zona Avd. Italia', 
                                  'Tiene piscina', 
                                  'Distancia al centro \n comercial más cercano', 
                                  'Área no cubierta', 'Cantidad de dormitorios',
                                  'Cantidad de baños \n completos',
                                  'Ingreso medio ECH','Distancia a la rambla este',
                                  'Área total',
                                  'Área cubierta'))

p4 <- importancia_SVR_mr$data %>% 
      ggplot(aes(y=reorder(Variable,Importance),x=Importance)) +
      geom_col(fill='#3D56B2') +
            theme(legend.position="none",
            axis.text.y = element_text(face = 'bold'),
            axis.title = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45)) +
      labs(y="Variable",x="Importancia") +
      scale_y_discrete(labels = c('Condición', 
                                  'Tiene piscina', 
                                  'Distancia al centro \n comercial más cercano', 
                                  'Cantidad de dormitorios',
                                  'Cantidad de baños \n completos',
                                  'Área no cubierta', 'Área total', 
                                  'Ingreso medio ECH','Distancia a la rambla este',
                                  'Área cubierta'))
                                  

grid.arrange(p3, p4, ncol = 2)

@
\captionof{figure}{Gráfico de la importancia permutada de las 10 primeras variables en los modelos ajustados por \textit{Support Vector Regression}. Se observa que las variables área cubierta, área total y distancia a la rambla este son las 3 variables más importantes en el ajuste utilizando método de imputación de valores faltantes por la media. En lo que respecta al ajuste con imputación de valores faltantes por \textit{Random Forest}, las variables área cubierta, distancia a la rambla este e ingreso medio ECH son las 3 variables más importantes.}
\label{svrimport}
\end{figure}

A su vez, en lo que respecta a las variables geoespaciales construidas, se observa en la Figura \ref{svrimport} que la variable distancia al centro comercial más cercano e ingreso medio ECH se encuentran entre las 10 variables más importantes para los ajustes según ambos métodos de imputación de valores faltantes utilizados. Por otro lado, se destaca que a la hora de realizar la imputación de valores faltantes por \textit{Random Forest}, la variable condición se encuentra entre las 10 variables más importantes.

Por último, se procedió a obtener cierta medida de la performance predictiva en función de las métricas definidas en la Sección \ref{sec:cv}. Las mismas se presentan a continuación en la Tabla \ref{tabsvr}.

<<>>=
# RMSE test

predictions_svr <- kernlab::predict(SVR_train, test_std)
predictions_svr_mr <- kernlab::predict(SVR_train_mr, test_mr_std)

RMSE_svr <- RMSE(kernlab::predict(SVR_train, test_std), test_std$price)
RMSE_svr_mr <- RMSE(kernlab::predict(SVR_train_mr, test_mr_std), test_mr_std$price)

r2_svr <- sum((predictions_svr-mean(test_std$price))^2) /
                        sum((test_std$price - mean(test_std$price))^2)

r2_svr_mr <- sum((predictions_svr_mr-mean(test_mr_std$price))^2) /
                        sum((test_mr_std$price - mean(test_mr_std$price))^2)

mae_svr <- mean(abs((test_std$price-predictions_svr)))
mae_svr_mr <- mean(abs((test_mr_std$price-predictions_svr_mr)))

res_svr <- tribble(
  ~Imput, ~RMSE, ~R2, ~MAE, 
  "Media",   round(RMSE_svr), round(r2_svr,2), round(mae_svr),
  "Random Forest", round(RMSE_svr_mr), round(r2_svr_mr,2), round(mae_svr_mr)
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(res_svr, booktabs = F, align = "c", col.names = c('Imputación', 'RMSE', "$R^2$", "MAE"), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 12) %>% row_spec(0,bold=TRUE)
@
\caption{Principales medidas de resumen de los modelos ajustados por \textit{Support Vector Regression} según metodología de imputación utilizada, considerando muestra de entrenamiento y muestra de testeo. Se presentan la raíz cuadrada del error cuadrático medio (RMSE), el coeficiente de determinación ($R^2$), y el error absoluto medio (MAE).}
\label{tabsvr}
\end{table}


La tabla \ref{tabsvr} muestra que ambos modelos tienen un error promedio de predicción de aproximadamente 33,000 y 23,000 dólares si se utiliza  como medida de error a la raíz cuadrada del error cuadrático medio y a el error absoluto medio respectivamente. Asimismo, los modelos explican aproximadamente un 80 \% de la varianza en los datos.

\section{Parámetros de ajuste \label{sec:hypt}}

Una vez realizado los ajustes de los modelos detallados en el Capítulo \ref{cap:MT} utilizando los valores por defecto, con el fin de mejorar la performance predictiva se procedió a realizar una búsqueda orientada sobre los posibles valores para los \textit{parámetros de ajuste} de cada modelo. 

Para ello, para todos los ajustes se trabajó con la función \textit{train} del paquete \textit{CARET} (\cite{caret}), con excepción del ajuste realizado por \textit{Boosting}. Éste último se realizó mediante una función construída que permite realizar el proceso de selección de los \textit{parámetros de ajuste}, a partir de la función \textit{gbm} del paquete \textit{gbm} y emulando la función \textit{train}. Se realizó esta excepción en la medida que a la fecha del informe el paquete \textit{CARET} presenta ciertos inconvenientes a la hora de realizar un ajuste por el algoritmo \textit{Boosting}.

Por otro lado, se destaca que la función \textit{train} tiene especificada por defecto una grilla de valores para los \textit{parámetros de ajuste} de cada modelo. De esta manera, para cada combinación de \textit{parámetros de ajuste}, se realiza un ajuste y luego se evalúan en función de las métricas detalladas en la Sección \ref{sec:cv}. En la Sección del Anexo \ref{hyptA} se detallan las grillas mencionadas en las Tablas \ref{grilla1}, \ref{grillab}, y \ref{grillac}.

Mediante los resultados obtenidos en estos ajustes iniciales, se realizó el proceso de búsqueda de combinaciones que puedan mejorar la performance predictiva en cada modelo.

Se destaca que algunos de los \textit{parámetros de ajuste} detallados en el Capítulo \ref{cap:MT} permanecen constantes a la hora de realizar los diferentes ajustes utilizando las grillas por defecto. Los mismos se detallan en la Tabla \ref{hypdef}.

<<>>=
hyp_def <- tribble(
  ~Modelo, ~Hiperparametro, ~Valor_por_defecto,
  "Ranfom Forest", "Cantidad de árboles", '500',
  "Ranfom Forest", "Min. Obs", '5',
  "Boosting", "Tasa de aprendizaje", '0.1',
   "Boosting", "Min. Obs", '10',
  "SVR", "Parámetro de escala", '0.03085',
  "SVR", "Umbral", '0.1'
)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(hyp_def, booktabs = F, align = "c", col.names = c('Modelo', 'Hyperparámetro', 'Valor por defecto'), escape = F, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Tabla resumen de los valores de los \textit{parámetros de ajuste} que utiliza por defecto la función \textit{train} del paquete \textit{CARET} y que se mantienen constantes para todos los ajustes.} 
\label{hypdef}
\end{table}

En particular en la Tabla \ref{hypdef} se detalla la cantidad de árboles y cantidad de observaciones mínimas en cada nodo terminal para los ajustes por \textit{Random Forest}. Asimismo, se detalla la tasa de aprendizaje y la cantidad mínima de observaciones en cada nodo terminal para los ajustes por \textit{Boosting}. Por último, se especifican los valores para el parámetro de escala y el umbral tolerado para los ajustes por \textit{SVR}.

De esta forma, en la Figura \ref{fig30} se observa la evolución del error de predicción en función de los valores de los \textit{parámetros de ajuste} utilizados para cada modelo. 

En lo que respecta a los ajustes realizados por \textit{Random Forest}, en la Figura \ref{fig30} se observa el error de predicción en función de la cantidad de variables seleccionadas de manera aleatoria en cada partición, definidas en la grilla por defecto (2, 12, y 23). Se observa un comportamiento decreciente entre los valores 3 y 12 y un comportamiento creciente de menor magnitud, entre los valores 12 y 23. De esta forma, a la hora de realizar el proceso de selección de los \textit{parámetros de ajuste} se consideraron todos los valores enteros entre los números 12 y 20.

\newpage
 
\begin{figure}[H]
\centering
<<fig = TRUE>>=

a <- RF_caret$results %>% 
      filter(splitrule == 'variance') %>% 
      ggplot(aes(x = mtry, y = RMSE)) +
      theme(legend.position = 'none',
            plot.title = element_text(face = 'bold', hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5)) +
      geom_line(color = 'green2') +
      geom_point(color = 'green2') +
      xlab('Cantidad de variables incluidas') +
      labs(title ='Random Forest') +
      ylab('RMSE')

c <- Boosting_caret$tunning %>% 
      mutate(max_tree_depth = factor(interaction.depth)) %>% 
   ggplot(aes(y=RMSE, x= n.trees,color=max_tree_depth)) + 
   geom_point() + 
      geom_line()  +
       theme(plot.title = element_text(face = 'bold',hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5),
            legend.title =  element_text(face='bold'),
             legend.position = 'bottom') +
      xlab('Cantidad de árboles') +
      labs(title ='Boosting', color = 'Número de particiones')

b <- ggplot(SVR_caret) + 
      geom_point(color = 'blue') + 
      geom_line(color = 'blue') +
       theme(axis.title.y = element_blank(),
            plot.title = element_text(face = 'bold', hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5)) +
      xlab('Complejidad del modelo') +
      ylab('RMSE') +
      labs(title ='Support Vector Regression') +
      ylab('RMSE')
      
grid.arrange(arrangeGrob(a,b, ncol = 2), c, ncol = 1)      
      
@
\captionof{figure}{Gráfico de la evolución del error cuadrático medio de predicción (RMSE) en función de los valores de los \textit{parámetros de ajuste} por defecto para el caso de imputación de valores faltantes por la media. En el panel suerior izquierdo se encuentran los resultados para los ajustes por \textit{Random Forest}, en el panel superior derecho se encuentran los resultados para el ajuste por \textit{Support Vector Regression}, y en el panel inferior izquierdo se encuentran los resultados para el ajuste por \textit{Boosting}.}
\label{fig30}
\end{figure}

Por otro lado, en cuanto a los ajustes realizados por \textit{Boosting}, se observa que el error de predicción es decreciente conforme crece el número de árboles. Asimismo, se observa que el mismo es decreciente en cuanto al número de particiones. De esta manera, en la medida que los comportamientos observados para dichos \textit{parámetros de ajuste} son decrecientes conforme se incrementa el valor de los mismos, se tomaron valores del número de particiones entre 3 y 10; y cantidad de árboles entre 500 y 5000. Asimismo, se decidió variar la tasa de aprendizaje tomando valor 0.1 (valor fijo por defecto) y 0.01.

Por último, sobre los ajustes realizados por \textit{Support Vector Regression}, se observa que el error de predicción es decreciente a medida que aumenta el parámetro de complejidad. Por lo tanto, se trabajó con valores enteros entre 1 y 5. A su vez, se utilizaron valores de 0.05 y 0.01 del parámetro de escala.

Se destaca que en la medida que los resultados obtenidos son similares para el caso de imputación de valores faltantes por \textit{Random Forest}, la grilla seleccionada para realizar el proceso de selección de los \textit{parámetros de ajuste} es análoga para cada modelo. En la Figura \ref{hyptrf} de la Sección del Anexo \ref{hyptA} se encuentran los gráficos correspondientes.

Un vez definida la grilla de \textit{parámetros de ajuste} para cada modelo se procedió a implementar el proceso de selección de los \textit{parámetros de ajuste}. En la Sección del Anexo \ref{hyptA} se presenta en las Tablas \ref{tab30}, \ref{tab31} y \ref{tab32} los resultados de cada ajuste para cada modelo según la combinación de \textit{parámetros de ajuste} utilizada. 

De esta forma, en función de los resultados obtenidos se procedió a seleccionar aquel ajuste con mejor performance predictiva para cada modelo y considerando ambos métodos de imputación. A modo de resumen, se presenta en la Tabla \ref{carettuning} los resultados obtenidos. Se destaca que en la misma se incluyen los resultados obtenidos  sobre las métricas definidas en la Sección \ref{sec:cv} para el caso de un ajuste mediante un \textit{Modelo de Regresión Lineal Múltiple}.

<<>>=
#### Caret

#### LM

LM_caret_best <- LM_caret$results %>% select(RMSE, Rsquared, MAE) %>% mutate( algoritmo = 'Modelo Lineal', metodo_imput = 'Media') %>% relocate(algoritmo, metodo_imput)

LM_caret_mr_best <- LM_caret_mr$results %>% select(RMSE, Rsquared, MAE) %>% mutate( algoritmo = 'Modelo Lineal', metodo_imput = 'Random Forest') %>% relocate(algoritmo, metodo_imput)


#### RF

RF_caret_tunning_best <-RF_caret_tunning$results[which.min(RF_caret_tunning$results$RMSE), ] %>%
      select(-RMSESD, -RsquaredSD, -MAESD, -splitrule, -min.node.size, -mtry) %>% mutate( algoritmo = 'Random Forest', metodo_imput = 'Media') %>% relocate(algoritmo, metodo_imput)

RF_caret_tunning_mr_best <-RF_caret_tunning_mr$results[which.min(RF_caret_tunning_mr$results$RMSE), ] %>% 
      select(-RMSESD, -RsquaredSD, -MAESD, -splitrule, -min.node.size, -mtry) %>% mutate( algoritmo = 'Random Forest', metodo_imput = 'Random Forest') %>% relocate(algoritmo, metodo_imput)

#### Boosting

Boosting_caret_tunning_best <-Boosting_caret_tunning$tunning[which.min(Boosting_caret_tunning$tunning$RMSE), ] %>% select(RMSE, Rsquared, MAE)%>% mutate( algoritmo = 'Boosting', metodo_imput = 'Media') %>% relocate(algoritmo, metodo_imput)

Boosting_caret_tunning_mr_best <-Boosting_caret_tunning_mr$tunning[which.min(Boosting_caret_tunning_mr$tunning$RMSE), ] %>% select(RMSE, Rsquared, MAE)%>% mutate( algoritmo = 'Boosting', metodo_imput = 'Random Forest') %>% relocate(algoritmo, metodo_imput)

#### SVR

SVR_caret_tunning_best <- SVR_caret_tunning$results[which.min(SVR_caret_tunning$results$RMSE), ]  %>% select(RMSE, Rsquared, MAE)%>% mutate( algoritmo = 'SVR', metodo_imput = 'Media') %>% relocate(algoritmo, metodo_imput)

SVR_caret_tunning_best_mr <- SVR_caret_tunning_mr$results[which.min(SVR_caret_tunning_mr$results$RMSE), ]  %>% select(RMSE, Rsquared, MAE)%>% mutate( algoritmo = 'SVR', metodo_imput = 'Random Forest') %>% relocate(algoritmo, metodo_imput)

caret_tuning_row <- bind_rows(LM_caret_best, LM_caret_mr_best, RF_caret_tunning_best, RF_caret_tunning_mr_best, Boosting_caret_tunning_best,Boosting_caret_tunning_mr_best, SVR_caret_tunning_best, SVR_caret_tunning_best_mr) %>% arrange(RMSE)

caret_tuning_row <- caret_tuning_row %>% mutate(RMSE = round(RMSE),
                                                Rsquared = round(Rsquared,2),
                                                MAE = round(MAE))
library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(caret_tuning_row, col.names=c('Algoritmo','Método de imputación','RMSE','$R^2$','MAE'),  escape = F, booktabs = F, align = "c", format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE) 
@
\caption{Tabla con las principales medidas de resumen para los modelos con mejor poder predictivo de cada algortimo utilizado luego de haber realizado el proceso de selección de los \textit{parámetros de ajuste}.}
\label{carettuning}
\end{table}

En función de lo observado en la tabla \ref{carettuning}, para ambos métodos de imputación de valores faltantes, el modelo con mejor performance predicitiva con respecto a las tres métricas, es resultado de realizar un ajuste mediante el algoritmo \textit{Boosting}, seguido por \textit{Random Forest}. Por su contraparte, el modelo con menor desempeño en términos de performance predictiva con respecto a las tres métricas, es resultado de realizar un ajuste mediante un \textit{Modelo de Regresión Lineal Múltiple}.

Se destaca que en todos los casos los ajustes considerando metodología de imputación de valores faltantes por \textit{Random Forest} tienen un poder predictivo superior. No obstante, como fue mencionado en la Sección \ref{sec:secfalt} estos ajustes incluyen adicionalmente a la variable condición.

De esta forma, con el fin de seleccionar al mejor modelo en términos de performance predictiva y menor complejidad, en lo que respecta a la cantidad de variables utilizadas en el ajuste y el método de imputación de valores faltantes utilizado, se optó por el ajuste que es resultado de aplicar el algoritmo \textit{Boosting} con método de imputación de valores faltantes por la media. 

Dicho ajuste se realizó utilizando los siguientes valores para los \textit{parámetros de ajuste} del mismo: 1) 5000 árboles, 2) 10 particiones, 3) tasa de aprendizaje 0.1 y 4) 10 observaciones como cantidad mínima en un nodo terminal.

\section{Interpretabilidad \label{sec:resinterp}}

Una vez implementado el proceso de selección de los \textit{parámetros de ajuste} y obtenido el modelo con mejor desempeño en términos de performance predictiva (ajuste por \textit{Boosting}) se procedió a realizar el análisis de interpretabilidad detallado en la Sección \ref{sec:interp}.

En primer lugar se realizó un análisis sobre la importancia de cada predictor según se especifica en la Sección \ref{subsec:import}. De esta forma se presenta en la Figura \ref{fig40} los resultados obtenidos utilizando la función \textit{vip} del paquete \textit{vip} (\cite{vip}).

\begin{figure}[H]
\centering
<<fig = TRUE, width=6, height=3>>=
importancia_best$data %>% 
      ggplot(aes(y=reorder(Variable,Importance),x=Importance)) +
      geom_col(fill='#3D56B2') +
      theme(legend.position="none",
            axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            axis.text.x = element_text(angle = 45),
            axis.text.y = element_text(face = 'bold')
            ) +
      labs(x="Importancia") +
      scale_y_discrete(labels =
                             c('Diferencia fecha de publicación \n y fecha última modificación',
                                  'Tiene gimnasio',
                                  'Área no cubierta',
                                  'Cantidad de baños \n completos',
                                  'Distancia al centro \n comercial más cercano',
                                  'cantidad de dormitorios',
                                  'Ingreso medio ECH',
                                  'Área cubierta',
                                  'Área total',
                                  'Distancia a la rambla este'))
@
\captionof{figure}{Gráfico de la importancia permutada de las 10 primeras variables en el modelo con la mejor performance predictiva según el error de predicción (ajuste por \textit{Boosting}). Se observa que las variables distancia a la rambla este, área total y área cubierta son las 3 variables más importantes en ambos modelos.}
\label{fig40}
\end{figure}

En lo que respecta a las variables geoespaciales construidas, se observa en la Figura \ref{fig40} que las mismas, con la salvedad de zona Avd. Italia, se encuentran entre las 10 variables más importantes. A su vez, se destaca que todas las variables de naturaleza contínua se encuentran entre las 10 variables más importantes.

Sin embargo, según se observa en la Figura \ref{figcor} las variables área total, área cubierta, y área no cubierta presentan una alta correlación lineal. Esto último implicando que los resultados obtenidos en cuanto a la importancia de los mismos se ven afectados según se explicita en la Sección \ref{subsec:import}.

Posteriormente, según lo observado en la Figura \ref{fig40} se procedió a obtener una medida del efecto marginal de algunas de las variables más relevantes sobre la predicción de la variable precio de oferta en dólares, según se detalla en la Sección \ref{subsec:pdp}. De esta manera, se presenta en la Figura \ref{fig41} los resultados obtenidos. Se destaca que los resultados fueron obtenidos utilizando el paquete \textit{iml} (\cite{iml}).

En primer lugar, se observa en la Figura \ref{fig41} un comportamiento decreciente en la predicción del precio de oferta en dólares a medida que se incrementan los valores de distancia a la rambla este. Este resultado se encuentra en concordancia con la correlación lineal negativa entre ambas variables observada en la Figura \ref{figcor}. Asimismo, el resultado se encuentra en línea con el signo de la estimación del coeficiente asociado a esta variable obtenido mediante el ajuste del \textit{Modelo de Regresión Lineal Múltiple}, como se observa en las Tablas \ref{taba} y \ref{tabb} que se encuentran en la Sección del Anexo \ref{mlA}.

Por otra parte, en lo que respecta a la variable área total se observa en la Figura \ref{fig41} un comportamiento creciente en la predicción del precio de oferta en dólares para los apartamentos con un área total inferor a 210 metros cuadrados. Luego, se tiene un comportamiento decreciente para los apartamentos con un área total de entre 210 metros cuadrados y 250 metros cuadrados. A partir de allí, se observa un comportamiento aproximadamente estable en la predicción. Sin embargo, debe considerarse que estos resultados están sujetos a la cantidad de observaciones en el rango de valores entre cada punto de corte definido en la Sección \ref{subsec:pdp}.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
gg1 <- ggplot() + 
       geom_point(data = pdp_dist_shop, aes(x=dist_shop,y=.value)) +
       geom_line(data = pdp_dist_shop, aes(x=dist_shop,y=.value)) +
       theme(axis.title.x = element_text(face = 'bold', size = 8),
              axis.title.y = element_text(size = 8),
             axis.text = element_text(size = 8)) +
      labs(x = 'Distancia al centro comercial más cercano', 
           y = 'Precio de oferta en dólares') +
      geom_rug(data = aptos_sin_na, aes(x = dist_shop))

gg2 <- ggplot() + 
       geom_point(data = pdp_dist_rambla, aes(x=dist_rambla,y=.value)) +
       geom_line(data = pdp_dist_rambla, aes(x=dist_rambla,y=.value)) +
       theme(axis.title.x = element_text(face = 'bold', size = 8),
              axis.title.y = element_text(size = 8),
             axis.text = element_text(size = 8)) +
      labs(x = 'Distancia a la rambla este', 
           y = 'Precio de oferta en dólares') +
      geom_rug(data = aptos_sin_na, aes(x = dist_rambla))  

gg3 <- ggplot() + 
       geom_point(data = pdp_total_area, aes(x=total_area,y=.value)) +
       geom_line(data = pdp_total_area, aes(x=total_area,y=.value)) +
       theme(axis.title.x = element_text(face = 'bold', size = 8),
              axis.title.y = element_text(size = 8),
             axis.text = element_text(size = 8)) +
      labs(x = 'Área total', 
           y = 'Precio de oferta en dólares')  +
      geom_rug(data = aptos_sin_na, aes(x = total_area))

gg4 <- pdp_full_bathrooms %>% 
       ggplot(aes(x=full_bathrooms,y=.value)) + 
       geom_col(fill = 'green4') +
       geom_line() +
       theme(axis.title.x = element_text(face = 'bold', size = 8),
             axis.title.y = element_text(size = 8),
             axis.text = element_text(size = 8)) +
      labs(x = 'Cantidad de baños completos', 
           y = 'Precio de oferta en dólares')  

grid.arrange(gg2, gg1, gg3, gg4, ncol =2)
@
\captionof{figure}{\textit{Gráfico de dependencia parcial} para las variables distancia a la rambla este (panel superior izquierdo), distancia al centro comercial más cercano (panel superior derecho), área total (panel inferior izquierdo) y cantidad de baños completos (panel inferior derecho). Para las variables distancia a la rambla este, distancia al centro comercial más cercano  y área total se incluye en el eje de las abscisas la distribución marginal de cada una de ellas.}
\label{fig41}
\end{figure}

Por su parte, para la variable distancia al centro comercial más cercano se observa en la Figura \ref{fig41} un comportamiento creciente de la predicción del precio de oferta en dólares para los apartamentos con una distancia al centro comercial más cercano inferior a aproximadamente 8,600 metros. Para los apartamentos con una distancia al centro comercial más cercano superior a dicho valor, se observa un comportamiento irregular en la predicción del precio de oferta en dólares. Esto último puede deberse al reducido número de observaciones que se tiene en los datos para valores superiores a 10,000 metros.

Por último, en la Figura  \ref{fig41} se observa que para aquellos apartamentos con dos o más baños completos el modelo realiza en promedio una predicción del precio de oferta en dólares de aproximadamente 22,000 dólares superior respecto a aquellos apartamentos con un solo baño completo.

Una vez realizados los análisis del efecto marginal para una sola variable, se procedió a obtener los efectos marginales para la interacción de dos variables. Para ello, en función de lo observado en las figuras \ref{fig40} y \ref{fig41} se realizó el analisis de intepretabilidad para las variables distancia a la rambla este y cantidad de baños completos, y área total e ingreso medio ECH. En las figuras \ref{fig42} y \ref{fig43} respectivamente se presentan los resultados obtenidos.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
a <- pdp_dist_rambla

a <- a %>% 
      mutate(full_bathrooms = 'Ambas categorías') %>%
      relocate(dist_rambla,full_bathrooms,.value,.type)

c <- rbind(a,pdp_dr_fb)

ggplot() + 
      geom_point(data = c,  aes(x = dist_rambla, y = .value, color=full_bathrooms)) +
      geom_line(data = c, aes(x = dist_rambla, y = .value, color=full_bathrooms)) +
      theme(axis.title.x = element_text(size = 10),
            axis.title.y = element_text(size = 10),
            axis.text = element_text(size = 8),
            legend.title = element_text(size = 10),
            legend.position = 'bottom') +
      labs(x = 'Distancia a la rambla este', y = 'Precio de oferta en dólares') +
      scale_color_manual('Cantidad de baños \n completos', values = c('red','green2','black')) +
      geom_rug(data = aptos_sin_na, aes(x = dist_rambla)) 
@
\captionof{figure}{Gráfico de dependencia parcial para las variables distancia a la rambla este y cantidad de baños completos.}
\label{fig42}
\end{figure}

Según se observa en la Figura \ref{fig42} para cada valor de distancia a la rambla este los apartamentos con dos o más baños completos tienen una predicción promedio del precio de oferta en dólares superior a los apartamentos con un solo baño completo. Donde esta diferencia se acentúa para los apartamentos ubicados a más de 5,500 metros de la rambla Este.

A su vez, a grandes razgos se observa en la Figura \ref{fig42} que la predicción promedio del precio de oferta en dólares en función de la distancia a la rambla este sigue un mismo comportamiento tanto para los apartamentos con un solo baño completo como para los apartamentos con dos o más baños completos.

\begin{figure}[H]
\centering
<<fig = TRUE>>=
pdp_ta_ech <- pdp_ta_ech %>% 
      mutate(.value = .value/1000)

ggplot() +
      geom_tile(data = pdp_ta_ech, aes(x = total_area, y =ingresomedio_ech, fill = .value) ) +
      scale_fill_viridis(discrete = FALSE,
      name = 'Precio de oferta en dólares \n por mil (predicción promedio)') +
      geom_rug(data = aptos_sin_na, aes(x = total_area, y = ingresomedio_ech)) +
            theme(axis.title = element_text(size = 9),
            axis.text = element_text(size = 8),
            legend.title = element_text(size = 9),
            legend.position = 'bottom') +
      labs(x = 'Área total', y = 'Ingreso medio ECH') 

@
\captionof{figure}{Gráfico de dependencia parcial para las variables área total e ingreso medio ECH. Para ambas variables se incluye la distribución marginal en sus respectivos ejes.}
\label{fig43}
\end{figure}

En primer lugar, como se observa en la Figura \ref{fig43} para los apartamentos ubicados en barrios con ingresos medios entre 50,000 y 70,000 pesos uruguayos, la predicción promedio del precio de oferta en dólares se mantiene en torno a los 150,000 dólares. Esto último para todos los valores de la variable área total, con excepción de los apartamentos de entre 100 y 250 metros cuadrados, donde se observa una predicción promedio superior.

Por otra parte, para los apartamentos con un área total de hasta aproximadamente 150 metros cuadrados, se observa a grandes razgos que la predicción del precio de oferta en dólares se incrementa a medida que aumenta el ingreso medio del barrio donde está ubicado el apartamento.

Por último, se observa que los valores de la predicción promedio para los apartamentos con un área entre 150 y 250 metros cuadrados y ubicados en barrios con ingreso medio superior a 150,000 pesos uruguayos son superiores a 200,000 dólares aproximadamente.

\chapter{Comentarios finales \label{cap:conclu}}

En este trabajo se presentaron distintos métodos de aprendizaje supervisado para predecir el precio de oferta de los apartamentos en Montevideo, Uruguay. A su vez, una parte muy importante del trabajo consistió en la generación y la transformación de datos para su porterior uso con énfasis en la reproducibilidad de los resultados.

En cuanto a la capacidad predictiva de los distintos modelos, se observó la incapacidad del \textit{Modelo de Regresión Lineal Múltiple} a la hora de captar relaciones no lineales entre las variables predictoras y la variable de respuesta. Siendo este último el ajuste con menor capacidad predictiva. 

Por otra parte, el ajuste con mayor performance predictiva fue mediante el algoritmo \textit{Boosting} una vez realizado el proceso de selección de los \textit{parámetros de ajuste}. En el cual se observó un error absoluto medio de aproximadamente 18,000 dólares. 

Sin embargo, se destaca que el ajuste previo a la realización de dicho proceso tuvo una performance predictiva inferior a la del resto de los modelos de aprendizaje estadístico utilizados, y similar a la obtenida a partir del ajuste por el \textit{Modelo de Regresión Lineal Múltiple}. Por lo tanto, se recalca la relevancia de la realización del proceso de selección de los \textit{parámetros de ajuste} para los modelos de aprendizaje estadístico.

Por otro lado, en cuanto a los métodos de imputación de valores faltantes implementados, no se observaron diferencias sustanciales en los resultados obtenidos. De esta forma, se consideró apropiado tomar como mejor modelización aquella obtenida mediante la realización de imputación de valores faltantes por la media, principalmente por su simplicidad de cálculo.

En lo que respecta a la importancia de las variables en el ajuste con mayor performance predictiva (ajuste mediante \textit{Boosting}), se observó que todas las variables construidas mediante fuentes externas resultaron entre las 10 variables más importantes con excepción de la variable zona Avd. Italia. En particular, se destaca que la variable distancia a la rambla este se consideró la más importante en función de la metodología implementada (\textit{importancia de las variables permutadas}).

Sobre éste punto, se sugiere para futuros trabajos replicar el análisis construyendo otras variables geoespaciales que puedan ser de interés para el problema planteado, tales como distancia a espacios verdes, ubicación respecto a la calle Bulevar Artigas, distancia a centros hospitalarios, entre otras.

A su vez, se destaca que todas las variables asociadas al tamaño (área) del apartamento se encontraron entre las variables más relevantes. No obstante como se menciona en la Sección \ref{sec:interp} y según se observó en el Capítulo \ref{cap:EDA} este resultado puede estar influenciado por la correlación lineal entre las mismas.

En cuanto al rol que cumple la variable distancia a la rambla este en el algoritmo con mayor performance predictiva, se tiene que a medida que el apartamento se encuentra más cerca de la rambla este de Montevideo, mayor es su precio de oferta. Esto debido a que la misma tiene un efecto inverso y no lineal sobre la predicción promedio del precio de oferta del apartamento. Donde esta última disminuye a medida que se incrementa la distancia entre el apartamento y la rambla.

A su vez, se observó que en el modelo aquellos apartamentos con dos o más baños completos tienen un precio superior a los apartamentos con un solo baño completo, en la medida que la predicción promedio de estos últimos es menor con respecto a los primeros. Más aún, esta diferencia se mantiene en la predicción considerando la distancia a la rambla este.

Por otra parte, en lo que respecta al área del apartamento y el ingreso medio del barrio donde este se encuentra, se tiene un mayor precio de oferta a medida que se incrementa el ingreso medio del barrio, dada el área total del apartamento. Esto debido a que el modelo realiza una predicción promedio superior para barrios con mayores niveles de ingreso. 

Asimismo, para los apartamentos con área total inferior a 150 metros cuadrados, se observó a grandes rasgos que, dado el ingreso del barrio, la predicción del precio de oferta se incrementa a medida que aumenta el área total del apartamento.

Sin embargo, la metodología considerada (\textit{gráficos de dependencia parcial}) posee ciertas limitantes ya que implica el cálculo de los efectos marginales expresado a través de un promedio. Donde dicho resultado puede estar influenciado por observaciones individuales y por lo tanto no lograr captar la heterogeneidad en las predicciones. De esta forma, se propone para futuros trabajos complementar el análisis implementando una metodología de \textit{metódos locales modelo-agnósticos} principalmente mediante la construcción de los gráficos denominados \textit{expectativas individuales condicionales (ICE plots)}. (Molnar, 2021) (\cite{molnar2020interpretable})

Más aún, como se mencionó en la Sección \ref{sec:interp}, la metodología de \textit{gráficos de dependencia parcial} no logra captar correlaciones entre las variables utilizadas en el análisis. Donde en este caso en particular se observó la presencia de una alta correlación lineal en ciertas variables. 

Con el fin de solucionar esta limitante, para futuros trabajos se propone la utilización de los denominados \textit{gráficos de efectos locales acumulados (ALE plots)} que trabajan con distribuciones condicionales en lugar de con distribuciones marginales como es el caso de la metodología de \textit{gráficos de dependencia parcial}. (Molnar, 2021) (\cite{molnar2020interpretable})

En cuanto a la selección del modelo con mejor performance predictiva, se destaca que no se tomó en cuenta el costo computacional que conllevó el ajuste. En éste sentido, si bien se observó que el ajuste por \textit{Random Forest} tiene una performance predictiva inferior, el mismo implica un costo computacional considerablemente menor. De esta forma, dependiendo de los recursos computacionales con los que cuenta el investigador, se propone a \textit{Random Forest} como una alternativa para el problema planteado.

Siguiendo en esta línea, se considera de interés replicar el análisis con un mayor poder de cómputo donde, a pesar de que se trabajó con procesamiento en paralelo, con el fin de realizar un análisis más exhaustivo (principalmente en el proceso de selección de los \textit{parámetros de ajuste}) se considera de importancia aumentar el poder de cómputo.

A su vez, debido a la limitante que presenta la API de \textit{Mercado Libre} en la medida que no permite la obtención de datos históricos, para futuras investigaciones se considera de interés realizar un procedimiento de integración contínua con el fin de obtener descargas periódicas de la información disponible. Donde dicho procedimiento a la fecha de realización del trabajo se realiza de forma manual.

Por último, a la hora de realizar los ajustes de los diferentes modelos de aprendizaje estadístico, se considera de interés trabajar con otros paquetes del lenguaje de programación \textit{R} con mayor capacidad de análisis tales como \textit{mlr} (\cite{mlr}) y \textit{h20} (\cite{h2o}) . 


\printbibliography  


%%%%%%%%%%%%%%%%%%%%%%%%%%% Anexo %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{chapter}
\setcounter{section}
\setcounter{subsection}
\setcounter{subsubsection}

\begin{appendix}

\chapter{Anexo} 

\section{Variables utilizadas \label{varsA}} 

<<>>=
vars <- readxl::read_excel(here('Auxiliar', 'anexo_vars.xlsx'),
                                sheet = 'anexo_vars') %>% arrange(Fuente)
n <- vars %>% group_by(Fuente) %>% summarise(conteo = n())

aux <- as_tibble(matrix(rep(" ",4), ncol = 4, nrow = 1))
colnames(aux) <- colnames(vars)
vars <- bind_rows(aux, vars)

aux_vars <- vars %>% filter(Nombre %in% names(aptos_mr))
  
aux_vars <- aux_vars %>% mutate(`Método de imputación` = case_when(
    Nombre == 'no_covered_area' ~ 'Media - Random Forest',
    Nombre == 'total_area' ~ 'Media - Random Forest',
    Nombre == 'covered_area' ~ 'Media - Random Forest',
    Nombre == 'item_condition' ~ 'Random Forest',
    TRUE ~ 'No Requiere'
  ))

  
  # Tipo de datos
  
t_data <- sapply(aptos_mr,class) %>% data.frame() %>% rename(tipo = ".") 
      
t_data <- t_data %>% mutate(Nombre = rownames(t_data))
  
aux_vars <- aux_vars %>% 
      select(Nombre,Fuente,`Descripción`,`Método de imputación`) %>% 
    full_join(t_data,by="Nombre") %>% 
      mutate(Naturaleza = ifelse(tipo =="factor",
                                 "cualitativa",
                                 "cuantitativa")) %>% 
      relocate(Nombre, Fuente, `Descripción`,Naturaleza,`Método de imputación`) %>% 
      select(-tipo)

 aux_vars$`Descripción`[which(aux_vars$Nombre=="price")] <- "Precio de oferta en dolares estadounidenses"
 
n <- aux_vars %>% group_by(Fuente) %>% summarise(conteo = n())

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(aux_vars %>% select(Nombre, Fuente, `Descripción`, Naturaleza), booktabs = F, align = "l", 
        longtable = TRUE, format.args = list(big.mark = ","), caption = 'Variables utilizadas en los modelos implementados. Se detalla el nombre de la variable en la base de datos, la fuente de obtención de cada una de ellas, la descripción con el nombre utilizado en el informe y la naturaleza.') %>% 
    kable_styling(latex_options = c("hold_position"), font_size = 9) %>% 
    row_spec(0,bold=TRUE) #%>% 
      #pack_rows("Fuente: API Mercado Libre", 1, as.numeric(n[1,2]))  %>% 
      #pack_rows("Fuente: Elaboración propia", as.numeric(n[1,2] + 1), nrow(aux_vars)) 
@
\label{tab10}

\begin{table}[H]
\centering
<<results=tex>>=
kable(aux_vars %>% select(Nombre, `Descripción`, `Método de imputación`), booktabs = F, align = "l", 
        longtable = TRUE, format.args = list(big.mark = ",")) %>% 
    kable_styling(latex_options = c("hold_position"), font_size = 9) %>% 
    row_spec(0,bold=TRUE) #%>% 
      #pack_rows("Fuente: API Mercado Libre", 1, as.numeric(n[1,2]))  %>% 
      #pack_rows("Fuente: Elaboración propia", as.numeric(n[1,2] + 1), nrow(aux_vars)) 
@
\caption{Método de imputación utilizado para las variables de entrada de los modelos implementados. En la columna método de imputación se detalla el o los métodos utilizados, o se especifica que no requiere en caso que la variable no presente valores faltantes.}
\label{tab10b}
\end{table}

<<>>=
p_na2 <- p_na %>% mutate(aux = rownames(p_na))  %>%  
      filter(prop_na > 0, prop_na <= 0.15, !(aux %in% c('tags','with_virtual_tour', 
                                                        'covered_area_unidad', 
                                                        'condition'))) %>%
      mutate(`Descripción` = c('Condición', 'Área no cubierta',
                                           'Área total', 'Área cubierta')) %>%
      relocate(aux, `Descripción`, prop_na)



library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(p_na2, booktabs = F, col.names = c('Nombre','Descripción','Proporción de \n valores faltantes'), row.names = FALSE, align = "l", longtable = TRUE, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = c("hold_position"), font_size = 9) %>% 
    row_spec(0,bold=TRUE) 
@
\caption{Propoción de valores faltantes en las variables utilizadas en los modelos estadísticos implementados. Se detallan las variables con proporción estrictamente positiva.}
\label{tabna}
\end{table}

\section{Barrios de Montevideo \label{barriosA}}

<<>>=
barrios <- readr::read_csv(here('Auxiliar', 'mapeo_barrios.csv'))
                          
library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(barrios, booktabs = F, align = "l", caption = 'Barrios de Montevideo. En la columna identificador se especifica el código de cada barrio en la API de Mercado libre. En la columna Nombre ML se especifica el nombre asociado a cada identificador en Mercado libre. En la columna Nombre INE se especifica el nombre INE asociado a cada nombre en Mercado libre.', col.names = c('Identificador', 'Nombre ML', 'Nombre INE'), longtable = TRUE, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = c("hold_position"), font_size = 8) %>% 
    row_spec(0,bold=TRUE)
@
\label{tab11}

\newpage

\section{Fórmula de Haversine para el cálculo de distancias \label{harvesineA}}

La fórmula de Haversine para el cálculo de distancias sobre un cuerpo esférico tiene la siguiente expresión:

$$d = 2 \, r \, \text{sen}^{-1} \bigg(\sqrt{ \text{sen}^{2} \, \frac{\phi_2 - \phi_1}{2} + \text{cos} (\phi_1) \, \text{cos}(\phi_2) \, \text{sen}^{2} \,  \frac{\psi_2 - \psi_1}{2}} \bigg)$$ (Chopde, 2013) (\cite{chopde2013landmark}) 

siendo d la distancia entre dos puntos de longitud y latitud $(\psi_1, \phi_1)$ y $(\psi_2, \phi_2)$ respectivamente, y r el radio de la tierra.

\newpage

\section{Árbol de regresión de la variable precio de oferta en funcion de la latitud y longitud donde se ubica el apartamento \label{arbollatlonA}}

\begin{figure}[H]
\centering
<<fig = TRUE>>=
rpart.plot(arbol.prune.lat.lon,roundint = FALSE,digits = 4)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable precio de oferta en dólares mediante la base de datos construida en base a información obtenida de \textit{Mercado Libre}, considerando como variables de entrada latitud y longitud del apartamento. A dicho árbol se le realizó el correspondiente proceso de poda, el cual queda conformado por 7 nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. Donde a mayor intensidad del color azul, mayor la predicción en cuanto al precio de oferta del apartamento.}
\label{arbAnexo}
\end{figure}

\section{Modelo de Regresión Lineal Múltiple \label{mlA}} 

<<>>=
a <- broom::tidy(lm)
b <- broom::tidy(lm_mr)

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(a, booktabs = F, align = "c", col.names = c("Variable", "Coeficientes estimados", "Error estándar", "Estadístico T", "P-Valor"), longtable = TRUE, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 8) %>% row_spec(0,bold=TRUE)
@
\caption{Tabla de resumen de los coeficientes del ajuste mediante el \textit{Modelo de Regresión Lineal Múltiple} con imputación de valores faltantes por la media. Se presentan los coeficientes estimados, la estimación del error estándar de cada uno de ellos, el estadístico T de student y su p-valor asociado.}
\label{taba}
\end{table}

<<>>=
library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(b, booktabs = F, align = "c", col.names = c("Variable", "Coeficientes estimados", "Error estándar", "Estadístico T", "P-Valor"), longtable = TRUE, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 8) %>% row_spec(0,bold=TRUE)
@
\caption{Tabla de resumen de los coeficientes del ajuste mediante el \textit{Modelo de Regresión Lineal Múltiple} con imputación de valores faltantes por \textit{Random Forest}. Se presentan los coeficientes estimados, la estimación del error estándar de cada uno de ellos, el estadístico T de student y su p-valor asociado.}
\label{tabb}
\end{table}

\newpage

<<>>=
bp <- lmtest::bptest(lm) # se rechaza h0 de varianza constante 

bp_mr <- lmtest::bptest(lm_mr) # se rechaza h0 de varianza constante 

bp_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media",   "< 2.2e-16", "Se rechaza H0",
  "Random Forest", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(bp_t, booktabs = F, align = "c", col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Resultados de la prueba de \textit{Breush-Pagan} para el análisis de homocedasticidad de los residuos de los \textit{modelos de regresión lineal múltiple} ajustados según método de imputación de valores faltantes utilizado.}
\label{tab13}
\end{table}

<<>>=
lt <- lillie.test(residuals(lm)) #se rechaza h0 de normalidad

lt_mr <- lillie.test(residuals(lm_mr)) #se rechaza h0 de normalidad

lt_t <- tribble(
  ~Metodo, ~Pvalor, ~Decision,
  "Media", "< 2.2e-16", "Se rechaza H0",
  "Random Forest", "< 2.2e-16",  "Se rechaza H0"
)


library(knitr)
options(knitr.table.format = "latex")
@


\begin{table}[H]
\centering
<<results=tex>>=
kable(lt_t, booktabs = F, align = "c", col.names = c("Método de imputación", "P-Valor de la prueba", "Decisión" ), format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 11) %>% row_spec(0,bold=TRUE)
@
\caption{Resultados de la prueba de \textit{Lilliefors} para el análisis de normalidad de los residuos de los \textit{modelos de regresión lineal múltiple} ajustados según método de imputación de valores faltantes utilizado.}
\label{tab14}
\end{table}

\section{Arbol de regressión \label{arbolA}} 

<<>>=
cp_error_mr <- data.frame(arbol_mr$cptable)
library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(cp_error %>% select(-rel.error, -xstd), col.names = c('CP','Nro particiones','Error de CV') ,booktabs = F, align = "c", format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 9) %>% row_spec(0,bold=TRUE)
@
\caption{Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de \textit{validación cruzada}. La metodología de imputación de valores faltantes utilizada en éste caso es imputación por la media.}
\label{tab17}
\end{table}

\begin{table}[H]
\centering
<<results=tex>>=
kable(cp_error_mr %>% select(-rel.error, -xstd), col.names = c('CP','Nro particiones','Error de CV'),  booktabs = F, align = "c", format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 9) %>% row_spec(0,bold=TRUE)
@
\caption{Tabla resumen, indicadores para realizar el proceso de poda. Se tiene que CP es el parámetro de complejidad, donde se selecciona aquel valor el cual implique un menor error en el proceso de \textit{validación cruzada}. La metodología de imputación de valores  faltantes utilizada en éste caso es imputación por \textit{Random Forest}.}
\label{tab18}
\end{table}

\newpage

\begin{figure}[H]
\centering
<<fig = TRUE>>=
##### Arbol de regresion

# Grafico de la evolucion del erro

# Obtengamos el cp

cp_opt_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"CP"]

npart_mr <- arbol_mr$cptable[which.min(arbol_mr$cptable[,"xerror"]),"nsplit"]

# Grafico

rpart.plot(arbol.prune.mr,roundint = T,digits = -3)
@
\captionof{figure}{Árbol de regresión obtenido al ajustar la variable precio de oferta en dólares mediante la base de datos construida en base a información obtenida de \textit{Mercado Libre},  una vez realizado el proceso de poda con un valor de CP igual a \Sexpr{cp_opt_mr}. El método de imputación sobre valores faltantes es en éste caso imputación por \textit{Random Forest}. El árbol se conforma por \Sexpr{npart_mr+1} nodos terminales (hojas). Los porcentajes dentro de cada nodo indican el porcentaje del número de observaciones que se encuentran en el mismo. Además, se explicita la predicción de las observaciones pertenecientes al nodo. A su vez, se destaca que se realizan \Sexpr{npart_mr} particiones.}
\label{figarbol}
\end{figure}

\newpage

\section{Parámetros de ajuste \label{hyptA}}

<<>>=
#Grillas por defecto
#rf
a <- expand.grid(
      splitrule = 'Variance',
      min.node.size = 5,
      n.trees = 500,
      .mtry = c(2,12,23)
      
) 

#boosting
b  <- expand.grid(
      n.minobsinnode = 10,
         n.trees = c(50,100,150), 
   interaction.depth = c(1,2,3),
   shrinkage = c(0.1)
) 

#SVR
c <- expand.grid(
  C = c(.25,.5,1),
  epsilon = .1,
  sigma = .03085
)

#Tuning grids and results

#rf
d <- RF_caret_tunning$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD, -splitrule, -min.node.size) %>% 
      mutate( algoritmo = 'Random Forest', 
              metodo_imput = 'Media',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput) 



d_mr <- RF_caret_tunning_mr$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD, -splitrule, -min.node.size) %>% 
      mutate( algoritmo = 'Random Forest', 
              metodo_imput = 'Random Forest',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput) 


#boosting
e <- Boosting_caret_tunning$tunning %>%
      select(-n.minobsinnode) %>%
      mutate( algoritmo = 'Random Forest', 
              metodo_imput = 'Media',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput) 

e_mr <- Boosting_caret_tunning_mr$tunning  %>%
      select(-n.minobsinnode) %>%
      mutate( algoritmo = 'Random Forest', 
              metodo_imput = 'Random Forest',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput) 


#SVR
f <- SVR_caret_tunning$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% 
      mutate( algoritmo = 'SVR', 
              metodo_imput = 'Media',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput, sigma, C) 

f_mr <- SVR_caret_tunning_mr$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% 
      mutate( algoritmo = 'SVR', 
              metodo_imput = 'Random Forest',
              RMSE =  round(RMSE),
              Rsquared = round( Rsquared, 2),
              MAE = round(MAE)) %>% 
      relocate(algoritmo, metodo_imput, sigma, C) 

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(a, escape = F, booktabs = F, align = "c", col.names = c("Regla de partición", "Min. obs.", "Can. de arboles", "Cant. de variables"), format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 9) %>% row_spec(0,bold=TRUE) 
@
\caption{Grilla de \textit{parámetros de ajuste} por defecto para los modelos ajustados por \textit{Random Forest}. Se destaca que en todos los casos la regla de partición utilizada es la que minimiza la suma de cuadrados de los residuos y la cantidad de árboles utilizada en el ajuste es 500. Asimismo, la cantidad de observaciones mínimas en un nodo terminal se mantiene constate en 5.}
\label{grilla1}
\end{table}

\begin{table}[H]
\centering
<<results=tex>>=
kable(b, booktabs = F, align = "c", col.names = c( "Min. obs.", "Cant. de árboles", "Particiones", "Tasa de aprendizaje"), format.args = list(big.mark = ","),  escape = F) %>% kable_styling(latex_options = "HOLD_position", font_size = 9) %>% row_spec(0,bold=TRUE) 
@
\caption{Grilla de \textit{parámetros de ajuste} por defecto para los modelos ajustados por \textit{Boosting}. Se destaca que el número de observaciones mínimas en cada nodo terminal se mantiene constante en 10.}
\label{grillab}
\end{table}

\newpage

\begin{table}[H]
\centering
<<results=tex>>=
kable(c, booktabs = F, align = "c", col.names = c("Parámetro de complejidad", "Umbral", "Parámetro de escala"), format.args = list(big.mark = ","),  escape = F) %>% kable_styling(latex_options = "HOLD_position", font_size = 9) %>% row_spec(0,bold=TRUE) 
@
\caption{Grilla de \textit{parámetros de ajuste} por defecto para los modelos ajustados por \textit{Support Vector Regression}. Se destaca que el valor del umbral se mantiene constante en 0.1.}
\label{grillac}
\end{table}

\newpage

\begin{figure}[H]
\centering
<<fig = TRUE>>=

a <- RF_caret_mr$results %>% 
      filter(splitrule == 'variance') %>% 
      ggplot(aes(x = mtry, y = RMSE)) +
      theme(legend.position = 'none',
            axis.title.y = element_blank(),
            plot.title = element_text(face = 'bold', hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5)) +
      geom_line(color = 'green2') +
      geom_point(color = 'green2') +
      xlab('Cantidad de variables incluidas') +
      labs(title ='Random Forest')

c <- Boosting_caret_mr$tunning %>% 
      mutate(max_tree_depth = factor(interaction.depth)) %>% 
   ggplot(aes(y=RMSE, x= n.trees,color=max_tree_depth)) + 
   geom_point() + 
      geom_line()  +
       theme(axis.title.y = element_blank(),
            plot.title = element_text(face = 'bold',hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5),
            legend.title =  element_text(face='bold'),
             legend.position = 'bottom') +
      xlab('Cantidad de árboles') +
      labs(title ='Boosting', color = 'Número de particiones') 

b <- ggplot(SVR_caret_mr) + 
      geom_point(color = 'blue') + 
      geom_line(color = 'blue') +
       theme(axis.title.y = element_blank(),
            plot.title = element_text(face = 'bold', hjust = 0.5, size = 13),
            plot.subtitle = element_text(hjust = 0.5)) +
      xlab('Complejidad del modelo') +
      ylab('RMSE') +
      labs(title ='Support Vector Regression')
      
grid.arrange(arrangeGrob(a,b, ncol = 2), c, ncol = 1)      
      
@
\captionof{figure}{Gráfico de la evolución del error cuadrático medio de predicción (RMSE) en función de los valores de los \textit{parámetros de ajuste} por defecto para el caso de imputación por \textit{Random Forest}. En el panel suerior izquierdo se encuentran los resultados para los ajustes por \textit{Random Forest}, en el panel superior derecho se encuentran los resultados para el ajuste por \textit{Support Vector Regression}, y en el panel inferior izquierdo se encuentran los resultados para el ajuste por \textit{Boosting}.}
\label{hyptrf}
\end{figure}

\newpage

<<>>=
#### Caret

#### RF

RF_caret_tunning_tot <-RF_caret_tunning$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% 
      mutate( n.trees = 500,
      metodo_imput = 'Media') %>% 
      relocate(metodo_imput, n.trees) 

RF_caret_tunning_mr_tot <-RF_caret_tunning_mr$results %>%
      select(-RMSESD, -RsquaredSD, -MAESD) %>% 
      mutate( n.trees = 500,
      metodo_imput = 'Random Forest') %>% 
      relocate(metodo_imput, n.trees)

rf <- bind_rows(RF_caret_tunning_tot, RF_caret_tunning_mr_tot) %>%
      relocate(metodo_imput, splitrule, min.node.size, n.trees, mtry) %>%
      mutate(RMSE = round(RMSE),
             Rsquared = round(Rsquared,2),
             MAE = round(MAE))

#### Boosting

Boosting_caret_tunning_tot <-Boosting_caret_tunning$tunning %>% 
      mutate( metodo_imput = 'Media') %>% 
      relocate(metodo_imput)

Boosting_caret_tunning_mr_tot <-Boosting_caret_tunning_mr$tunning  %>% 
      mutate( metodo_imput = 'Random Forest') %>% 
      relocate(metodo_imput)

boost <- bind_rows(Boosting_caret_tunning_tot, Boosting_caret_tunning_mr_tot) %>%
      relocate(metodo_imput, n.minobsinnode, n.trees, shrinkage, interaction.depth,
               RMSE, Rsquared, MAE) %>%
      mutate(RMSE = round(RMSE),
             Rsquared = round(Rsquared,2),
             MAE = round(MAE))

#### SVR

SVR_caret_tunning_tot <- SVR_caret_tunning$results %>% 
      mutate(epsilon  = 0.1,
             metodo_imput = 'Media') %>% 
      relocate(metodo_imput, epsilon) %>%
      select(-RMSESD, -RsquaredSD, -MAESD)

SVR_caret_tunning_mr_tot <- SVR_caret_tunning_mr$results %>% 
      mutate(epsilon  = 0.1,
      metodo_imput = 'Random Forest') %>% 
      relocate(metodo_imput, epsilon) %>%
      select(-RMSESD, -RsquaredSD, -MAESD)

svr <- bind_rows(SVR_caret_tunning_tot, SVR_caret_tunning_mr_tot) %>%
      mutate(RMSE = round(RMSE),
             Rsquared = round(Rsquared,2),
             MAE = round(MAE))

library(knitr)
options(knitr.table.format = "latex")
@

\begin{table}[H]
\centering
<<results=tex>>=
kable(rf, booktabs = F, align = "c", col.names = c("Imputación", "Regla part.", "Obs. min.", "Cant. de árboles", "Particiones","RMSE","$R^2$", "MAE"), longtable = TRUE, escape = FALSE, format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 8) %>% row_spec(0,bold=TRUE) 
@
\caption{Principales medidas de resumen para los modelos ajustados por \textit{Random Forest} según la grilla especificada en el proceso de selección de los \textit{parámetros de ajuste} según la metodología de imputación de valores faltantes utilizada. Se destaca que en todos los casos la cantidad de árboles utilizada en el ajuste es 500. Asimismo, la cantidad de observaciones mínimas en un nodo terminal se mantiene constate en 5.}
\label{tab30}
\end{table}

\newpage

<<>>=

library(knitr)
options(knitr.table.format = "latex")
@

<<results=tex>>=
kable(boost, caption = 'Principales medidas de resumen para los modelos ajustados por Boosting según la grilla especificada en el proceso de parámetros de ajuste según la metodología de imputación de valores faltantes utilizada.', booktabs = F, longtable = TRUE, escape= FALSE, align = "c", format.args = list(big.mark = ","),  col.names = c("Imputación", "Obs. min.", "Cant. de árboles", "Tasa de aprendizaje", "Particiones","RMSE","$R^2$", "MAE")) %>% kable_styling(latex_options = "HOLD_position", font_size = 8) %>% row_spec(0,bold=TRUE) 
@
\label{tab31}

\newpage

\begin{table}[H]
\centering
<<results=tex>>=
kable(svr, booktabs = F, escape = FALSE, align = "c", col.names = c('Imputación','Umbral','Parámetro de escala','Parámetro de complejidad','RMSE','$R^2$', 'MAE'), format.args = list(big.mark = ",")) %>% kable_styling(latex_options = "HOLD_position", font_size = 8) %>% row_spec(0,bold=TRUE) 
@
\caption{Principales medidas de resumen para los modelos ajustados por \textit{Support Vector Regression} según la grilla especificada en el proceso de \textit{parámetros de ajuste} y  según la metodología de imputación de valores faltantes utilizada. Se destaca que el valor del umbral tolerado para los ajustes se mantiene constante en 0.1.}
\label{tab32}
\end{table}

\end{appendix}

\end{document}




